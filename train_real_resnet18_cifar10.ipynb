{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0og4/gOi5IlmsEul4+woI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a42e5e9e26254642a6eeb76b4b89dafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14ea82b0ee0b435f89e6ef2f9b6b5b35",
              "IPY_MODEL_aea954a800b74161bb42c6979327bf97",
              "IPY_MODEL_4e27af3c01e645258dcdf74c06951e5c"
            ],
            "layout": "IPY_MODEL_94429bb9489241f0b2f838d6609a8926"
          }
        },
        "14ea82b0ee0b435f89e6ef2f9b6b5b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f1bbaade724d91b0590faa22ca92d6",
            "placeholder": "​",
            "style": "IPY_MODEL_fbd11657091f46a1aed7f95e4ddff803",
            "value": "100%"
          }
        },
        "aea954a800b74161bb42c6979327bf97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e2b2ea408b401e9997b11768fbaaf9",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a2950ba6236435888169ec6f7d6b9c0",
            "value": 170498071
          }
        },
        "4e27af3c01e645258dcdf74c06951e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33543580b1b440f3959fcc3669c71660",
            "placeholder": "​",
            "style": "IPY_MODEL_8bf2bab32b944401a5e797e8c4f00a9e",
            "value": " 170498071/170498071 [00:02&lt;00:00, 87936971.85it/s]"
          }
        },
        "94429bb9489241f0b2f838d6609a8926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f1bbaade724d91b0590faa22ca92d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd11657091f46a1aed7f95e4ddff803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00e2b2ea408b401e9997b11768fbaaf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2950ba6236435888169ec6f7d6b9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33543580b1b440f3959fcc3669c71660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bf2bab32b944401a5e797e8c4f00a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vfrantc/quaternion_neurons/blob/main/train_real_resnet18_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9jfw2vhJ9KpU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blocks of the model"
      ],
      "metadata": {
        "id": "qKks01hTCa5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZYu8xpYR9bqX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "v5HwRxMp9gbX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The model (Real resnet)"
      ],
      "metadata": {
        "id": "kPp5hYfbCWhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "5LZmrGGI9k98"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training procedure"
      ],
      "metadata": {
        "id": "-vXUt5jbCQfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(net, criterion, trainloader, optimizer, device, epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
      ],
      "metadata": {
        "id": "mMr29xai9o8k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing procedure"
      ],
      "metadata": {
        "id": "laEOjmLJCN53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, testloader, device, criterion, epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n"
      ],
      "metadata": {
        "id": "BYoM7lBCBTYD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "sMRq-0cHCK7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0\n",
        "start_epoch = 0\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "a42e5e9e26254642a6eeb76b4b89dafe",
            "14ea82b0ee0b435f89e6ef2f9b6b5b35",
            "aea954a800b74161bb42c6979327bf97",
            "4e27af3c01e645258dcdf74c06951e5c",
            "94429bb9489241f0b2f838d6609a8926",
            "a8f1bbaade724d91b0590faa22ca92d6",
            "fbd11657091f46a1aed7f95e4ddff803",
            "00e2b2ea408b401e9997b11768fbaaf9",
            "5a2950ba6236435888169ec6f7d6b9c0",
            "33543580b1b440f3959fcc3669c71660",
            "8bf2bab32b944401a5e797e8c4f00a9e"
          ]
        },
        "id": "tuJZNjC4BXsD",
        "outputId": "3f20162f-5ea2-42f3-f0c3-13a7886d93ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a42e5e9e26254642a6eeb76b4b89dafe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model"
      ],
      "metadata": {
        "id": "kKL4NZOWCImz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "net = ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFNygP9JBeX6",
        "outputId": "5649ee42-7a86-46db-d96d-09dc800c439d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training "
      ],
      "metadata": {
        "id": "RmVN3r5GCGZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    train(net, criterion, trainloader, optimizer, device, epoch)\n",
        "    test(net, testloader, device, criterion, epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC15bHmXCEjs",
        "outputId": "4f60a3a5-2bbe-4cb1-c31b-2b778bdc26cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.498 | Acc: 7.812% (10/128)\n",
            "1 391 Loss: 2.898 | Acc: 10.938% (28/256)\n",
            "2 391 Loss: 3.267 | Acc: 12.500% (48/384)\n",
            "3 391 Loss: 3.567 | Acc: 14.258% (73/512)\n",
            "4 391 Loss: 3.569 | Acc: 14.844% (95/640)\n",
            "5 391 Loss: 3.700 | Acc: 14.844% (114/768)\n",
            "6 391 Loss: 3.721 | Acc: 15.067% (135/896)\n",
            "7 391 Loss: 3.734 | Acc: 15.234% (156/1024)\n",
            "8 391 Loss: 3.748 | Acc: 15.191% (175/1152)\n",
            "9 391 Loss: 3.621 | Acc: 15.000% (192/1280)\n",
            "10 391 Loss: 3.705 | Acc: 14.773% (208/1408)\n",
            "11 391 Loss: 3.639 | Acc: 15.430% (237/1536)\n",
            "12 391 Loss: 3.609 | Acc: 15.385% (256/1664)\n",
            "13 391 Loss: 3.578 | Acc: 15.848% (284/1792)\n",
            "14 391 Loss: 3.522 | Acc: 15.781% (303/1920)\n",
            "15 391 Loss: 3.484 | Acc: 15.625% (320/2048)\n",
            "16 391 Loss: 3.434 | Acc: 15.349% (334/2176)\n",
            "17 391 Loss: 3.372 | Acc: 15.799% (364/2304)\n",
            "18 391 Loss: 3.341 | Acc: 16.036% (390/2432)\n",
            "19 391 Loss: 3.277 | Acc: 16.406% (420/2560)\n",
            "20 391 Loss: 3.240 | Acc: 16.518% (444/2688)\n",
            "21 391 Loss: 3.240 | Acc: 16.371% (461/2816)\n",
            "22 391 Loss: 3.204 | Acc: 16.236% (478/2944)\n",
            "23 391 Loss: 3.170 | Acc: 16.211% (498/3072)\n",
            "24 391 Loss: 3.133 | Acc: 16.125% (516/3200)\n",
            "25 391 Loss: 3.111 | Acc: 16.136% (537/3328)\n",
            "26 391 Loss: 3.084 | Acc: 16.262% (562/3456)\n",
            "27 391 Loss: 3.051 | Acc: 16.239% (582/3584)\n",
            "28 391 Loss: 3.019 | Acc: 16.433% (610/3712)\n",
            "29 391 Loss: 2.992 | Acc: 16.510% (634/3840)\n",
            "30 391 Loss: 2.969 | Acc: 16.658% (661/3968)\n",
            "31 391 Loss: 2.944 | Acc: 16.870% (691/4096)\n",
            "32 391 Loss: 2.917 | Acc: 17.093% (722/4224)\n",
            "33 391 Loss: 2.893 | Acc: 17.096% (744/4352)\n",
            "34 391 Loss: 2.876 | Acc: 17.388% (779/4480)\n",
            "35 391 Loss: 2.862 | Acc: 17.426% (803/4608)\n",
            "36 391 Loss: 2.850 | Acc: 17.378% (823/4736)\n",
            "37 391 Loss: 2.831 | Acc: 17.578% (855/4864)\n",
            "38 391 Loss: 2.812 | Acc: 17.648% (881/4992)\n",
            "39 391 Loss: 2.794 | Acc: 17.793% (911/5120)\n",
            "40 391 Loss: 2.779 | Acc: 17.797% (934/5248)\n",
            "41 391 Loss: 2.770 | Acc: 17.690% (951/5376)\n",
            "42 391 Loss: 2.754 | Acc: 17.896% (985/5504)\n",
            "43 391 Loss: 2.739 | Acc: 18.129% (1021/5632)\n",
            "44 391 Loss: 2.722 | Acc: 18.229% (1050/5760)\n",
            "45 391 Loss: 2.709 | Acc: 18.325% (1079/5888)\n",
            "46 391 Loss: 2.695 | Acc: 18.451% (1110/6016)\n",
            "47 391 Loss: 2.689 | Acc: 18.473% (1135/6144)\n",
            "48 391 Loss: 2.678 | Acc: 18.654% (1170/6272)\n",
            "49 391 Loss: 2.667 | Acc: 18.719% (1198/6400)\n",
            "50 391 Loss: 2.661 | Acc: 18.765% (1225/6528)\n",
            "51 391 Loss: 2.653 | Acc: 18.825% (1253/6656)\n",
            "52 391 Loss: 2.644 | Acc: 18.956% (1286/6784)\n",
            "53 391 Loss: 2.636 | Acc: 19.025% (1315/6912)\n",
            "54 391 Loss: 2.626 | Acc: 19.062% (1342/7040)\n",
            "55 391 Loss: 2.617 | Acc: 19.043% (1365/7168)\n",
            "56 391 Loss: 2.606 | Acc: 19.052% (1390/7296)\n",
            "57 391 Loss: 2.597 | Acc: 19.073% (1416/7424)\n",
            "58 391 Loss: 2.587 | Acc: 19.094% (1442/7552)\n",
            "59 391 Loss: 2.576 | Acc: 19.245% (1478/7680)\n",
            "60 391 Loss: 2.567 | Acc: 19.326% (1509/7808)\n",
            "61 391 Loss: 2.558 | Acc: 19.481% (1546/7936)\n",
            "62 391 Loss: 2.548 | Acc: 19.630% (1583/8064)\n",
            "63 391 Loss: 2.538 | Acc: 19.763% (1619/8192)\n",
            "64 391 Loss: 2.530 | Acc: 19.880% (1654/8320)\n",
            "65 391 Loss: 2.525 | Acc: 19.898% (1681/8448)\n",
            "66 391 Loss: 2.518 | Acc: 19.951% (1711/8576)\n",
            "67 391 Loss: 2.512 | Acc: 20.060% (1746/8704)\n",
            "68 391 Loss: 2.505 | Acc: 20.120% (1777/8832)\n",
            "69 391 Loss: 2.500 | Acc: 20.145% (1805/8960)\n",
            "70 391 Loss: 2.493 | Acc: 20.202% (1836/9088)\n",
            "71 391 Loss: 2.487 | Acc: 20.269% (1868/9216)\n",
            "72 391 Loss: 2.481 | Acc: 20.345% (1901/9344)\n",
            "73 391 Loss: 2.475 | Acc: 20.386% (1931/9472)\n",
            "74 391 Loss: 2.469 | Acc: 20.417% (1960/9600)\n",
            "75 391 Loss: 2.462 | Acc: 20.415% (1986/9728)\n",
            "76 391 Loss: 2.456 | Acc: 20.485% (2019/9856)\n",
            "77 391 Loss: 2.450 | Acc: 20.553% (2052/9984)\n",
            "78 391 Loss: 2.447 | Acc: 20.659% (2089/10112)\n",
            "79 391 Loss: 2.443 | Acc: 20.703% (2120/10240)\n",
            "80 391 Loss: 2.437 | Acc: 20.814% (2158/10368)\n",
            "81 391 Loss: 2.431 | Acc: 20.913% (2195/10496)\n",
            "82 391 Loss: 2.426 | Acc: 20.924% (2223/10624)\n",
            "83 391 Loss: 2.420 | Acc: 21.038% (2262/10752)\n",
            "84 391 Loss: 2.414 | Acc: 21.039% (2289/10880)\n",
            "85 391 Loss: 2.408 | Acc: 21.157% (2329/11008)\n",
            "86 391 Loss: 2.402 | Acc: 21.193% (2360/11136)\n",
            "87 391 Loss: 2.396 | Acc: 21.298% (2399/11264)\n",
            "88 391 Loss: 2.390 | Acc: 21.357% (2433/11392)\n",
            "89 391 Loss: 2.387 | Acc: 21.398% (2465/11520)\n",
            "90 391 Loss: 2.382 | Acc: 21.437% (2497/11648)\n",
            "91 391 Loss: 2.380 | Acc: 21.450% (2526/11776)\n",
            "92 391 Loss: 2.374 | Acc: 21.472% (2556/11904)\n",
            "93 391 Loss: 2.371 | Acc: 21.484% (2585/12032)\n",
            "94 391 Loss: 2.368 | Acc: 21.439% (2607/12160)\n",
            "95 391 Loss: 2.364 | Acc: 21.452% (2636/12288)\n",
            "96 391 Loss: 2.359 | Acc: 21.561% (2677/12416)\n",
            "97 391 Loss: 2.354 | Acc: 21.644% (2715/12544)\n",
            "98 391 Loss: 2.350 | Acc: 21.717% (2752/12672)\n",
            "99 391 Loss: 2.345 | Acc: 21.781% (2788/12800)\n",
            "100 391 Loss: 2.342 | Acc: 21.821% (2821/12928)\n",
            "101 391 Loss: 2.338 | Acc: 21.867% (2855/13056)\n",
            "102 391 Loss: 2.335 | Acc: 21.837% (2879/13184)\n",
            "103 391 Loss: 2.332 | Acc: 21.920% (2918/13312)\n",
            "104 391 Loss: 2.330 | Acc: 21.927% (2947/13440)\n",
            "105 391 Loss: 2.327 | Acc: 21.949% (2978/13568)\n",
            "106 391 Loss: 2.323 | Acc: 21.992% (3012/13696)\n",
            "107 391 Loss: 2.320 | Acc: 22.056% (3049/13824)\n",
            "108 391 Loss: 2.316 | Acc: 22.104% (3084/13952)\n",
            "109 391 Loss: 2.314 | Acc: 22.131% (3116/14080)\n",
            "110 391 Loss: 2.311 | Acc: 22.142% (3146/14208)\n",
            "111 391 Loss: 2.309 | Acc: 22.189% (3181/14336)\n",
            "112 391 Loss: 2.306 | Acc: 22.214% (3213/14464)\n",
            "113 391 Loss: 2.302 | Acc: 22.231% (3244/14592)\n",
            "114 391 Loss: 2.298 | Acc: 22.296% (3282/14720)\n",
            "115 391 Loss: 2.294 | Acc: 22.387% (3324/14848)\n",
            "116 391 Loss: 2.291 | Acc: 22.456% (3363/14976)\n",
            "117 391 Loss: 2.287 | Acc: 22.497% (3398/15104)\n",
            "118 391 Loss: 2.283 | Acc: 22.532% (3432/15232)\n",
            "119 391 Loss: 2.281 | Acc: 22.578% (3468/15360)\n",
            "120 391 Loss: 2.279 | Acc: 22.572% (3496/15488)\n",
            "121 391 Loss: 2.275 | Acc: 22.663% (3539/15616)\n",
            "122 391 Loss: 2.271 | Acc: 22.745% (3581/15744)\n",
            "123 391 Loss: 2.268 | Acc: 22.820% (3622/15872)\n",
            "124 391 Loss: 2.265 | Acc: 22.900% (3664/16000)\n",
            "125 391 Loss: 2.261 | Acc: 22.941% (3700/16128)\n",
            "126 391 Loss: 2.259 | Acc: 22.995% (3738/16256)\n",
            "127 391 Loss: 2.255 | Acc: 23.071% (3780/16384)\n",
            "128 391 Loss: 2.254 | Acc: 23.086% (3812/16512)\n",
            "129 391 Loss: 2.251 | Acc: 23.167% (3855/16640)\n",
            "130 391 Loss: 2.248 | Acc: 23.187% (3888/16768)\n",
            "131 391 Loss: 2.246 | Acc: 23.195% (3919/16896)\n",
            "132 391 Loss: 2.244 | Acc: 23.214% (3952/17024)\n",
            "133 391 Loss: 2.241 | Acc: 23.286% (3994/17152)\n",
            "134 391 Loss: 2.238 | Acc: 23.281% (4023/17280)\n",
            "135 391 Loss: 2.235 | Acc: 23.357% (4066/17408)\n",
            "136 391 Loss: 2.232 | Acc: 23.415% (4106/17536)\n",
            "137 391 Loss: 2.229 | Acc: 23.454% (4143/17664)\n",
            "138 391 Loss: 2.226 | Acc: 23.550% (4190/17792)\n",
            "139 391 Loss: 2.223 | Acc: 23.560% (4222/17920)\n",
            "140 391 Loss: 2.221 | Acc: 23.626% (4264/18048)\n",
            "141 391 Loss: 2.218 | Acc: 23.685% (4305/18176)\n",
            "142 391 Loss: 2.216 | Acc: 23.749% (4347/18304)\n",
            "143 391 Loss: 2.213 | Acc: 23.779% (4383/18432)\n",
            "144 391 Loss: 2.209 | Acc: 23.858% (4428/18560)\n",
            "145 391 Loss: 2.207 | Acc: 23.930% (4472/18688)\n",
            "146 391 Loss: 2.204 | Acc: 23.980% (4512/18816)\n",
            "147 391 Loss: 2.201 | Acc: 24.045% (4555/18944)\n",
            "148 391 Loss: 2.199 | Acc: 24.098% (4596/19072)\n",
            "149 391 Loss: 2.197 | Acc: 24.151% (4637/19200)\n",
            "150 391 Loss: 2.195 | Acc: 24.167% (4671/19328)\n",
            "151 391 Loss: 2.193 | Acc: 24.234% (4715/19456)\n",
            "152 391 Loss: 2.190 | Acc: 24.295% (4758/19584)\n",
            "153 391 Loss: 2.187 | Acc: 24.361% (4802/19712)\n",
            "154 391 Loss: 2.187 | Acc: 24.385% (4838/19840)\n",
            "155 391 Loss: 2.185 | Acc: 24.434% (4879/19968)\n",
            "156 391 Loss: 2.182 | Acc: 24.517% (4927/20096)\n",
            "157 391 Loss: 2.180 | Acc: 24.570% (4969/20224)\n",
            "158 391 Loss: 2.178 | Acc: 24.558% (4998/20352)\n",
            "159 391 Loss: 2.176 | Acc: 24.575% (5033/20480)\n",
            "160 391 Loss: 2.174 | Acc: 24.646% (5079/20608)\n",
            "161 391 Loss: 2.171 | Acc: 24.725% (5127/20736)\n",
            "162 391 Loss: 2.170 | Acc: 24.736% (5161/20864)\n",
            "163 391 Loss: 2.168 | Acc: 24.728% (5191/20992)\n",
            "164 391 Loss: 2.166 | Acc: 24.787% (5235/21120)\n",
            "165 391 Loss: 2.165 | Acc: 24.821% (5274/21248)\n",
            "166 391 Loss: 2.162 | Acc: 24.911% (5325/21376)\n",
            "167 391 Loss: 2.160 | Acc: 24.953% (5366/21504)\n",
            "168 391 Loss: 2.158 | Acc: 24.986% (5405/21632)\n",
            "169 391 Loss: 2.156 | Acc: 25.028% (5446/21760)\n",
            "170 391 Loss: 2.154 | Acc: 25.078% (5489/21888)\n",
            "171 391 Loss: 2.152 | Acc: 25.114% (5529/22016)\n",
            "172 391 Loss: 2.150 | Acc: 25.185% (5577/22144)\n",
            "173 391 Loss: 2.149 | Acc: 25.211% (5615/22272)\n",
            "174 391 Loss: 2.147 | Acc: 25.277% (5662/22400)\n",
            "175 391 Loss: 2.146 | Acc: 25.311% (5702/22528)\n",
            "176 391 Loss: 2.145 | Acc: 25.322% (5737/22656)\n",
            "177 391 Loss: 2.143 | Acc: 25.369% (5780/22784)\n",
            "178 391 Loss: 2.141 | Acc: 25.445% (5830/22912)\n",
            "179 391 Loss: 2.139 | Acc: 25.503% (5876/23040)\n",
            "180 391 Loss: 2.137 | Acc: 25.552% (5920/23168)\n",
            "181 391 Loss: 2.135 | Acc: 25.601% (5964/23296)\n",
            "182 391 Loss: 2.133 | Acc: 25.640% (6006/23424)\n",
            "183 391 Loss: 2.131 | Acc: 25.692% (6051/23552)\n",
            "184 391 Loss: 2.129 | Acc: 25.739% (6095/23680)\n",
            "185 391 Loss: 2.127 | Acc: 25.794% (6141/23808)\n",
            "186 391 Loss: 2.125 | Acc: 25.856% (6189/23936)\n",
            "187 391 Loss: 2.123 | Acc: 25.877% (6227/24064)\n",
            "188 391 Loss: 2.121 | Acc: 25.901% (6266/24192)\n",
            "189 391 Loss: 2.119 | Acc: 25.933% (6307/24320)\n",
            "190 391 Loss: 2.117 | Acc: 25.973% (6350/24448)\n",
            "191 391 Loss: 2.116 | Acc: 25.989% (6387/24576)\n",
            "192 391 Loss: 2.114 | Acc: 26.012% (6426/24704)\n",
            "193 391 Loss: 2.112 | Acc: 26.043% (6467/24832)\n",
            "194 391 Loss: 2.111 | Acc: 26.066% (6506/24960)\n",
            "195 391 Loss: 2.109 | Acc: 26.100% (6548/25088)\n",
            "196 391 Loss: 2.108 | Acc: 26.130% (6589/25216)\n",
            "197 391 Loss: 2.106 | Acc: 26.192% (6638/25344)\n",
            "198 391 Loss: 2.104 | Acc: 26.221% (6679/25472)\n",
            "199 391 Loss: 2.102 | Acc: 26.258% (6722/25600)\n",
            "200 391 Loss: 2.100 | Acc: 26.306% (6768/25728)\n",
            "201 391 Loss: 2.098 | Acc: 26.334% (6809/25856)\n",
            "202 391 Loss: 2.097 | Acc: 26.374% (6853/25984)\n",
            "203 391 Loss: 2.096 | Acc: 26.383% (6889/26112)\n",
            "204 391 Loss: 2.094 | Acc: 26.429% (6935/26240)\n",
            "205 391 Loss: 2.093 | Acc: 26.426% (6968/26368)\n",
            "206 391 Loss: 2.091 | Acc: 26.479% (7016/26496)\n",
            "207 391 Loss: 2.089 | Acc: 26.544% (7067/26624)\n",
            "208 391 Loss: 2.087 | Acc: 26.589% (7113/26752)\n",
            "209 391 Loss: 2.085 | Acc: 26.652% (7164/26880)\n",
            "210 391 Loss: 2.084 | Acc: 26.696% (7210/27008)\n",
            "211 391 Loss: 2.082 | Acc: 26.750% (7259/27136)\n",
            "212 391 Loss: 2.081 | Acc: 26.779% (7301/27264)\n",
            "213 391 Loss: 2.079 | Acc: 26.814% (7345/27392)\n",
            "214 391 Loss: 2.077 | Acc: 26.890% (7400/27520)\n",
            "215 391 Loss: 2.076 | Acc: 26.939% (7448/27648)\n",
            "216 391 Loss: 2.074 | Acc: 27.002% (7500/27776)\n",
            "217 391 Loss: 2.073 | Acc: 27.028% (7542/27904)\n",
            "218 391 Loss: 2.071 | Acc: 27.076% (7590/28032)\n",
            "219 391 Loss: 2.070 | Acc: 27.095% (7630/28160)\n",
            "220 391 Loss: 2.069 | Acc: 27.135% (7676/28288)\n",
            "221 391 Loss: 2.067 | Acc: 27.196% (7728/28416)\n",
            "222 391 Loss: 2.065 | Acc: 27.246% (7777/28544)\n",
            "223 391 Loss: 2.063 | Acc: 27.288% (7824/28672)\n",
            "224 391 Loss: 2.062 | Acc: 27.309% (7865/28800)\n",
            "225 391 Loss: 2.060 | Acc: 27.344% (7910/28928)\n",
            "226 391 Loss: 2.058 | Acc: 27.395% (7960/29056)\n",
            "227 391 Loss: 2.057 | Acc: 27.443% (8009/29184)\n",
            "228 391 Loss: 2.055 | Acc: 27.487% (8057/29312)\n",
            "229 391 Loss: 2.054 | Acc: 27.507% (8098/29440)\n",
            "230 391 Loss: 2.052 | Acc: 27.543% (8144/29568)\n",
            "231 391 Loss: 2.051 | Acc: 27.583% (8191/29696)\n",
            "232 391 Loss: 2.049 | Acc: 27.602% (8232/29824)\n",
            "233 391 Loss: 2.048 | Acc: 27.624% (8274/29952)\n",
            "234 391 Loss: 2.047 | Acc: 27.653% (8318/30080)\n",
            "235 391 Loss: 2.045 | Acc: 27.698% (8367/30208)\n",
            "236 391 Loss: 2.045 | Acc: 27.720% (8409/30336)\n",
            "237 391 Loss: 2.044 | Acc: 27.744% (8452/30464)\n",
            "238 391 Loss: 2.043 | Acc: 27.756% (8491/30592)\n",
            "239 391 Loss: 2.042 | Acc: 27.777% (8533/30720)\n",
            "240 391 Loss: 2.040 | Acc: 27.814% (8580/30848)\n",
            "241 391 Loss: 2.039 | Acc: 27.847% (8626/30976)\n",
            "242 391 Loss: 2.037 | Acc: 27.884% (8673/31104)\n",
            "243 391 Loss: 2.036 | Acc: 27.920% (8720/31232)\n",
            "244 391 Loss: 2.035 | Acc: 27.943% (8763/31360)\n",
            "245 391 Loss: 2.034 | Acc: 27.998% (8816/31488)\n",
            "246 391 Loss: 2.032 | Acc: 28.043% (8866/31616)\n",
            "247 391 Loss: 2.030 | Acc: 28.106% (8922/31744)\n",
            "248 391 Loss: 2.029 | Acc: 28.147% (8971/31872)\n",
            "249 391 Loss: 2.028 | Acc: 28.178% (9017/32000)\n",
            "250 391 Loss: 2.028 | Acc: 28.203% (9061/32128)\n",
            "251 391 Loss: 2.027 | Acc: 28.252% (9113/32256)\n",
            "252 391 Loss: 2.025 | Acc: 28.313% (9169/32384)\n",
            "253 391 Loss: 2.024 | Acc: 28.343% (9215/32512)\n",
            "254 391 Loss: 2.023 | Acc: 28.376% (9262/32640)\n",
            "255 391 Loss: 2.021 | Acc: 28.427% (9315/32768)\n",
            "256 391 Loss: 2.020 | Acc: 28.469% (9365/32896)\n",
            "257 391 Loss: 2.019 | Acc: 28.507% (9414/33024)\n",
            "258 391 Loss: 2.017 | Acc: 28.550% (9465/33152)\n",
            "259 391 Loss: 2.016 | Acc: 28.564% (9506/33280)\n",
            "260 391 Loss: 2.014 | Acc: 28.604% (9556/33408)\n",
            "261 391 Loss: 2.013 | Acc: 28.641% (9605/33536)\n",
            "262 391 Loss: 2.012 | Acc: 28.683% (9656/33664)\n",
            "263 391 Loss: 2.010 | Acc: 28.708% (9701/33792)\n",
            "264 391 Loss: 2.009 | Acc: 28.771% (9759/33920)\n",
            "265 391 Loss: 2.008 | Acc: 28.806% (9808/34048)\n",
            "266 391 Loss: 2.006 | Acc: 28.824% (9851/34176)\n",
            "267 391 Loss: 2.005 | Acc: 28.851% (9897/34304)\n",
            "268 391 Loss: 2.004 | Acc: 28.900% (9951/34432)\n",
            "269 391 Loss: 2.002 | Acc: 28.961% (10009/34560)\n",
            "270 391 Loss: 2.001 | Acc: 28.984% (10054/34688)\n",
            "271 391 Loss: 2.000 | Acc: 29.024% (10105/34816)\n",
            "272 391 Loss: 1.999 | Acc: 29.052% (10152/34944)\n",
            "273 391 Loss: 1.998 | Acc: 29.072% (10196/35072)\n",
            "274 391 Loss: 1.996 | Acc: 29.099% (10243/35200)\n",
            "275 391 Loss: 1.995 | Acc: 29.127% (10290/35328)\n",
            "276 391 Loss: 1.994 | Acc: 29.174% (10344/35456)\n",
            "277 391 Loss: 1.992 | Acc: 29.224% (10399/35584)\n",
            "278 391 Loss: 1.991 | Acc: 29.242% (10443/35712)\n",
            "279 391 Loss: 1.990 | Acc: 29.283% (10495/35840)\n",
            "280 391 Loss: 1.989 | Acc: 29.304% (10540/35968)\n",
            "281 391 Loss: 1.988 | Acc: 29.338% (10590/36096)\n",
            "282 391 Loss: 1.987 | Acc: 29.353% (10633/36224)\n",
            "283 391 Loss: 1.986 | Acc: 29.368% (10676/36352)\n",
            "284 391 Loss: 1.985 | Acc: 29.364% (10712/36480)\n",
            "285 391 Loss: 1.984 | Acc: 29.409% (10766/36608)\n",
            "286 391 Loss: 1.983 | Acc: 29.423% (10809/36736)\n",
            "287 391 Loss: 1.983 | Acc: 29.457% (10859/36864)\n",
            "288 391 Loss: 1.981 | Acc: 29.474% (10903/36992)\n",
            "289 391 Loss: 1.980 | Acc: 29.507% (10953/37120)\n",
            "290 391 Loss: 1.979 | Acc: 29.537% (11002/37248)\n",
            "291 391 Loss: 1.978 | Acc: 29.543% (11042/37376)\n",
            "292 391 Loss: 1.977 | Acc: 29.565% (11088/37504)\n",
            "293 391 Loss: 1.976 | Acc: 29.610% (11143/37632)\n",
            "294 391 Loss: 1.975 | Acc: 29.656% (11198/37760)\n",
            "295 391 Loss: 1.974 | Acc: 29.674% (11243/37888)\n",
            "296 391 Loss: 1.973 | Acc: 29.685% (11285/38016)\n",
            "297 391 Loss: 1.971 | Acc: 29.716% (11335/38144)\n",
            "298 391 Loss: 1.970 | Acc: 29.748% (11385/38272)\n",
            "299 391 Loss: 1.969 | Acc: 29.786% (11438/38400)\n",
            "300 391 Loss: 1.967 | Acc: 29.817% (11488/38528)\n",
            "301 391 Loss: 1.966 | Acc: 29.863% (11544/38656)\n",
            "302 391 Loss: 1.965 | Acc: 29.889% (11592/38784)\n",
            "303 391 Loss: 1.963 | Acc: 29.916% (11641/38912)\n",
            "304 391 Loss: 1.962 | Acc: 29.944% (11690/39040)\n",
            "305 391 Loss: 1.961 | Acc: 29.963% (11736/39168)\n",
            "306 391 Loss: 1.960 | Acc: 30.003% (11790/39296)\n",
            "307 391 Loss: 1.959 | Acc: 30.055% (11849/39424)\n",
            "308 391 Loss: 1.958 | Acc: 30.064% (11891/39552)\n",
            "309 391 Loss: 1.957 | Acc: 30.088% (11939/39680)\n",
            "310 391 Loss: 1.956 | Acc: 30.110% (11986/39808)\n",
            "311 391 Loss: 1.956 | Acc: 30.118% (12028/39936)\n",
            "312 391 Loss: 1.956 | Acc: 30.122% (12068/40064)\n",
            "313 391 Loss: 1.955 | Acc: 30.130% (12110/40192)\n",
            "314 391 Loss: 1.954 | Acc: 30.151% (12157/40320)\n",
            "315 391 Loss: 1.953 | Acc: 30.160% (12199/40448)\n",
            "316 391 Loss: 1.953 | Acc: 30.190% (12250/40576)\n",
            "317 391 Loss: 1.952 | Acc: 30.206% (12295/40704)\n",
            "318 391 Loss: 1.951 | Acc: 30.231% (12344/40832)\n",
            "319 391 Loss: 1.950 | Acc: 30.261% (12395/40960)\n",
            "320 391 Loss: 1.949 | Acc: 30.276% (12440/41088)\n",
            "321 391 Loss: 1.948 | Acc: 30.306% (12491/41216)\n",
            "322 391 Loss: 1.947 | Acc: 30.321% (12536/41344)\n",
            "323 391 Loss: 1.946 | Acc: 30.353% (12588/41472)\n",
            "324 391 Loss: 1.945 | Acc: 30.387% (12641/41600)\n",
            "325 391 Loss: 1.945 | Acc: 30.411% (12690/41728)\n",
            "326 391 Loss: 1.944 | Acc: 30.452% (12746/41856)\n",
            "327 391 Loss: 1.943 | Acc: 30.471% (12793/41984)\n",
            "328 391 Loss: 1.942 | Acc: 30.504% (12846/42112)\n",
            "329 391 Loss: 1.941 | Acc: 30.535% (12898/42240)\n",
            "330 391 Loss: 1.940 | Acc: 30.561% (12948/42368)\n",
            "331 391 Loss: 1.939 | Acc: 30.617% (13011/42496)\n",
            "332 391 Loss: 1.938 | Acc: 30.647% (13063/42624)\n",
            "333 391 Loss: 1.937 | Acc: 30.672% (13113/42752)\n",
            "334 391 Loss: 1.935 | Acc: 30.704% (13166/42880)\n",
            "335 391 Loss: 1.934 | Acc: 30.731% (13217/43008)\n",
            "336 391 Loss: 1.933 | Acc: 30.749% (13264/43136)\n",
            "337 391 Loss: 1.932 | Acc: 30.811% (13330/43264)\n",
            "338 391 Loss: 1.930 | Acc: 30.851% (13387/43392)\n",
            "339 391 Loss: 1.930 | Acc: 30.875% (13437/43520)\n",
            "340 391 Loss: 1.929 | Acc: 30.904% (13489/43648)\n",
            "341 391 Loss: 1.927 | Acc: 30.942% (13545/43776)\n",
            "342 391 Loss: 1.927 | Acc: 30.952% (13589/43904)\n",
            "343 391 Loss: 1.925 | Acc: 31.000% (13650/44032)\n",
            "344 391 Loss: 1.924 | Acc: 31.042% (13708/44160)\n",
            "345 391 Loss: 1.923 | Acc: 31.076% (13763/44288)\n",
            "346 391 Loss: 1.922 | Acc: 31.099% (13813/44416)\n",
            "347 391 Loss: 1.921 | Acc: 31.133% (13868/44544)\n",
            "348 391 Loss: 1.920 | Acc: 31.181% (13929/44672)\n",
            "349 391 Loss: 1.918 | Acc: 31.212% (13983/44800)\n",
            "350 391 Loss: 1.917 | Acc: 31.248% (14039/44928)\n",
            "351 391 Loss: 1.916 | Acc: 31.288% (14097/45056)\n",
            "352 391 Loss: 1.915 | Acc: 31.294% (14140/45184)\n",
            "353 391 Loss: 1.914 | Acc: 31.334% (14198/45312)\n",
            "354 391 Loss: 1.913 | Acc: 31.382% (14260/45440)\n",
            "355 391 Loss: 1.912 | Acc: 31.406% (14311/45568)\n",
            "356 391 Loss: 1.911 | Acc: 31.440% (14367/45696)\n",
            "357 391 Loss: 1.910 | Acc: 31.475% (14423/45824)\n",
            "358 391 Loss: 1.909 | Acc: 31.511% (14480/45952)\n",
            "359 391 Loss: 1.908 | Acc: 31.547% (14537/46080)\n",
            "360 391 Loss: 1.906 | Acc: 31.585% (14595/46208)\n",
            "361 391 Loss: 1.906 | Acc: 31.595% (14640/46336)\n",
            "362 391 Loss: 1.905 | Acc: 31.614% (14689/46464)\n",
            "363 391 Loss: 1.904 | Acc: 31.649% (14746/46592)\n",
            "364 391 Loss: 1.903 | Acc: 31.670% (14796/46720)\n",
            "365 391 Loss: 1.902 | Acc: 31.681% (14842/46848)\n",
            "366 391 Loss: 1.901 | Acc: 31.706% (14894/46976)\n",
            "367 391 Loss: 1.900 | Acc: 31.719% (14941/47104)\n",
            "368 391 Loss: 1.899 | Acc: 31.741% (14992/47232)\n",
            "369 391 Loss: 1.898 | Acc: 31.776% (15049/47360)\n",
            "370 391 Loss: 1.897 | Acc: 31.795% (15099/47488)\n",
            "371 391 Loss: 1.897 | Acc: 31.832% (15157/47616)\n",
            "372 391 Loss: 1.896 | Acc: 31.874% (15218/47744)\n",
            "373 391 Loss: 1.895 | Acc: 31.895% (15269/47872)\n",
            "374 391 Loss: 1.893 | Acc: 31.919% (15321/48000)\n",
            "375 391 Loss: 1.892 | Acc: 31.952% (15378/48128)\n",
            "376 391 Loss: 1.892 | Acc: 31.982% (15433/48256)\n",
            "377 391 Loss: 1.891 | Acc: 31.998% (15482/48384)\n",
            "378 391 Loss: 1.891 | Acc: 32.007% (15527/48512)\n",
            "379 391 Loss: 1.890 | Acc: 32.033% (15581/48640)\n",
            "380 391 Loss: 1.889 | Acc: 32.050% (15630/48768)\n",
            "381 391 Loss: 1.888 | Acc: 32.060% (15676/48896)\n",
            "382 391 Loss: 1.887 | Acc: 32.090% (15732/49024)\n",
            "383 391 Loss: 1.886 | Acc: 32.127% (15791/49152)\n",
            "384 391 Loss: 1.886 | Acc: 32.153% (15845/49280)\n",
            "385 391 Loss: 1.885 | Acc: 32.177% (15898/49408)\n",
            "386 391 Loss: 1.884 | Acc: 32.205% (15953/49536)\n",
            "387 391 Loss: 1.883 | Acc: 32.249% (16016/49664)\n",
            "388 391 Loss: 1.882 | Acc: 32.272% (16069/49792)\n",
            "389 391 Loss: 1.882 | Acc: 32.290% (16119/49920)\n",
            "390 391 Loss: 1.881 | Acc: 32.302% (16151/50000)\n",
            "0 100 Loss: 1.366 | Acc: 52.000% (52/100)\n",
            "1 100 Loss: 1.428 | Acc: 46.000% (92/200)\n",
            "2 100 Loss: 1.397 | Acc: 48.000% (144/300)\n",
            "3 100 Loss: 1.424 | Acc: 46.750% (187/400)\n",
            "4 100 Loss: 1.443 | Acc: 45.600% (228/500)\n",
            "5 100 Loss: 1.461 | Acc: 44.667% (268/600)\n",
            "6 100 Loss: 1.465 | Acc: 44.571% (312/700)\n",
            "7 100 Loss: 1.480 | Acc: 43.500% (348/800)\n",
            "8 100 Loss: 1.478 | Acc: 43.778% (394/900)\n",
            "9 100 Loss: 1.465 | Acc: 44.900% (449/1000)\n",
            "10 100 Loss: 1.464 | Acc: 44.909% (494/1100)\n",
            "11 100 Loss: 1.464 | Acc: 45.000% (540/1200)\n",
            "12 100 Loss: 1.478 | Acc: 44.231% (575/1300)\n",
            "13 100 Loss: 1.492 | Acc: 43.786% (613/1400)\n",
            "14 100 Loss: 1.480 | Acc: 44.400% (666/1500)\n",
            "15 100 Loss: 1.488 | Acc: 44.000% (704/1600)\n",
            "16 100 Loss: 1.489 | Acc: 44.118% (750/1700)\n",
            "17 100 Loss: 1.485 | Acc: 44.222% (796/1800)\n",
            "18 100 Loss: 1.489 | Acc: 44.263% (841/1900)\n",
            "19 100 Loss: 1.494 | Acc: 44.250% (885/2000)\n",
            "20 100 Loss: 1.492 | Acc: 43.952% (923/2100)\n",
            "21 100 Loss: 1.494 | Acc: 44.000% (968/2200)\n",
            "22 100 Loss: 1.494 | Acc: 43.826% (1008/2300)\n",
            "23 100 Loss: 1.495 | Acc: 43.750% (1050/2400)\n",
            "24 100 Loss: 1.491 | Acc: 43.880% (1097/2500)\n",
            "25 100 Loss: 1.502 | Acc: 43.615% (1134/2600)\n",
            "26 100 Loss: 1.499 | Acc: 43.889% (1185/2700)\n",
            "27 100 Loss: 1.499 | Acc: 43.786% (1226/2800)\n",
            "28 100 Loss: 1.504 | Acc: 43.862% (1272/2900)\n",
            "29 100 Loss: 1.505 | Acc: 43.567% (1307/3000)\n",
            "30 100 Loss: 1.502 | Acc: 43.742% (1356/3100)\n",
            "31 100 Loss: 1.497 | Acc: 44.031% (1409/3200)\n",
            "32 100 Loss: 1.497 | Acc: 44.182% (1458/3300)\n",
            "33 100 Loss: 1.498 | Acc: 44.000% (1496/3400)\n",
            "34 100 Loss: 1.500 | Acc: 43.914% (1537/3500)\n",
            "35 100 Loss: 1.498 | Acc: 43.972% (1583/3600)\n",
            "36 100 Loss: 1.500 | Acc: 43.838% (1622/3700)\n",
            "37 100 Loss: 1.500 | Acc: 43.921% (1669/3800)\n",
            "38 100 Loss: 1.496 | Acc: 44.077% (1719/3900)\n",
            "39 100 Loss: 1.497 | Acc: 43.975% (1759/4000)\n",
            "40 100 Loss: 1.500 | Acc: 43.854% (1798/4100)\n",
            "41 100 Loss: 1.501 | Acc: 43.667% (1834/4200)\n",
            "42 100 Loss: 1.499 | Acc: 43.791% (1883/4300)\n",
            "43 100 Loss: 1.500 | Acc: 43.705% (1923/4400)\n",
            "44 100 Loss: 1.498 | Acc: 43.778% (1970/4500)\n",
            "45 100 Loss: 1.497 | Acc: 43.783% (2014/4600)\n",
            "46 100 Loss: 1.495 | Acc: 43.851% (2061/4700)\n",
            "47 100 Loss: 1.497 | Acc: 43.875% (2106/4800)\n",
            "48 100 Loss: 1.496 | Acc: 43.939% (2153/4900)\n",
            "49 100 Loss: 1.497 | Acc: 43.920% (2196/5000)\n",
            "50 100 Loss: 1.499 | Acc: 43.843% (2236/5100)\n",
            "51 100 Loss: 1.499 | Acc: 43.904% (2283/5200)\n",
            "52 100 Loss: 1.498 | Acc: 43.830% (2323/5300)\n",
            "53 100 Loss: 1.502 | Acc: 43.759% (2363/5400)\n",
            "54 100 Loss: 1.506 | Acc: 43.764% (2407/5500)\n",
            "55 100 Loss: 1.509 | Acc: 43.589% (2441/5600)\n",
            "56 100 Loss: 1.510 | Acc: 43.561% (2483/5700)\n",
            "57 100 Loss: 1.506 | Acc: 43.724% (2536/5800)\n",
            "58 100 Loss: 1.506 | Acc: 43.576% (2571/5900)\n",
            "59 100 Loss: 1.507 | Acc: 43.617% (2617/6000)\n",
            "60 100 Loss: 1.508 | Acc: 43.459% (2651/6100)\n",
            "61 100 Loss: 1.509 | Acc: 43.435% (2693/6200)\n",
            "62 100 Loss: 1.509 | Acc: 43.429% (2736/6300)\n",
            "63 100 Loss: 1.507 | Acc: 43.484% (2783/6400)\n",
            "64 100 Loss: 1.507 | Acc: 43.508% (2828/6500)\n",
            "65 100 Loss: 1.508 | Acc: 43.409% (2865/6600)\n",
            "66 100 Loss: 1.508 | Acc: 43.373% (2906/6700)\n",
            "67 100 Loss: 1.508 | Acc: 43.324% (2946/6800)\n",
            "68 100 Loss: 1.506 | Acc: 43.377% (2993/6900)\n",
            "69 100 Loss: 1.508 | Acc: 43.314% (3032/7000)\n",
            "70 100 Loss: 1.510 | Acc: 43.211% (3068/7100)\n",
            "71 100 Loss: 1.510 | Acc: 43.153% (3107/7200)\n",
            "72 100 Loss: 1.510 | Acc: 43.178% (3152/7300)\n",
            "73 100 Loss: 1.508 | Acc: 43.230% (3199/7400)\n",
            "74 100 Loss: 1.508 | Acc: 43.160% (3237/7500)\n",
            "75 100 Loss: 1.508 | Acc: 43.158% (3280/7600)\n",
            "76 100 Loss: 1.505 | Acc: 43.195% (3326/7700)\n",
            "77 100 Loss: 1.506 | Acc: 43.231% (3372/7800)\n",
            "78 100 Loss: 1.505 | Acc: 43.228% (3415/7900)\n",
            "79 100 Loss: 1.506 | Acc: 43.263% (3461/8000)\n",
            "80 100 Loss: 1.506 | Acc: 43.321% (3509/8100)\n",
            "81 100 Loss: 1.506 | Acc: 43.256% (3547/8200)\n",
            "82 100 Loss: 1.508 | Acc: 43.133% (3580/8300)\n",
            "83 100 Loss: 1.507 | Acc: 43.155% (3625/8400)\n",
            "84 100 Loss: 1.509 | Acc: 43.000% (3655/8500)\n",
            "85 100 Loss: 1.508 | Acc: 43.081% (3705/8600)\n",
            "86 100 Loss: 1.510 | Acc: 42.989% (3740/8700)\n",
            "87 100 Loss: 1.510 | Acc: 42.977% (3782/8800)\n",
            "88 100 Loss: 1.510 | Acc: 42.933% (3821/8900)\n",
            "89 100 Loss: 1.511 | Acc: 42.900% (3861/9000)\n",
            "90 100 Loss: 1.510 | Acc: 42.879% (3902/9100)\n",
            "91 100 Loss: 1.510 | Acc: 42.935% (3950/9200)\n",
            "92 100 Loss: 1.508 | Acc: 43.065% (4005/9300)\n",
            "93 100 Loss: 1.509 | Acc: 43.117% (4053/9400)\n",
            "94 100 Loss: 1.509 | Acc: 43.063% (4091/9500)\n",
            "95 100 Loss: 1.509 | Acc: 43.062% (4134/9600)\n",
            "96 100 Loss: 1.509 | Acc: 43.093% (4180/9700)\n",
            "97 100 Loss: 1.509 | Acc: 43.051% (4219/9800)\n",
            "98 100 Loss: 1.510 | Acc: 43.000% (4257/9900)\n",
            "99 100 Loss: 1.510 | Acc: 42.950% (4295/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 1.523 | Acc: 48.438% (62/128)\n",
            "1 391 Loss: 1.542 | Acc: 46.094% (118/256)\n",
            "2 391 Loss: 1.515 | Acc: 44.792% (172/384)\n",
            "3 391 Loss: 1.556 | Acc: 43.945% (225/512)\n",
            "4 391 Loss: 1.532 | Acc: 45.000% (288/640)\n",
            "5 391 Loss: 1.513 | Acc: 45.443% (349/768)\n",
            "6 391 Loss: 1.521 | Acc: 44.531% (399/896)\n",
            "7 391 Loss: 1.521 | Acc: 44.727% (458/1024)\n",
            "8 391 Loss: 1.524 | Acc: 44.705% (515/1152)\n",
            "9 391 Loss: 1.527 | Acc: 44.766% (573/1280)\n",
            "10 391 Loss: 1.525 | Acc: 45.028% (634/1408)\n",
            "11 391 Loss: 1.523 | Acc: 45.052% (692/1536)\n",
            "12 391 Loss: 1.526 | Acc: 45.132% (751/1664)\n",
            "13 391 Loss: 1.531 | Acc: 44.922% (805/1792)\n",
            "14 391 Loss: 1.532 | Acc: 44.688% (858/1920)\n",
            "15 391 Loss: 1.539 | Acc: 44.336% (908/2048)\n",
            "16 391 Loss: 1.540 | Acc: 44.118% (960/2176)\n",
            "17 391 Loss: 1.548 | Acc: 43.793% (1009/2304)\n",
            "18 391 Loss: 1.537 | Acc: 44.243% (1076/2432)\n",
            "19 391 Loss: 1.536 | Acc: 44.219% (1132/2560)\n",
            "20 391 Loss: 1.533 | Acc: 43.973% (1182/2688)\n",
            "21 391 Loss: 1.544 | Acc: 43.288% (1219/2816)\n",
            "22 391 Loss: 1.544 | Acc: 43.308% (1275/2944)\n",
            "23 391 Loss: 1.547 | Acc: 42.969% (1320/3072)\n",
            "24 391 Loss: 1.549 | Acc: 42.688% (1366/3200)\n",
            "25 391 Loss: 1.550 | Acc: 42.638% (1419/3328)\n",
            "26 391 Loss: 1.550 | Acc: 42.622% (1473/3456)\n",
            "27 391 Loss: 1.555 | Acc: 42.383% (1519/3584)\n",
            "28 391 Loss: 1.554 | Acc: 42.268% (1569/3712)\n",
            "29 391 Loss: 1.556 | Acc: 42.292% (1624/3840)\n",
            "30 391 Loss: 1.558 | Acc: 42.188% (1674/3968)\n",
            "31 391 Loss: 1.555 | Acc: 42.261% (1731/4096)\n",
            "32 391 Loss: 1.554 | Acc: 42.424% (1792/4224)\n",
            "33 391 Loss: 1.552 | Acc: 42.509% (1850/4352)\n",
            "34 391 Loss: 1.559 | Acc: 42.366% (1898/4480)\n",
            "35 391 Loss: 1.557 | Acc: 42.405% (1954/4608)\n",
            "36 391 Loss: 1.551 | Acc: 42.652% (2020/4736)\n",
            "37 391 Loss: 1.550 | Acc: 42.660% (2075/4864)\n",
            "38 391 Loss: 1.551 | Acc: 42.628% (2128/4992)\n",
            "39 391 Loss: 1.548 | Acc: 42.812% (2192/5120)\n",
            "40 391 Loss: 1.552 | Acc: 42.626% (2237/5248)\n",
            "41 391 Loss: 1.549 | Acc: 42.801% (2301/5376)\n",
            "42 391 Loss: 1.550 | Acc: 42.678% (2349/5504)\n",
            "43 391 Loss: 1.549 | Acc: 42.773% (2409/5632)\n",
            "44 391 Loss: 1.550 | Acc: 42.743% (2462/5760)\n",
            "45 391 Loss: 1.551 | Acc: 42.816% (2521/5888)\n",
            "46 391 Loss: 1.549 | Acc: 42.753% (2572/6016)\n",
            "47 391 Loss: 1.549 | Acc: 42.790% (2629/6144)\n",
            "48 391 Loss: 1.548 | Acc: 42.809% (2685/6272)\n",
            "49 391 Loss: 1.549 | Acc: 42.859% (2743/6400)\n",
            "50 391 Loss: 1.549 | Acc: 42.862% (2798/6528)\n",
            "51 391 Loss: 1.551 | Acc: 42.834% (2851/6656)\n",
            "52 391 Loss: 1.551 | Acc: 42.821% (2905/6784)\n",
            "53 391 Loss: 1.548 | Acc: 43.027% (2974/6912)\n",
            "54 391 Loss: 1.550 | Acc: 42.926% (3022/7040)\n",
            "55 391 Loss: 1.553 | Acc: 42.843% (3071/7168)\n",
            "56 391 Loss: 1.550 | Acc: 42.996% (3137/7296)\n",
            "57 391 Loss: 1.549 | Acc: 42.942% (3188/7424)\n",
            "58 391 Loss: 1.548 | Acc: 43.048% (3251/7552)\n",
            "59 391 Loss: 1.547 | Acc: 43.112% (3311/7680)\n",
            "60 391 Loss: 1.546 | Acc: 43.110% (3366/7808)\n",
            "61 391 Loss: 1.550 | Acc: 42.994% (3412/7936)\n",
            "62 391 Loss: 1.550 | Acc: 42.882% (3458/8064)\n",
            "63 391 Loss: 1.549 | Acc: 42.920% (3516/8192)\n",
            "64 391 Loss: 1.549 | Acc: 42.873% (3567/8320)\n",
            "65 391 Loss: 1.549 | Acc: 42.862% (3621/8448)\n",
            "66 391 Loss: 1.549 | Acc: 42.840% (3674/8576)\n",
            "67 391 Loss: 1.549 | Acc: 42.865% (3731/8704)\n",
            "68 391 Loss: 1.547 | Acc: 42.923% (3791/8832)\n",
            "69 391 Loss: 1.545 | Acc: 42.924% (3846/8960)\n",
            "70 391 Loss: 1.545 | Acc: 42.859% (3895/9088)\n",
            "71 391 Loss: 1.545 | Acc: 42.817% (3946/9216)\n",
            "72 391 Loss: 1.544 | Acc: 42.830% (4002/9344)\n",
            "73 391 Loss: 1.543 | Acc: 42.895% (4063/9472)\n",
            "74 391 Loss: 1.541 | Acc: 42.865% (4115/9600)\n",
            "75 391 Loss: 1.540 | Acc: 42.897% (4173/9728)\n",
            "76 391 Loss: 1.539 | Acc: 42.918% (4230/9856)\n",
            "77 391 Loss: 1.538 | Acc: 42.889% (4282/9984)\n",
            "78 391 Loss: 1.536 | Acc: 42.998% (4348/10112)\n",
            "79 391 Loss: 1.535 | Acc: 42.969% (4400/10240)\n",
            "80 391 Loss: 1.535 | Acc: 43.027% (4461/10368)\n",
            "81 391 Loss: 1.536 | Acc: 43.035% (4517/10496)\n",
            "82 391 Loss: 1.536 | Acc: 43.091% (4578/10624)\n",
            "83 391 Loss: 1.536 | Acc: 43.090% (4633/10752)\n",
            "84 391 Loss: 1.534 | Acc: 43.226% (4703/10880)\n",
            "85 391 Loss: 1.533 | Acc: 43.241% (4760/11008)\n",
            "86 391 Loss: 1.533 | Acc: 43.229% (4814/11136)\n",
            "87 391 Loss: 1.531 | Acc: 43.288% (4876/11264)\n",
            "88 391 Loss: 1.529 | Acc: 43.329% (4936/11392)\n",
            "89 391 Loss: 1.529 | Acc: 43.299% (4988/11520)\n",
            "90 391 Loss: 1.528 | Acc: 43.312% (5045/11648)\n",
            "91 391 Loss: 1.528 | Acc: 43.385% (5109/11776)\n",
            "92 391 Loss: 1.528 | Acc: 43.347% (5160/11904)\n",
            "93 391 Loss: 1.529 | Acc: 43.376% (5219/12032)\n",
            "94 391 Loss: 1.526 | Acc: 43.544% (5295/12160)\n",
            "95 391 Loss: 1.526 | Acc: 43.522% (5348/12288)\n",
            "96 391 Loss: 1.525 | Acc: 43.565% (5409/12416)\n",
            "97 391 Loss: 1.524 | Acc: 43.654% (5476/12544)\n",
            "98 391 Loss: 1.524 | Acc: 43.687% (5536/12672)\n",
            "99 391 Loss: 1.523 | Acc: 43.750% (5600/12800)\n",
            "100 391 Loss: 1.523 | Acc: 43.765% (5658/12928)\n",
            "101 391 Loss: 1.521 | Acc: 43.804% (5719/13056)\n",
            "102 391 Loss: 1.521 | Acc: 43.826% (5778/13184)\n",
            "103 391 Loss: 1.520 | Acc: 43.855% (5838/13312)\n",
            "104 391 Loss: 1.520 | Acc: 43.854% (5894/13440)\n",
            "105 391 Loss: 1.521 | Acc: 43.838% (5948/13568)\n",
            "106 391 Loss: 1.521 | Acc: 43.816% (6001/13696)\n",
            "107 391 Loss: 1.520 | Acc: 43.801% (6055/13824)\n",
            "108 391 Loss: 1.519 | Acc: 43.865% (6120/13952)\n",
            "109 391 Loss: 1.518 | Acc: 43.906% (6182/14080)\n",
            "110 391 Loss: 1.518 | Acc: 43.891% (6236/14208)\n",
            "111 391 Loss: 1.517 | Acc: 43.924% (6297/14336)\n",
            "112 391 Loss: 1.516 | Acc: 43.937% (6355/14464)\n",
            "113 391 Loss: 1.517 | Acc: 43.969% (6416/14592)\n",
            "114 391 Loss: 1.515 | Acc: 43.995% (6476/14720)\n",
            "115 391 Loss: 1.514 | Acc: 44.053% (6541/14848)\n",
            "116 391 Loss: 1.513 | Acc: 44.064% (6599/14976)\n",
            "117 391 Loss: 1.514 | Acc: 44.015% (6648/15104)\n",
            "118 391 Loss: 1.513 | Acc: 44.032% (6707/15232)\n",
            "119 391 Loss: 1.513 | Acc: 44.043% (6765/15360)\n",
            "120 391 Loss: 1.514 | Acc: 44.002% (6815/15488)\n",
            "121 391 Loss: 1.514 | Acc: 43.942% (6862/15616)\n",
            "122 391 Loss: 1.512 | Acc: 43.979% (6924/15744)\n",
            "123 391 Loss: 1.510 | Acc: 44.059% (6993/15872)\n",
            "124 391 Loss: 1.510 | Acc: 44.081% (7053/16000)\n",
            "125 391 Loss: 1.510 | Acc: 44.054% (7105/16128)\n",
            "126 391 Loss: 1.510 | Acc: 44.070% (7164/16256)\n",
            "127 391 Loss: 1.508 | Acc: 44.122% (7229/16384)\n",
            "128 391 Loss: 1.509 | Acc: 44.144% (7289/16512)\n",
            "129 391 Loss: 1.508 | Acc: 44.195% (7354/16640)\n",
            "130 391 Loss: 1.509 | Acc: 44.167% (7406/16768)\n",
            "131 391 Loss: 1.509 | Acc: 44.141% (7458/16896)\n",
            "132 391 Loss: 1.509 | Acc: 44.120% (7511/17024)\n",
            "133 391 Loss: 1.508 | Acc: 44.211% (7583/17152)\n",
            "134 391 Loss: 1.508 | Acc: 44.213% (7640/17280)\n",
            "135 391 Loss: 1.507 | Acc: 44.278% (7708/17408)\n",
            "136 391 Loss: 1.506 | Acc: 44.326% (7773/17536)\n",
            "137 391 Loss: 1.506 | Acc: 44.350% (7834/17664)\n",
            "138 391 Loss: 1.505 | Acc: 44.323% (7886/17792)\n",
            "139 391 Loss: 1.504 | Acc: 44.375% (7952/17920)\n",
            "140 391 Loss: 1.502 | Acc: 44.448% (8022/18048)\n",
            "141 391 Loss: 1.501 | Acc: 44.482% (8085/18176)\n",
            "142 391 Loss: 1.500 | Acc: 44.509% (8147/18304)\n",
            "143 391 Loss: 1.499 | Acc: 44.542% (8210/18432)\n",
            "144 391 Loss: 1.498 | Acc: 44.553% (8269/18560)\n",
            "145 391 Loss: 1.498 | Acc: 44.558% (8327/18688)\n",
            "146 391 Loss: 1.499 | Acc: 44.542% (8381/18816)\n",
            "147 391 Loss: 1.499 | Acc: 44.505% (8431/18944)\n",
            "148 391 Loss: 1.498 | Acc: 44.552% (8497/19072)\n",
            "149 391 Loss: 1.498 | Acc: 44.583% (8560/19200)\n",
            "150 391 Loss: 1.496 | Acc: 44.645% (8629/19328)\n",
            "151 391 Loss: 1.495 | Acc: 44.649% (8687/19456)\n",
            "152 391 Loss: 1.494 | Acc: 44.659% (8746/19584)\n",
            "153 391 Loss: 1.493 | Acc: 44.704% (8812/19712)\n",
            "154 391 Loss: 1.493 | Acc: 44.713% (8871/19840)\n",
            "155 391 Loss: 1.493 | Acc: 44.752% (8936/19968)\n",
            "156 391 Loss: 1.493 | Acc: 44.750% (8993/20096)\n",
            "157 391 Loss: 1.492 | Acc: 44.778% (9056/20224)\n",
            "158 391 Loss: 1.493 | Acc: 44.787% (9115/20352)\n",
            "159 391 Loss: 1.491 | Acc: 44.805% (9176/20480)\n",
            "160 391 Loss: 1.491 | Acc: 44.847% (9242/20608)\n",
            "161 391 Loss: 1.490 | Acc: 44.874% (9305/20736)\n",
            "162 391 Loss: 1.489 | Acc: 44.886% (9365/20864)\n",
            "163 391 Loss: 1.488 | Acc: 44.946% (9435/20992)\n",
            "164 391 Loss: 1.488 | Acc: 44.957% (9495/21120)\n",
            "165 391 Loss: 1.487 | Acc: 45.021% (9566/21248)\n",
            "166 391 Loss: 1.487 | Acc: 45.004% (9620/21376)\n",
            "167 391 Loss: 1.486 | Acc: 45.038% (9685/21504)\n",
            "168 391 Loss: 1.486 | Acc: 45.049% (9745/21632)\n",
            "169 391 Loss: 1.487 | Acc: 45.060% (9805/21760)\n",
            "170 391 Loss: 1.487 | Acc: 45.061% (9863/21888)\n",
            "171 391 Loss: 1.487 | Acc: 45.058% (9920/22016)\n",
            "172 391 Loss: 1.487 | Acc: 45.064% (9979/22144)\n",
            "173 391 Loss: 1.486 | Acc: 45.088% (10042/22272)\n",
            "174 391 Loss: 1.486 | Acc: 45.098% (10102/22400)\n",
            "175 391 Loss: 1.486 | Acc: 45.073% (10154/22528)\n",
            "176 391 Loss: 1.485 | Acc: 45.092% (10216/22656)\n",
            "177 391 Loss: 1.484 | Acc: 45.146% (10286/22784)\n",
            "178 391 Loss: 1.485 | Acc: 45.142% (10343/22912)\n",
            "179 391 Loss: 1.484 | Acc: 45.174% (10408/23040)\n",
            "180 391 Loss: 1.484 | Acc: 45.196% (10471/23168)\n",
            "181 391 Loss: 1.484 | Acc: 45.179% (10525/23296)\n",
            "182 391 Loss: 1.484 | Acc: 45.202% (10588/23424)\n",
            "183 391 Loss: 1.483 | Acc: 45.206% (10647/23552)\n",
            "184 391 Loss: 1.482 | Acc: 45.224% (10709/23680)\n",
            "185 391 Loss: 1.482 | Acc: 45.270% (10778/23808)\n",
            "186 391 Loss: 1.482 | Acc: 45.275% (10837/23936)\n",
            "187 391 Loss: 1.481 | Acc: 45.300% (10901/24064)\n",
            "188 391 Loss: 1.481 | Acc: 45.308% (10961/24192)\n",
            "189 391 Loss: 1.482 | Acc: 45.296% (11016/24320)\n",
            "190 391 Loss: 1.481 | Acc: 45.345% (11086/24448)\n",
            "191 391 Loss: 1.480 | Acc: 45.357% (11147/24576)\n",
            "192 391 Loss: 1.480 | Acc: 45.381% (11211/24704)\n",
            "193 391 Loss: 1.479 | Acc: 45.381% (11269/24832)\n",
            "194 391 Loss: 1.479 | Acc: 45.369% (11324/24960)\n",
            "195 391 Loss: 1.478 | Acc: 45.404% (11391/25088)\n",
            "196 391 Loss: 1.478 | Acc: 45.435% (11457/25216)\n",
            "197 391 Loss: 1.478 | Acc: 45.415% (11510/25344)\n",
            "198 391 Loss: 1.477 | Acc: 45.458% (11579/25472)\n",
            "199 391 Loss: 1.477 | Acc: 45.465% (11639/25600)\n",
            "200 391 Loss: 1.476 | Acc: 45.499% (11706/25728)\n",
            "201 391 Loss: 1.477 | Acc: 45.467% (11756/25856)\n",
            "202 391 Loss: 1.476 | Acc: 45.474% (11816/25984)\n",
            "203 391 Loss: 1.476 | Acc: 45.500% (11881/26112)\n",
            "204 391 Loss: 1.476 | Acc: 45.514% (11943/26240)\n",
            "205 391 Loss: 1.475 | Acc: 45.548% (12010/26368)\n",
            "206 391 Loss: 1.474 | Acc: 45.584% (12078/26496)\n",
            "207 391 Loss: 1.473 | Acc: 45.609% (12143/26624)\n",
            "208 391 Loss: 1.472 | Acc: 45.686% (12222/26752)\n",
            "209 391 Loss: 1.472 | Acc: 45.685% (12280/26880)\n",
            "210 391 Loss: 1.471 | Acc: 45.723% (12349/27008)\n",
            "211 391 Loss: 1.472 | Acc: 45.740% (12412/27136)\n",
            "212 391 Loss: 1.471 | Acc: 45.764% (12477/27264)\n",
            "213 391 Loss: 1.470 | Acc: 45.780% (12540/27392)\n",
            "214 391 Loss: 1.469 | Acc: 45.789% (12601/27520)\n",
            "215 391 Loss: 1.469 | Acc: 45.837% (12673/27648)\n",
            "216 391 Loss: 1.468 | Acc: 45.853% (12736/27776)\n",
            "217 391 Loss: 1.468 | Acc: 45.868% (12799/27904)\n",
            "218 391 Loss: 1.468 | Acc: 45.865% (12857/28032)\n",
            "219 391 Loss: 1.468 | Acc: 45.874% (12918/28160)\n",
            "220 391 Loss: 1.468 | Acc: 45.882% (12979/28288)\n",
            "221 391 Loss: 1.467 | Acc: 45.921% (13049/28416)\n",
            "222 391 Loss: 1.466 | Acc: 45.943% (13114/28544)\n",
            "223 391 Loss: 1.466 | Acc: 45.954% (13176/28672)\n",
            "224 391 Loss: 1.466 | Acc: 45.955% (13235/28800)\n",
            "225 391 Loss: 1.466 | Acc: 45.955% (13294/28928)\n",
            "226 391 Loss: 1.466 | Acc: 45.942% (13349/29056)\n",
            "227 391 Loss: 1.465 | Acc: 45.974% (13417/29184)\n",
            "228 391 Loss: 1.465 | Acc: 45.961% (13472/29312)\n",
            "229 391 Loss: 1.465 | Acc: 45.982% (13537/29440)\n",
            "230 391 Loss: 1.465 | Acc: 45.996% (13600/29568)\n",
            "231 391 Loss: 1.465 | Acc: 46.006% (13662/29696)\n",
            "232 391 Loss: 1.464 | Acc: 46.050% (13734/29824)\n",
            "233 391 Loss: 1.464 | Acc: 46.050% (13793/29952)\n",
            "234 391 Loss: 1.464 | Acc: 46.054% (13853/30080)\n",
            "235 391 Loss: 1.462 | Acc: 46.090% (13923/30208)\n",
            "236 391 Loss: 1.462 | Acc: 46.094% (13983/30336)\n",
            "237 391 Loss: 1.462 | Acc: 46.130% (14053/30464)\n",
            "238 391 Loss: 1.461 | Acc: 46.179% (14127/30592)\n",
            "239 391 Loss: 1.461 | Acc: 46.198% (14192/30720)\n",
            "240 391 Loss: 1.461 | Acc: 46.194% (14250/30848)\n",
            "241 391 Loss: 1.460 | Acc: 46.200% (14311/30976)\n",
            "242 391 Loss: 1.460 | Acc: 46.222% (14377/31104)\n",
            "243 391 Loss: 1.460 | Acc: 46.225% (14437/31232)\n",
            "244 391 Loss: 1.460 | Acc: 46.237% (14500/31360)\n",
            "245 391 Loss: 1.459 | Acc: 46.287% (14575/31488)\n",
            "246 391 Loss: 1.459 | Acc: 46.321% (14645/31616)\n",
            "247 391 Loss: 1.458 | Acc: 46.346% (14712/31744)\n",
            "248 391 Loss: 1.457 | Acc: 46.354% (14774/31872)\n",
            "249 391 Loss: 1.458 | Acc: 46.366% (14837/32000)\n",
            "250 391 Loss: 1.457 | Acc: 46.368% (14897/32128)\n",
            "251 391 Loss: 1.457 | Acc: 46.401% (14967/32256)\n",
            "252 391 Loss: 1.456 | Acc: 46.430% (15036/32384)\n",
            "253 391 Loss: 1.455 | Acc: 46.460% (15105/32512)\n",
            "254 391 Loss: 1.456 | Acc: 46.452% (15162/32640)\n",
            "255 391 Loss: 1.456 | Acc: 46.448% (15220/32768)\n",
            "256 391 Loss: 1.455 | Acc: 46.459% (15283/32896)\n",
            "257 391 Loss: 1.455 | Acc: 46.475% (15348/33024)\n",
            "258 391 Loss: 1.456 | Acc: 46.426% (15391/33152)\n",
            "259 391 Loss: 1.456 | Acc: 46.439% (15455/33280)\n",
            "260 391 Loss: 1.456 | Acc: 46.438% (15514/33408)\n",
            "261 391 Loss: 1.456 | Acc: 46.455% (15579/33536)\n",
            "262 391 Loss: 1.455 | Acc: 46.474% (15645/33664)\n",
            "263 391 Loss: 1.455 | Acc: 46.475% (15705/33792)\n",
            "264 391 Loss: 1.455 | Acc: 46.474% (15764/33920)\n",
            "265 391 Loss: 1.455 | Acc: 46.484% (15827/34048)\n",
            "266 391 Loss: 1.455 | Acc: 46.486% (15887/34176)\n",
            "267 391 Loss: 1.455 | Acc: 46.508% (15954/34304)\n",
            "268 391 Loss: 1.454 | Acc: 46.541% (16025/34432)\n",
            "269 391 Loss: 1.454 | Acc: 46.562% (16092/34560)\n",
            "270 391 Loss: 1.453 | Acc: 46.584% (16159/34688)\n",
            "271 391 Loss: 1.452 | Acc: 46.605% (16226/34816)\n",
            "272 391 Loss: 1.452 | Acc: 46.595% (16282/34944)\n",
            "273 391 Loss: 1.452 | Acc: 46.570% (16333/35072)\n",
            "274 391 Loss: 1.452 | Acc: 46.594% (16401/35200)\n",
            "275 391 Loss: 1.451 | Acc: 46.637% (16476/35328)\n",
            "276 391 Loss: 1.451 | Acc: 46.652% (16541/35456)\n",
            "277 391 Loss: 1.450 | Acc: 46.664% (16605/35584)\n",
            "278 391 Loss: 1.450 | Acc: 46.659% (16663/35712)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model and compute accuracy"
      ],
      "metadata": {
        "id": "HqoOO6IDCzGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Convert testset to appropriate format\n",
        "    test_labels = []\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            test_labels.extend(labels.cpu().numpy().tolist())\n",
        "            predictions.extend(predicted.cpu().numpy().tolist())\n",
        "\n",
        "    # Compute the classification report using sklearn library\n",
        "    from sklearn.metrics import classification_report\n",
        "\n",
        "    cr = classification_report(test_labels, predictions, target_names=classes)\n",
        "\n",
        "    # Display the classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(cr)\n"
      ],
      "metadata": {
        "id": "1f3nMug4CnZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}