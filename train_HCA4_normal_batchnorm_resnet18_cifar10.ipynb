{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vfrantc/quaternion_neurons/blob/main/train_HCA4_normal_batchnorm_resnet18_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFn4lZebEm29",
        "outputId": "f5847a62-5116-475b-92b8-c91f71b30d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/TParcollet/Quaternion-Neural-Networks.git\n",
            "  Cloning https://github.com/TParcollet/Quaternion-Neural-Networks.git to /tmp/pip-req-build-_ela789d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/TParcollet/Quaternion-Neural-Networks.git /tmp/pip-req-build-_ela789d\n",
            "  Resolved https://github.com/TParcollet/Quaternion-Neural-Networks.git to commit f8de5d5e5a3f9c694a0d62cffc64ec4ccdffd1bc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Pytorch-QNN\n",
            "  Building wheel for Pytorch-QNN (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pytorch-QNN: filename=Pytorch_QNN-1-py3-none-any.whl size=21516 sha256=d091954d90b1c44f0425941c75e3ff7859697c353e616633c045aaccd4a2263f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-meu1tcup/wheels/55/78/10/235c627601beea89722aa1507e19d17aae118511b3de0799b6\n",
            "Successfully built Pytorch-QNN\n",
            "Installing collected packages: Pytorch-QNN\n",
            "Successfully installed Pytorch-QNN-1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/TParcollet/Quaternion-Neural-Networks.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jfw2vhJ9KpU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.nn import Module\n",
        "from torch.nn import BatchNorm2d\n",
        "from torch.nn import Parameter\n",
        "from core_qnn.quaternion_layers import RandomState\n",
        "from core_qnn.quaternion_layers import quaternion_init\n",
        "from core_qnn.quaternion_layers import affect_init\n",
        "from core_qnn.quaternion_ops import unitary_init\n",
        "from core_qnn.quaternion_ops import random_init\n",
        "from core_qnn.quaternion_ops import affect_init_conv\n",
        "from core_qnn.quaternion_layers import get_kernel_and_weight_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypercomplex fully-connected and convolutional layers HCA4"
      ],
      "metadata": {
        "id": "1aJv6-C-J8RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quaternion_conv(input, r_weight, i_weight, j_weight, k_weight, bias, stride,\n",
        "                    padding, groups, dilatation):\n",
        "    \"\"\"\n",
        "    Applies a quaternion convolution to the incoming data:\n",
        "    \"\"\"\n",
        "    # HCA4\n",
        "    cat_kernels_4_r = torch.cat([r_weight, -i_weight, j_weight, -k_weight], dim=1)\n",
        "    cat_kernels_4_i = torch.cat([i_weight,  r_weight, k_weight, j_weight], dim=1)\n",
        "    cat_kernels_4_j = torch.cat([j_weight,  -k_weight, r_weight, -i_weight], dim=1)\n",
        "    cat_kernels_4_k = torch.cat([k_weight,  j_weight, i_weight, r_weight], dim=1)\n",
        "\n",
        "    cat_kernels_4_quaternion   = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=0)\n",
        "\n",
        "    if   input.dim() == 3:\n",
        "        convfunc = F.conv1d\n",
        "    elif input.dim() == 4:\n",
        "        convfunc = F.conv2d\n",
        "    elif input.dim() == 5:\n",
        "        convfunc = F.conv3d\n",
        "    else:\n",
        "        raise Exception(\"The convolutional input is either 3, 4 or 5 dimensions.\"\n",
        "                        \" input.dim = \" + str(input.dim()))\n",
        "\n",
        "    return convfunc(input, cat_kernels_4_quaternion, bias, stride, padding, dilatation, groups)"
      ],
      "metadata": {
        "id": "91wNaZBOJ7Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quaternion_linear(input, r_weight, i_weight, j_weight, k_weight, bias=True):\n",
        "    \"\"\"\n",
        "    Applies a quaternion linear transformation to the incoming data:\n",
        "    It is important to notice that the forward phase of a QNN is defined\n",
        "    as W * Inputs (with * equal to the Hamilton product). The constructed\n",
        "    cat_kernels_4_quaternion is a modified version of the quaternion representation\n",
        "    so when we do torch.mm(Input,W) it's equivalent to W * Inputs.\n",
        "    \"\"\"\n",
        "    # HCA4\n",
        "    cat_kernels_4_r = torch.cat([r_weight, -i_weight, -j_weight, k_weight], dim=0)\n",
        "    cat_kernels_4_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=0)\n",
        "    cat_kernels_4_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=0)\n",
        "    cat_kernels_4_k = torch.cat([k_weight,  -j_weight, -i_weight, r_weight], dim=0)\n",
        "    cat_kernels_4_quaternion   = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=1)\n",
        "\n",
        "    if input.dim() == 2 :\n",
        "\n",
        "        if bias is not None:\n",
        "            return torch.addmm(bias, input, cat_kernels_4_quaternion)\n",
        "        else:\n",
        "            return torch.mm(input, cat_kernels_4_quaternion)\n",
        "    else:\n",
        "        output = torch.matmul(input, cat_kernels_4_quaternion)\n",
        "        if bias is not None:\n",
        "            return output+bias\n",
        "        else:\n",
        "            return output"
      ],
      "metadata": {
        "id": "FiasMLlhKkae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionConv(Module):\n",
        "    \"\"\"Applies a Quaternion Convolution to the incoming data.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 dilatation=1, padding=0, groups=1, bias=True, init_criterion='glorot',\n",
        "                 weight_init='quaternion', seed=None, operation='convolution2d', rotation=False, quaternion_format=True, scale=False):\n",
        "\n",
        "        super(QuaternionConv, self).__init__()\n",
        "\n",
        "        self.in_channels       = in_channels  // 4\n",
        "        self.out_channels      = out_channels // 4\n",
        "        self.stride            = stride\n",
        "        self.padding           = padding\n",
        "        self.groups            = groups\n",
        "        self.dilatation        = dilatation\n",
        "        self.init_criterion    = init_criterion\n",
        "        self.weight_init       = weight_init\n",
        "        self.seed              = seed if seed is not None else np.random.randint(0,1234)\n",
        "        self.rng               = RandomState(self.seed)\n",
        "        self.operation         = operation\n",
        "        self.rotation          = rotation\n",
        "        self.quaternion_format = quaternion_format\n",
        "        self.winit             =    {'quaternion': quaternion_init,\n",
        "                                     'unitary'   : unitary_init,\n",
        "                                     'random'    : random_init}[self.weight_init]\n",
        "        self.scale             = scale\n",
        "\n",
        "\n",
        "        (self.kernel_size, self.w_shape) = get_kernel_and_weight_shape( self.operation,\n",
        "            self.in_channels, self.out_channels, kernel_size )\n",
        "\n",
        "        self.r_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.i_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.j_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "        self.k_weight  = Parameter(torch.Tensor(*self.w_shape))\n",
        "\n",
        "        if self.scale:\n",
        "            self.scale_param  = Parameter(torch.Tensor(self.r_weight.shape))\n",
        "        else:\n",
        "            self.scale_param  = None\n",
        "\n",
        "        if self.rotation:\n",
        "            self.zero_kernel = Parameter(torch.zeros(self.r_weight.shape), requires_grad=False)\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        affect_init_conv(self.r_weight, self.i_weight, self.j_weight, self.k_weight,\n",
        "                    self.kernel_size, self.winit, self.rng, self.init_criterion)\n",
        "        if self.scale_param is not None:\n",
        "            torch.nn.init.xavier_uniform_(self.scale_param.data)\n",
        "        if self.bias is not None:\n",
        "           self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "\n",
        "        if self.rotation:\n",
        "            return quaternion_conv_rotation(input, self.zero_kernel, self.r_weight, self.i_weight, self.j_weight,\n",
        "                self.k_weight, self.bias, self.stride, self.padding, self.groups, self.dilatation,\n",
        "                self.quaternion_format, self.scale_param)\n",
        "        else:\n",
        "            return quaternion_conv(input, self.r_weight, self.i_weight, self.j_weight,\n",
        "                self.k_weight, self.bias, self.stride, self.padding, self.groups, self.dilatation)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_channels='      + str(self.in_channels) \\\n",
        "            + ', out_channels='   + str(self.out_channels) \\\n",
        "            + ', bias='           + str(self.bias is not None) \\\n",
        "            + ', kernel_size='    + str(self.kernel_size) \\\n",
        "            + ', stride='         + str(self.stride) \\\n",
        "            + ', padding='        + str(self.padding) \\\n",
        "            + ', init_criterion=' + str(self.init_criterion) \\\n",
        "            + ', weight_init='    + str(self.weight_init) \\\n",
        "            + ', seed='           + str(self.seed) \\\n",
        "            + ', rotation='       + str(self.rotation) \\\n",
        "            + ', q_format='       + str(self.quaternion_format) \\\n",
        "            + ', operation='      + str(self.operation) + ')'"
      ],
      "metadata": {
        "id": "oQPNf6EdKkdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuaternionLinear(Module):\n",
        "    r\"\"\"Applies a quaternion linear transformation to the incoming data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 init_criterion='he', weight_init='quaternion',\n",
        "                 seed=None):\n",
        "\n",
        "        super(QuaternionLinear, self).__init__()\n",
        "        self.in_features  = in_features//4\n",
        "        self.out_features = out_features//4\n",
        "        self.r_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.i_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.j_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "        self.k_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n",
        "\n",
        "        if bias:\n",
        "            self.bias     = Parameter(torch.Tensor(self.out_features*4))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.init_criterion = init_criterion\n",
        "        self.weight_init    = weight_init\n",
        "        self.seed           = seed if seed is not None else np.random.randint(0,1234)\n",
        "        self.rng            = RandomState(self.seed)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        winit = {'quaternion': quaternion_init,\n",
        "                 'unitary': unitary_init}[self.weight_init]\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.fill_(0)\n",
        "        affect_init(self.r_weight, self.i_weight, self.j_weight, self.k_weight, winit,\n",
        "                    self.rng, self.init_criterion)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here.\n",
        "        if input.dim() == 3:\n",
        "            T, N, C = input.size()\n",
        "            input  = input.view(T * N, C)\n",
        "            output = quaternion_linear(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n",
        "            output = output.view(T, N, output.size(1))\n",
        "        elif input.dim() == 2:\n",
        "            output = quaternion_linear(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_features=' + str(self.in_features) \\\n",
        "            + ', out_features=' + str(self.out_features) \\\n",
        "            + ', bias=' + str(self.bias is not None) \\\n",
        "            + ', init_criterion=' + str(self.init_criterion) \\\n",
        "            + ', weight_init=' + str(self.weight_init) \\\n",
        "            + ', seed=' + str(self.seed) + ')'"
      ],
      "metadata": {
        "id": "5QM5tDg7J4Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fN4OGfx7JhXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKks01hTCa5l"
      },
      "source": [
        "### Blocks of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYu8xpYR9bqX"
      },
      "outputs": [],
      "source": [
        "class QBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(QBasicBlock, self).__init__()\n",
        "        self.conv1 = QuaternionConv(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = QuaternionConv(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                QuaternionConv(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5HwRxMp9gbX"
      },
      "outputs": [],
      "source": [
        "class QBottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(QBottleneck, self).__init__()\n",
        "        self.conv1 = QuaternionConv(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = QuaternionConv(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = QuaternionConv(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.QuaternionConv(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPp5hYfbCWhH"
      },
      "source": [
        "### The model (Real resnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LZmrGGI9k98"
      },
      "outputs": [],
      "source": [
        "class QResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=12):\n",
        "        super(QResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = QuaternionConv(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = QuaternionLinear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        zeros = torch.zeros(x.shape[0], \n",
        "                            1, \n",
        "                            x.shape[2], \n",
        "                            x.shape[3], dtype=x.dtype, device=x.device)\n",
        "        x = torch.cat((zeros, x), dim=1)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        out = out[:, 2:]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vXUt5jbCQfv"
      },
      "source": [
        "### Training procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMr29xai9o8k"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(net, criterion, trainloader, optimizer, device, epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laEOjmLJCN53"
      },
      "source": [
        "### Testing procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYoM7lBCBTYD"
      },
      "outputs": [],
      "source": [
        "def test(net, testloader, device, criterion, epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMRq-0cHCK7d"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "005373afc6b041609bdf9544ad2f672b",
            "a5771af0af0f442cac636b10d8d368f9",
            "4bfaf8dd49aa477b8825c0b37c26a629",
            "403fb86925f144cbaca52883ebe5a84e",
            "87c19b59d8ee4a39bf4623c3aad1c912",
            "a99eb29023114fe7905c060f854f7aea",
            "e1d40381a9e3486e8f299537f3740d14",
            "65cfd10220c0446e88a6666762492ac4",
            "2c0fa492672c4a3baccaf08a4869032b",
            "2ffacaf7135a4942848c78791f56932f",
            "1b5b230f256847f3b83b987bf9da37f2"
          ]
        },
        "id": "tuJZNjC4BXsD",
        "outputId": "148dfea0-a6ed-4128-bedd-d58d7b762c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "005373afc6b041609bdf9544ad2f672b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0\n",
        "start_epoch = 0\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKL4NZOWCImz"
      },
      "source": [
        "### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFNygP9JBeX6",
        "outputId": "038a0307-56e3-445f-b610-2fb63a932b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "net = QResNet(QBasicBlock, [2, 2, 2, 2])\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmVN3r5GCGZ7"
      },
      "source": [
        "### Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC15bHmXCEjs",
        "outputId": "dde4ea98-4b81-42fd-e306-10745cd75d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "0 391 Loss: 2.988 | Acc: 10.156% (13/128)\n",
            "1 391 Loss: 5.151 | Acc: 11.719% (30/256)\n",
            "2 391 Loss: 6.334 | Acc: 11.979% (46/384)\n",
            "3 391 Loss: 7.035 | Acc: 12.109% (62/512)\n",
            "4 391 Loss: 6.711 | Acc: 10.781% (69/640)\n",
            "5 391 Loss: 6.533 | Acc: 10.547% (81/768)\n",
            "6 391 Loss: 6.549 | Acc: 10.603% (95/896)\n",
            "7 391 Loss: 6.181 | Acc: 10.449% (107/1024)\n",
            "8 391 Loss: 6.054 | Acc: 10.243% (118/1152)\n",
            "9 391 Loss: 5.931 | Acc: 10.781% (138/1280)\n",
            "10 391 Loss: 5.770 | Acc: 11.080% (156/1408)\n",
            "11 391 Loss: 5.641 | Acc: 11.068% (170/1536)\n",
            "12 391 Loss: 5.526 | Acc: 10.877% (181/1664)\n",
            "13 391 Loss: 5.370 | Acc: 10.826% (194/1792)\n",
            "14 391 Loss: 5.213 | Acc: 10.729% (206/1920)\n",
            "15 391 Loss: 5.058 | Acc: 10.645% (218/2048)\n",
            "16 391 Loss: 4.916 | Acc: 10.616% (231/2176)\n",
            "17 391 Loss: 4.771 | Acc: 10.938% (252/2304)\n",
            "18 391 Loss: 4.744 | Acc: 10.979% (267/2432)\n",
            "19 391 Loss: 4.621 | Acc: 10.938% (280/2560)\n",
            "20 391 Loss: 4.513 | Acc: 10.938% (294/2688)\n",
            "21 391 Loss: 4.418 | Acc: 10.902% (307/2816)\n",
            "22 391 Loss: 4.422 | Acc: 10.836% (319/2944)\n",
            "23 391 Loss: 4.344 | Acc: 10.970% (337/3072)\n",
            "24 391 Loss: 4.266 | Acc: 10.875% (348/3200)\n",
            "25 391 Loss: 4.206 | Acc: 10.968% (365/3328)\n",
            "26 391 Loss: 4.137 | Acc: 11.053% (382/3456)\n",
            "27 391 Loss: 4.075 | Acc: 11.189% (401/3584)\n",
            "28 391 Loss: 4.013 | Acc: 11.180% (415/3712)\n",
            "29 391 Loss: 3.965 | Acc: 11.224% (431/3840)\n",
            "30 391 Loss: 3.909 | Acc: 11.442% (454/3968)\n",
            "31 391 Loss: 3.859 | Acc: 11.426% (468/4096)\n",
            "32 391 Loss: 3.811 | Acc: 11.435% (483/4224)\n",
            "33 391 Loss: 3.767 | Acc: 11.397% (496/4352)\n",
            "34 391 Loss: 3.724 | Acc: 11.496% (515/4480)\n",
            "35 391 Loss: 3.683 | Acc: 11.545% (532/4608)\n",
            "36 391 Loss: 3.645 | Acc: 11.677% (553/4736)\n",
            "37 391 Loss: 3.611 | Acc: 11.719% (570/4864)\n",
            "38 391 Loss: 3.576 | Acc: 11.699% (584/4992)\n",
            "39 391 Loss: 3.549 | Acc: 11.914% (610/5120)\n",
            "40 391 Loss: 3.521 | Acc: 12.024% (631/5248)\n",
            "41 391 Loss: 3.503 | Acc: 12.016% (646/5376)\n",
            "42 391 Loss: 3.482 | Acc: 11.937% (657/5504)\n",
            "43 391 Loss: 3.457 | Acc: 11.985% (675/5632)\n",
            "44 391 Loss: 3.430 | Acc: 12.188% (702/5760)\n",
            "45 391 Loss: 3.411 | Acc: 12.313% (725/5888)\n",
            "46 391 Loss: 3.388 | Acc: 12.384% (745/6016)\n",
            "47 391 Loss: 3.373 | Acc: 12.516% (769/6144)\n",
            "48 391 Loss: 3.350 | Acc: 12.484% (783/6272)\n",
            "49 391 Loss: 3.340 | Acc: 12.484% (799/6400)\n",
            "50 391 Loss: 3.332 | Acc: 12.531% (818/6528)\n",
            "51 391 Loss: 3.327 | Acc: 12.635% (841/6656)\n",
            "52 391 Loss: 3.307 | Acc: 12.633% (857/6784)\n",
            "53 391 Loss: 3.287 | Acc: 12.601% (871/6912)\n",
            "54 391 Loss: 3.270 | Acc: 12.585% (886/7040)\n",
            "55 391 Loss: 3.263 | Acc: 12.640% (906/7168)\n",
            "56 391 Loss: 3.248 | Acc: 12.760% (931/7296)\n",
            "57 391 Loss: 3.238 | Acc: 12.796% (950/7424)\n",
            "58 391 Loss: 3.226 | Acc: 12.818% (968/7552)\n",
            "59 391 Loss: 3.212 | Acc: 12.878% (989/7680)\n",
            "60 391 Loss: 3.199 | Acc: 12.948% (1011/7808)\n",
            "61 391 Loss: 3.188 | Acc: 13.042% (1035/7936)\n",
            "62 391 Loss: 3.174 | Acc: 13.083% (1055/8064)\n",
            "63 391 Loss: 3.161 | Acc: 13.110% (1074/8192)\n",
            "64 391 Loss: 3.158 | Acc: 13.053% (1086/8320)\n",
            "65 391 Loss: 3.144 | Acc: 13.104% (1107/8448)\n",
            "66 391 Loss: 3.148 | Acc: 13.141% (1127/8576)\n",
            "67 391 Loss: 3.145 | Acc: 13.132% (1143/8704)\n",
            "68 391 Loss: 3.134 | Acc: 13.145% (1161/8832)\n",
            "69 391 Loss: 3.121 | Acc: 13.225% (1185/8960)\n",
            "70 391 Loss: 3.109 | Acc: 13.270% (1206/9088)\n",
            "71 391 Loss: 3.098 | Acc: 13.227% (1219/9216)\n",
            "72 391 Loss: 3.089 | Acc: 13.271% (1240/9344)\n",
            "73 391 Loss: 3.078 | Acc: 13.292% (1259/9472)\n",
            "74 391 Loss: 3.073 | Acc: 13.292% (1276/9600)\n",
            "75 391 Loss: 3.066 | Acc: 13.322% (1296/9728)\n",
            "76 391 Loss: 3.057 | Acc: 13.332% (1314/9856)\n",
            "77 391 Loss: 3.052 | Acc: 13.271% (1325/9984)\n",
            "78 391 Loss: 3.043 | Acc: 13.331% (1348/10112)\n",
            "79 391 Loss: 3.038 | Acc: 13.320% (1364/10240)\n",
            "80 391 Loss: 3.033 | Acc: 13.397% (1389/10368)\n",
            "81 391 Loss: 3.022 | Acc: 13.481% (1415/10496)\n",
            "82 391 Loss: 3.013 | Acc: 13.507% (1435/10624)\n",
            "83 391 Loss: 3.007 | Acc: 13.523% (1454/10752)\n",
            "84 391 Loss: 3.000 | Acc: 13.520% (1471/10880)\n",
            "85 391 Loss: 2.990 | Acc: 13.581% (1495/11008)\n",
            "86 391 Loss: 2.981 | Acc: 13.605% (1515/11136)\n",
            "87 391 Loss: 2.974 | Acc: 13.672% (1540/11264)\n",
            "88 391 Loss: 2.968 | Acc: 13.676% (1558/11392)\n",
            "89 391 Loss: 2.959 | Acc: 13.741% (1583/11520)\n",
            "90 391 Loss: 2.953 | Acc: 13.796% (1607/11648)\n",
            "91 391 Loss: 2.946 | Acc: 13.808% (1626/11776)\n",
            "92 391 Loss: 2.937 | Acc: 13.861% (1650/11904)\n",
            "93 391 Loss: 2.931 | Acc: 13.846% (1666/12032)\n",
            "94 391 Loss: 2.922 | Acc: 13.914% (1692/12160)\n",
            "95 391 Loss: 2.920 | Acc: 14.006% (1721/12288)\n",
            "96 391 Loss: 2.912 | Acc: 14.054% (1745/12416)\n",
            "97 391 Loss: 2.907 | Acc: 14.094% (1768/12544)\n",
            "98 391 Loss: 2.900 | Acc: 14.110% (1788/12672)\n",
            "99 391 Loss: 2.893 | Acc: 14.195% (1817/12800)\n",
            "100 391 Loss: 2.887 | Acc: 14.240% (1841/12928)\n",
            "101 391 Loss: 2.880 | Acc: 14.208% (1855/13056)\n",
            "102 391 Loss: 2.874 | Acc: 14.282% (1883/13184)\n",
            "103 391 Loss: 2.867 | Acc: 14.363% (1912/13312)\n",
            "104 391 Loss: 2.863 | Acc: 14.397% (1935/13440)\n",
            "105 391 Loss: 2.856 | Acc: 14.431% (1958/13568)\n",
            "106 391 Loss: 2.850 | Acc: 14.486% (1984/13696)\n",
            "107 391 Loss: 2.843 | Acc: 14.576% (2015/13824)\n",
            "108 391 Loss: 2.837 | Acc: 14.579% (2034/13952)\n",
            "109 391 Loss: 2.832 | Acc: 14.581% (2053/14080)\n",
            "110 391 Loss: 2.826 | Acc: 14.661% (2083/14208)\n",
            "111 391 Loss: 2.819 | Acc: 14.788% (2120/14336)\n",
            "112 391 Loss: 2.812 | Acc: 14.892% (2154/14464)\n",
            "113 391 Loss: 2.806 | Acc: 14.905% (2175/14592)\n",
            "114 391 Loss: 2.800 | Acc: 14.946% (2200/14720)\n",
            "115 391 Loss: 2.793 | Acc: 14.958% (2221/14848)\n",
            "116 391 Loss: 2.787 | Acc: 15.004% (2247/14976)\n",
            "117 391 Loss: 2.782 | Acc: 15.075% (2277/15104)\n",
            "118 391 Loss: 2.776 | Acc: 15.126% (2304/15232)\n",
            "119 391 Loss: 2.770 | Acc: 15.169% (2330/15360)\n",
            "120 391 Loss: 2.764 | Acc: 15.296% (2369/15488)\n",
            "121 391 Loss: 2.759 | Acc: 15.330% (2394/15616)\n",
            "122 391 Loss: 2.755 | Acc: 15.365% (2419/15744)\n",
            "123 391 Loss: 2.748 | Acc: 15.455% (2453/15872)\n",
            "124 391 Loss: 2.743 | Acc: 15.525% (2484/16000)\n",
            "125 391 Loss: 2.738 | Acc: 15.538% (2506/16128)\n",
            "126 391 Loss: 2.733 | Acc: 15.594% (2535/16256)\n",
            "127 391 Loss: 2.729 | Acc: 15.625% (2560/16384)\n",
            "128 391 Loss: 2.723 | Acc: 15.649% (2584/16512)\n",
            "129 391 Loss: 2.717 | Acc: 15.775% (2625/16640)\n",
            "130 391 Loss: 2.712 | Acc: 15.798% (2649/16768)\n",
            "131 391 Loss: 2.707 | Acc: 15.879% (2683/16896)\n",
            "132 391 Loss: 2.702 | Acc: 15.919% (2710/17024)\n",
            "133 391 Loss: 2.697 | Acc: 15.946% (2735/17152)\n",
            "134 391 Loss: 2.694 | Acc: 15.943% (2755/17280)\n",
            "135 391 Loss: 2.689 | Acc: 16.004% (2786/17408)\n",
            "136 391 Loss: 2.684 | Acc: 16.087% (2821/17536)\n",
            "137 391 Loss: 2.680 | Acc: 16.163% (2855/17664)\n",
            "138 391 Loss: 2.674 | Acc: 16.260% (2893/17792)\n",
            "139 391 Loss: 2.670 | Acc: 16.323% (2925/17920)\n",
            "140 391 Loss: 2.666 | Acc: 16.384% (2957/18048)\n",
            "141 391 Loss: 2.662 | Acc: 16.472% (2994/18176)\n",
            "142 391 Loss: 2.658 | Acc: 16.521% (3024/18304)\n",
            "143 391 Loss: 2.653 | Acc: 16.531% (3047/18432)\n",
            "144 391 Loss: 2.649 | Acc: 16.568% (3075/18560)\n",
            "145 391 Loss: 2.644 | Acc: 16.615% (3105/18688)\n",
            "146 391 Loss: 2.640 | Acc: 16.693% (3141/18816)\n",
            "147 391 Loss: 2.636 | Acc: 16.739% (3171/18944)\n",
            "148 391 Loss: 2.632 | Acc: 16.810% (3206/19072)\n",
            "149 391 Loss: 2.628 | Acc: 16.865% (3238/19200)\n",
            "150 391 Loss: 2.624 | Acc: 16.913% (3269/19328)\n",
            "151 391 Loss: 2.621 | Acc: 16.936% (3295/19456)\n",
            "152 391 Loss: 2.617 | Acc: 16.988% (3327/19584)\n",
            "153 391 Loss: 2.613 | Acc: 17.030% (3357/19712)\n",
            "154 391 Loss: 2.609 | Acc: 17.092% (3391/19840)\n",
            "155 391 Loss: 2.605 | Acc: 17.127% (3420/19968)\n",
            "156 391 Loss: 2.602 | Acc: 17.148% (3446/20096)\n",
            "157 391 Loss: 2.599 | Acc: 17.158% (3470/20224)\n",
            "158 391 Loss: 2.596 | Acc: 17.232% (3507/20352)\n",
            "159 391 Loss: 2.592 | Acc: 17.266% (3536/20480)\n",
            "160 391 Loss: 2.588 | Acc: 17.304% (3566/20608)\n",
            "161 391 Loss: 2.586 | Acc: 17.274% (3582/20736)\n",
            "162 391 Loss: 2.583 | Acc: 17.279% (3605/20864)\n",
            "163 391 Loss: 2.579 | Acc: 17.321% (3636/20992)\n",
            "164 391 Loss: 2.577 | Acc: 17.348% (3664/21120)\n",
            "165 391 Loss: 2.573 | Acc: 17.366% (3690/21248)\n",
            "166 391 Loss: 2.569 | Acc: 17.431% (3726/21376)\n",
            "167 391 Loss: 2.565 | Acc: 17.504% (3764/21504)\n",
            "168 391 Loss: 2.563 | Acc: 17.548% (3796/21632)\n",
            "169 391 Loss: 2.560 | Acc: 17.532% (3815/21760)\n",
            "170 391 Loss: 2.556 | Acc: 17.585% (3849/21888)\n",
            "171 391 Loss: 2.553 | Acc: 17.655% (3887/22016)\n",
            "172 391 Loss: 2.549 | Acc: 17.689% (3917/22144)\n",
            "173 391 Loss: 2.546 | Acc: 17.726% (3948/22272)\n",
            "174 391 Loss: 2.543 | Acc: 17.741% (3974/22400)\n",
            "175 391 Loss: 2.540 | Acc: 17.778% (4005/22528)\n",
            "176 391 Loss: 2.536 | Acc: 17.814% (4036/22656)\n",
            "177 391 Loss: 2.533 | Acc: 17.850% (4067/22784)\n",
            "178 391 Loss: 2.530 | Acc: 17.868% (4094/22912)\n",
            "179 391 Loss: 2.527 | Acc: 17.921% (4129/23040)\n",
            "180 391 Loss: 2.523 | Acc: 17.969% (4163/23168)\n",
            "181 391 Loss: 2.520 | Acc: 18.007% (4195/23296)\n",
            "182 391 Loss: 2.518 | Acc: 18.024% (4222/23424)\n",
            "183 391 Loss: 2.515 | Acc: 18.032% (4247/23552)\n",
            "184 391 Loss: 2.511 | Acc: 18.066% (4278/23680)\n",
            "185 391 Loss: 2.509 | Acc: 18.107% (4311/23808)\n",
            "186 391 Loss: 2.506 | Acc: 18.161% (4347/23936)\n",
            "187 391 Loss: 2.503 | Acc: 18.206% (4381/24064)\n",
            "188 391 Loss: 2.501 | Acc: 18.246% (4414/24192)\n",
            "189 391 Loss: 2.498 | Acc: 18.261% (4441/24320)\n",
            "190 391 Loss: 2.495 | Acc: 18.280% (4469/24448)\n",
            "191 391 Loss: 2.493 | Acc: 18.311% (4500/24576)\n",
            "192 391 Loss: 2.490 | Acc: 18.374% (4539/24704)\n",
            "193 391 Loss: 2.486 | Acc: 18.428% (4576/24832)\n",
            "194 391 Loss: 2.484 | Acc: 18.421% (4598/24960)\n",
            "195 391 Loss: 2.481 | Acc: 18.427% (4623/25088)\n",
            "196 391 Loss: 2.479 | Acc: 18.457% (4654/25216)\n",
            "197 391 Loss: 2.476 | Acc: 18.505% (4690/25344)\n",
            "198 391 Loss: 2.473 | Acc: 18.558% (4727/25472)\n",
            "199 391 Loss: 2.470 | Acc: 18.648% (4774/25600)\n",
            "200 391 Loss: 2.467 | Acc: 18.684% (4807/25728)\n",
            "201 391 Loss: 2.465 | Acc: 18.731% (4843/25856)\n",
            "202 391 Loss: 2.462 | Acc: 18.765% (4876/25984)\n",
            "203 391 Loss: 2.460 | Acc: 18.784% (4905/26112)\n",
            "204 391 Loss: 2.457 | Acc: 18.822% (4939/26240)\n",
            "205 391 Loss: 2.455 | Acc: 18.887% (4980/26368)\n",
            "206 391 Loss: 2.453 | Acc: 18.893% (5006/26496)\n",
            "207 391 Loss: 2.450 | Acc: 18.919% (5037/26624)\n",
            "208 391 Loss: 2.448 | Acc: 18.929% (5064/26752)\n",
            "209 391 Loss: 2.445 | Acc: 18.969% (5099/26880)\n",
            "210 391 Loss: 2.443 | Acc: 19.009% (5134/27008)\n",
            "211 391 Loss: 2.441 | Acc: 19.030% (5164/27136)\n",
            "212 391 Loss: 2.438 | Acc: 19.120% (5213/27264)\n",
            "213 391 Loss: 2.436 | Acc: 19.170% (5251/27392)\n",
            "214 391 Loss: 2.433 | Acc: 19.215% (5288/27520)\n",
            "215 391 Loss: 2.431 | Acc: 19.224% (5315/27648)\n",
            "216 391 Loss: 2.430 | Acc: 19.247% (5346/27776)\n",
            "217 391 Loss: 2.428 | Acc: 19.302% (5386/27904)\n",
            "218 391 Loss: 2.425 | Acc: 19.360% (5427/28032)\n",
            "219 391 Loss: 2.423 | Acc: 19.428% (5471/28160)\n",
            "220 391 Loss: 2.420 | Acc: 19.478% (5510/28288)\n",
            "221 391 Loss: 2.418 | Acc: 19.507% (5543/28416)\n",
            "222 391 Loss: 2.415 | Acc: 19.556% (5582/28544)\n",
            "223 391 Loss: 2.413 | Acc: 19.570% (5611/28672)\n",
            "224 391 Loss: 2.411 | Acc: 19.639% (5656/28800)\n",
            "225 391 Loss: 2.408 | Acc: 19.687% (5695/28928)\n",
            "226 391 Loss: 2.406 | Acc: 19.755% (5740/29056)\n",
            "227 391 Loss: 2.404 | Acc: 19.788% (5775/29184)\n",
            "228 391 Loss: 2.401 | Acc: 19.821% (5810/29312)\n",
            "229 391 Loss: 2.399 | Acc: 19.881% (5853/29440)\n",
            "230 391 Loss: 2.397 | Acc: 19.910% (5887/29568)\n",
            "231 391 Loss: 2.394 | Acc: 19.942% (5922/29696)\n",
            "232 391 Loss: 2.392 | Acc: 19.977% (5958/29824)\n",
            "233 391 Loss: 2.390 | Acc: 20.009% (5993/29952)\n",
            "234 391 Loss: 2.387 | Acc: 20.057% (6033/30080)\n",
            "235 391 Loss: 2.385 | Acc: 20.101% (6072/30208)\n",
            "236 391 Loss: 2.383 | Acc: 20.128% (6106/30336)\n",
            "237 391 Loss: 2.381 | Acc: 20.158% (6141/30464)\n",
            "238 391 Loss: 2.379 | Acc: 20.192% (6177/30592)\n",
            "239 391 Loss: 2.378 | Acc: 20.199% (6205/30720)\n",
            "240 391 Loss: 2.376 | Acc: 20.212% (6235/30848)\n",
            "241 391 Loss: 2.373 | Acc: 20.258% (6275/30976)\n",
            "242 391 Loss: 2.371 | Acc: 20.303% (6315/31104)\n",
            "243 391 Loss: 2.369 | Acc: 20.351% (6356/31232)\n",
            "244 391 Loss: 2.367 | Acc: 20.392% (6395/31360)\n",
            "245 391 Loss: 2.365 | Acc: 20.430% (6433/31488)\n",
            "246 391 Loss: 2.363 | Acc: 20.433% (6460/31616)\n",
            "247 391 Loss: 2.363 | Acc: 20.464% (6496/31744)\n",
            "248 391 Loss: 2.360 | Acc: 20.513% (6538/31872)\n",
            "249 391 Loss: 2.358 | Acc: 20.569% (6582/32000)\n",
            "250 391 Loss: 2.356 | Acc: 20.596% (6617/32128)\n",
            "251 391 Loss: 2.355 | Acc: 20.629% (6654/32256)\n",
            "252 391 Loss: 2.352 | Acc: 20.677% (6696/32384)\n",
            "253 391 Loss: 2.350 | Acc: 20.700% (6730/32512)\n",
            "254 391 Loss: 2.349 | Acc: 20.708% (6759/32640)\n",
            "255 391 Loss: 2.347 | Acc: 20.734% (6794/32768)\n",
            "256 391 Loss: 2.345 | Acc: 20.772% (6833/32896)\n",
            "257 391 Loss: 2.343 | Acc: 20.788% (6865/33024)\n",
            "258 391 Loss: 2.341 | Acc: 20.828% (6905/33152)\n",
            "259 391 Loss: 2.340 | Acc: 20.850% (6939/33280)\n",
            "260 391 Loss: 2.337 | Acc: 20.893% (6980/33408)\n",
            "261 391 Loss: 2.336 | Acc: 20.936% (7021/33536)\n",
            "262 391 Loss: 2.334 | Acc: 20.978% (7062/33664)\n",
            "263 391 Loss: 2.332 | Acc: 21.014% (7101/33792)\n",
            "264 391 Loss: 2.331 | Acc: 21.058% (7143/33920)\n",
            "265 391 Loss: 2.329 | Acc: 21.105% (7186/34048)\n",
            "266 391 Loss: 2.327 | Acc: 21.120% (7218/34176)\n",
            "267 391 Loss: 2.325 | Acc: 21.178% (7265/34304)\n",
            "268 391 Loss: 2.324 | Acc: 21.192% (7297/34432)\n",
            "269 391 Loss: 2.322 | Acc: 21.247% (7343/34560)\n",
            "270 391 Loss: 2.320 | Acc: 21.258% (7374/34688)\n",
            "271 391 Loss: 2.318 | Acc: 21.321% (7423/34816)\n",
            "272 391 Loss: 2.316 | Acc: 21.374% (7469/34944)\n",
            "273 391 Loss: 2.314 | Acc: 21.393% (7503/35072)\n",
            "274 391 Loss: 2.313 | Acc: 21.409% (7536/35200)\n",
            "275 391 Loss: 2.311 | Acc: 21.448% (7577/35328)\n",
            "276 391 Loss: 2.309 | Acc: 21.497% (7622/35456)\n",
            "277 391 Loss: 2.307 | Acc: 21.543% (7666/35584)\n",
            "278 391 Loss: 2.306 | Acc: 21.573% (7704/35712)\n",
            "279 391 Loss: 2.304 | Acc: 21.616% (7747/35840)\n",
            "280 391 Loss: 2.302 | Acc: 21.641% (7784/35968)\n",
            "281 391 Loss: 2.300 | Acc: 21.687% (7828/36096)\n",
            "282 391 Loss: 2.298 | Acc: 21.734% (7873/36224)\n",
            "283 391 Loss: 2.296 | Acc: 21.779% (7917/36352)\n",
            "284 391 Loss: 2.294 | Acc: 21.806% (7955/36480)\n",
            "285 391 Loss: 2.292 | Acc: 21.820% (7988/36608)\n",
            "286 391 Loss: 2.291 | Acc: 21.851% (8027/36736)\n",
            "287 391 Loss: 2.289 | Acc: 21.897% (8072/36864)\n",
            "288 391 Loss: 2.288 | Acc: 21.924% (8110/36992)\n",
            "289 391 Loss: 2.286 | Acc: 21.975% (8157/37120)\n",
            "290 391 Loss: 2.285 | Acc: 22.007% (8197/37248)\n",
            "291 391 Loss: 2.283 | Acc: 22.036% (8236/37376)\n",
            "292 391 Loss: 2.281 | Acc: 22.067% (8276/37504)\n",
            "293 391 Loss: 2.280 | Acc: 22.074% (8307/37632)\n",
            "294 391 Loss: 2.278 | Acc: 22.113% (8350/37760)\n",
            "295 391 Loss: 2.277 | Acc: 22.131% (8385/37888)\n",
            "296 391 Loss: 2.275 | Acc: 22.159% (8424/38016)\n",
            "297 391 Loss: 2.274 | Acc: 22.192% (8465/38144)\n",
            "298 391 Loss: 2.272 | Acc: 22.230% (8508/38272)\n",
            "299 391 Loss: 2.270 | Acc: 22.268% (8551/38400)\n",
            "300 391 Loss: 2.269 | Acc: 22.298% (8591/38528)\n",
            "301 391 Loss: 2.268 | Acc: 22.320% (8628/38656)\n",
            "302 391 Loss: 2.266 | Acc: 22.349% (8668/38784)\n",
            "303 391 Loss: 2.265 | Acc: 22.389% (8712/38912)\n",
            "304 391 Loss: 2.264 | Acc: 22.395% (8743/39040)\n",
            "305 391 Loss: 2.262 | Acc: 22.437% (8788/39168)\n",
            "306 391 Loss: 2.261 | Acc: 22.463% (8827/39296)\n",
            "307 391 Loss: 2.260 | Acc: 22.507% (8873/39424)\n",
            "308 391 Loss: 2.259 | Acc: 22.512% (8904/39552)\n",
            "309 391 Loss: 2.258 | Acc: 22.515% (8934/39680)\n",
            "310 391 Loss: 2.256 | Acc: 22.531% (8969/39808)\n",
            "311 391 Loss: 2.255 | Acc: 22.544% (9003/39936)\n",
            "312 391 Loss: 2.254 | Acc: 22.589% (9050/40064)\n",
            "313 391 Loss: 2.252 | Acc: 22.619% (9091/40192)\n",
            "314 391 Loss: 2.250 | Acc: 22.654% (9134/40320)\n",
            "315 391 Loss: 2.249 | Acc: 22.681% (9174/40448)\n",
            "316 391 Loss: 2.248 | Acc: 22.693% (9208/40576)\n",
            "317 391 Loss: 2.247 | Acc: 22.727% (9251/40704)\n",
            "318 391 Loss: 2.245 | Acc: 22.786% (9304/40832)\n",
            "319 391 Loss: 2.243 | Acc: 22.815% (9345/40960)\n",
            "320 391 Loss: 2.241 | Acc: 22.863% (9394/41088)\n",
            "321 391 Loss: 2.240 | Acc: 22.889% (9434/41216)\n",
            "322 391 Loss: 2.238 | Acc: 22.937% (9483/41344)\n",
            "323 391 Loss: 2.237 | Acc: 22.970% (9526/41472)\n",
            "324 391 Loss: 2.236 | Acc: 23.005% (9570/41600)\n",
            "325 391 Loss: 2.235 | Acc: 23.021% (9606/41728)\n",
            "326 391 Loss: 2.233 | Acc: 23.062% (9653/41856)\n",
            "327 391 Loss: 2.231 | Acc: 23.087% (9693/41984)\n",
            "328 391 Loss: 2.230 | Acc: 23.160% (9753/42112)\n",
            "329 391 Loss: 2.228 | Acc: 23.196% (9798/42240)\n",
            "330 391 Loss: 2.227 | Acc: 23.225% (9840/42368)\n",
            "331 391 Loss: 2.225 | Acc: 23.277% (9892/42496)\n",
            "332 391 Loss: 2.223 | Acc: 23.306% (9934/42624)\n",
            "333 391 Loss: 2.222 | Acc: 23.335% (9976/42752)\n",
            "334 391 Loss: 2.220 | Acc: 23.363% (10018/42880)\n",
            "335 391 Loss: 2.219 | Acc: 23.400% (10064/43008)\n",
            "336 391 Loss: 2.218 | Acc: 23.433% (10108/43136)\n",
            "337 391 Loss: 2.216 | Acc: 23.484% (10160/43264)\n",
            "338 391 Loss: 2.215 | Acc: 23.518% (10205/43392)\n",
            "339 391 Loss: 2.213 | Acc: 23.557% (10252/43520)\n",
            "340 391 Loss: 2.212 | Acc: 23.573% (10289/43648)\n",
            "341 391 Loss: 2.210 | Acc: 23.616% (10338/43776)\n",
            "342 391 Loss: 2.208 | Acc: 23.670% (10392/43904)\n",
            "343 391 Loss: 2.207 | Acc: 23.712% (10441/44032)\n",
            "344 391 Loss: 2.205 | Acc: 23.764% (10494/44160)\n",
            "345 391 Loss: 2.204 | Acc: 23.803% (10542/44288)\n",
            "346 391 Loss: 2.202 | Acc: 23.852% (10594/44416)\n",
            "347 391 Loss: 2.201 | Acc: 23.866% (10631/44544)\n",
            "348 391 Loss: 2.200 | Acc: 23.894% (10674/44672)\n",
            "349 391 Loss: 2.199 | Acc: 23.938% (10724/44800)\n",
            "350 391 Loss: 2.197 | Acc: 23.969% (10769/44928)\n",
            "351 391 Loss: 2.195 | Acc: 24.010% (10818/45056)\n",
            "352 391 Loss: 2.194 | Acc: 24.035% (10860/45184)\n",
            "353 391 Loss: 2.193 | Acc: 24.064% (10904/45312)\n",
            "354 391 Loss: 2.191 | Acc: 24.098% (10950/45440)\n",
            "355 391 Loss: 2.190 | Acc: 24.124% (10993/45568)\n",
            "356 391 Loss: 2.189 | Acc: 24.151% (11036/45696)\n",
            "357 391 Loss: 2.187 | Acc: 24.182% (11081/45824)\n",
            "358 391 Loss: 2.186 | Acc: 24.219% (11129/45952)\n",
            "359 391 Loss: 2.185 | Acc: 24.258% (11178/46080)\n",
            "360 391 Loss: 2.183 | Acc: 24.277% (11218/46208)\n",
            "361 391 Loss: 2.182 | Acc: 24.324% (11271/46336)\n",
            "362 391 Loss: 2.181 | Acc: 24.361% (11319/46464)\n",
            "363 391 Loss: 2.180 | Acc: 24.382% (11360/46592)\n",
            "364 391 Loss: 2.178 | Acc: 24.426% (11412/46720)\n",
            "365 391 Loss: 2.177 | Acc: 24.468% (11463/46848)\n",
            "366 391 Loss: 2.176 | Acc: 24.504% (11511/46976)\n",
            "367 391 Loss: 2.175 | Acc: 24.516% (11548/47104)\n",
            "368 391 Loss: 2.173 | Acc: 24.555% (11598/47232)\n",
            "369 391 Loss: 2.172 | Acc: 24.582% (11642/47360)\n",
            "370 391 Loss: 2.171 | Acc: 24.608% (11686/47488)\n",
            "371 391 Loss: 2.170 | Acc: 24.635% (11730/47616)\n",
            "372 391 Loss: 2.169 | Acc: 24.659% (11773/47744)\n",
            "373 391 Loss: 2.168 | Acc: 24.685% (11817/47872)\n",
            "374 391 Loss: 2.167 | Acc: 24.727% (11869/48000)\n",
            "375 391 Loss: 2.166 | Acc: 24.755% (11914/48128)\n",
            "376 391 Loss: 2.165 | Acc: 24.780% (11958/48256)\n",
            "377 391 Loss: 2.163 | Acc: 24.806% (12002/48384)\n",
            "378 391 Loss: 2.162 | Acc: 24.852% (12056/48512)\n",
            "379 391 Loss: 2.161 | Acc: 24.895% (12109/48640)\n",
            "380 391 Loss: 2.160 | Acc: 24.906% (12146/48768)\n",
            "381 391 Loss: 2.159 | Acc: 24.941% (12195/48896)\n",
            "382 391 Loss: 2.157 | Acc: 24.969% (12241/49024)\n",
            "383 391 Loss: 2.156 | Acc: 24.998% (12287/49152)\n",
            "384 391 Loss: 2.155 | Acc: 25.020% (12330/49280)\n",
            "385 391 Loss: 2.154 | Acc: 25.055% (12379/49408)\n",
            "386 391 Loss: 2.153 | Acc: 25.087% (12427/49536)\n",
            "387 391 Loss: 2.152 | Acc: 25.107% (12469/49664)\n",
            "388 391 Loss: 2.151 | Acc: 25.135% (12515/49792)\n",
            "389 391 Loss: 2.150 | Acc: 25.166% (12563/49920)\n",
            "390 391 Loss: 2.148 | Acc: 25.176% (12588/50000)\n",
            "0 100 Loss: 1.640 | Acc: 36.000% (36/100)\n",
            "1 100 Loss: 1.720 | Acc: 32.500% (65/200)\n",
            "2 100 Loss: 1.667 | Acc: 32.667% (98/300)\n",
            "3 100 Loss: 1.689 | Acc: 32.000% (128/400)\n",
            "4 100 Loss: 1.660 | Acc: 34.400% (172/500)\n",
            "5 100 Loss: 1.665 | Acc: 35.167% (211/600)\n",
            "6 100 Loss: 1.672 | Acc: 34.286% (240/700)\n",
            "7 100 Loss: 1.679 | Acc: 33.375% (267/800)\n",
            "8 100 Loss: 1.671 | Acc: 34.444% (310/900)\n",
            "9 100 Loss: 1.665 | Acc: 35.400% (354/1000)\n",
            "10 100 Loss: 1.667 | Acc: 35.455% (390/1100)\n",
            "11 100 Loss: 1.667 | Acc: 35.333% (424/1200)\n",
            "12 100 Loss: 1.672 | Acc: 35.231% (458/1300)\n",
            "13 100 Loss: 1.681 | Acc: 35.143% (492/1400)\n",
            "14 100 Loss: 1.674 | Acc: 35.467% (532/1500)\n",
            "15 100 Loss: 1.678 | Acc: 35.438% (567/1600)\n",
            "16 100 Loss: 1.674 | Acc: 36.059% (613/1700)\n",
            "17 100 Loss: 1.669 | Acc: 36.556% (658/1800)\n",
            "18 100 Loss: 1.667 | Acc: 37.000% (703/1900)\n",
            "19 100 Loss: 1.668 | Acc: 37.000% (740/2000)\n",
            "20 100 Loss: 1.669 | Acc: 36.810% (773/2100)\n",
            "21 100 Loss: 1.669 | Acc: 37.000% (814/2200)\n",
            "22 100 Loss: 1.665 | Acc: 37.348% (859/2300)\n",
            "23 100 Loss: 1.667 | Acc: 37.167% (892/2400)\n",
            "24 100 Loss: 1.662 | Acc: 37.160% (929/2500)\n",
            "25 100 Loss: 1.670 | Acc: 36.885% (959/2600)\n",
            "26 100 Loss: 1.671 | Acc: 36.741% (992/2700)\n",
            "27 100 Loss: 1.668 | Acc: 36.857% (1032/2800)\n",
            "28 100 Loss: 1.671 | Acc: 36.793% (1067/2900)\n",
            "29 100 Loss: 1.664 | Acc: 37.067% (1112/3000)\n",
            "30 100 Loss: 1.658 | Acc: 37.419% (1160/3100)\n",
            "31 100 Loss: 1.657 | Acc: 37.688% (1206/3200)\n",
            "32 100 Loss: 1.653 | Acc: 38.000% (1254/3300)\n",
            "33 100 Loss: 1.651 | Acc: 38.118% (1296/3400)\n",
            "34 100 Loss: 1.650 | Acc: 38.171% (1336/3500)\n",
            "35 100 Loss: 1.648 | Acc: 38.111% (1372/3600)\n",
            "36 100 Loss: 1.646 | Acc: 38.162% (1412/3700)\n",
            "37 100 Loss: 1.645 | Acc: 38.079% (1447/3800)\n",
            "38 100 Loss: 1.644 | Acc: 38.205% (1490/3900)\n",
            "39 100 Loss: 1.644 | Acc: 38.200% (1528/4000)\n",
            "40 100 Loss: 1.643 | Acc: 38.317% (1571/4100)\n",
            "41 100 Loss: 1.646 | Acc: 38.262% (1607/4200)\n",
            "42 100 Loss: 1.646 | Acc: 38.186% (1642/4300)\n",
            "43 100 Loss: 1.642 | Acc: 38.386% (1689/4400)\n",
            "44 100 Loss: 1.642 | Acc: 38.489% (1732/4500)\n",
            "45 100 Loss: 1.642 | Acc: 38.370% (1765/4600)\n",
            "46 100 Loss: 1.640 | Acc: 38.447% (1807/4700)\n",
            "47 100 Loss: 1.640 | Acc: 38.500% (1848/4800)\n",
            "48 100 Loss: 1.639 | Acc: 38.469% (1885/4900)\n",
            "49 100 Loss: 1.637 | Acc: 38.440% (1922/5000)\n",
            "50 100 Loss: 1.637 | Acc: 38.451% (1961/5100)\n",
            "51 100 Loss: 1.636 | Acc: 38.538% (2004/5200)\n",
            "52 100 Loss: 1.636 | Acc: 38.472% (2039/5300)\n",
            "53 100 Loss: 1.637 | Acc: 38.519% (2080/5400)\n",
            "54 100 Loss: 1.639 | Acc: 38.491% (2117/5500)\n",
            "55 100 Loss: 1.643 | Acc: 38.304% (2145/5600)\n",
            "56 100 Loss: 1.642 | Acc: 38.351% (2186/5700)\n",
            "57 100 Loss: 1.638 | Acc: 38.431% (2229/5800)\n",
            "58 100 Loss: 1.638 | Acc: 38.373% (2264/5900)\n",
            "59 100 Loss: 1.637 | Acc: 38.450% (2307/6000)\n",
            "60 100 Loss: 1.639 | Acc: 38.295% (2336/6100)\n",
            "61 100 Loss: 1.638 | Acc: 38.403% (2381/6200)\n",
            "62 100 Loss: 1.639 | Acc: 38.381% (2418/6300)\n",
            "63 100 Loss: 1.638 | Acc: 38.484% (2463/6400)\n",
            "64 100 Loss: 1.640 | Acc: 38.323% (2491/6500)\n",
            "65 100 Loss: 1.642 | Acc: 38.273% (2526/6600)\n",
            "66 100 Loss: 1.643 | Acc: 38.269% (2564/6700)\n",
            "67 100 Loss: 1.642 | Acc: 38.324% (2606/6800)\n",
            "68 100 Loss: 1.644 | Acc: 38.261% (2640/6900)\n",
            "69 100 Loss: 1.645 | Acc: 38.229% (2676/7000)\n",
            "70 100 Loss: 1.645 | Acc: 38.141% (2708/7100)\n",
            "71 100 Loss: 1.645 | Acc: 38.111% (2744/7200)\n",
            "72 100 Loss: 1.642 | Acc: 38.164% (2786/7300)\n",
            "73 100 Loss: 1.642 | Acc: 38.122% (2821/7400)\n",
            "74 100 Loss: 1.641 | Acc: 38.133% (2860/7500)\n",
            "75 100 Loss: 1.642 | Acc: 38.066% (2893/7600)\n",
            "76 100 Loss: 1.640 | Acc: 38.078% (2932/7700)\n",
            "77 100 Loss: 1.639 | Acc: 38.154% (2976/7800)\n",
            "78 100 Loss: 1.639 | Acc: 38.165% (3015/7900)\n",
            "79 100 Loss: 1.639 | Acc: 38.062% (3045/8000)\n",
            "80 100 Loss: 1.638 | Acc: 38.148% (3090/8100)\n",
            "81 100 Loss: 1.640 | Acc: 38.061% (3121/8200)\n",
            "82 100 Loss: 1.642 | Acc: 37.988% (3153/8300)\n",
            "83 100 Loss: 1.642 | Acc: 37.964% (3189/8400)\n",
            "84 100 Loss: 1.643 | Acc: 37.918% (3223/8500)\n",
            "85 100 Loss: 1.644 | Acc: 37.942% (3263/8600)\n",
            "86 100 Loss: 1.645 | Acc: 37.874% (3295/8700)\n",
            "87 100 Loss: 1.644 | Acc: 37.955% (3340/8800)\n",
            "88 100 Loss: 1.645 | Acc: 37.989% (3381/8900)\n",
            "89 100 Loss: 1.646 | Acc: 38.022% (3422/9000)\n",
            "90 100 Loss: 1.644 | Acc: 38.022% (3460/9100)\n",
            "91 100 Loss: 1.644 | Acc: 38.098% (3505/9200)\n",
            "92 100 Loss: 1.641 | Acc: 38.172% (3550/9300)\n",
            "93 100 Loss: 1.641 | Acc: 38.202% (3591/9400)\n",
            "94 100 Loss: 1.641 | Acc: 38.200% (3629/9500)\n",
            "95 100 Loss: 1.640 | Acc: 38.198% (3667/9600)\n",
            "96 100 Loss: 1.641 | Acc: 38.175% (3703/9700)\n",
            "97 100 Loss: 1.642 | Acc: 38.122% (3736/9800)\n",
            "98 100 Loss: 1.642 | Acc: 38.081% (3770/9900)\n",
            "99 100 Loss: 1.641 | Acc: 38.030% (3803/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            "0 391 Loss: 1.701 | Acc: 39.062% (50/128)\n",
            "1 391 Loss: 1.718 | Acc: 35.938% (92/256)\n",
            "2 391 Loss: 1.715 | Acc: 38.281% (147/384)\n",
            "3 391 Loss: 1.685 | Acc: 39.258% (201/512)\n",
            "4 391 Loss: 1.681 | Acc: 39.531% (253/640)\n",
            "5 391 Loss: 1.683 | Acc: 39.453% (303/768)\n",
            "6 391 Loss: 1.677 | Acc: 39.955% (358/896)\n",
            "7 391 Loss: 1.697 | Acc: 39.648% (406/1024)\n",
            "8 391 Loss: 1.701 | Acc: 39.670% (457/1152)\n",
            "9 391 Loss: 1.692 | Acc: 39.766% (509/1280)\n",
            "10 391 Loss: 1.683 | Acc: 40.057% (564/1408)\n",
            "11 391 Loss: 1.675 | Acc: 40.560% (623/1536)\n",
            "12 391 Loss: 1.674 | Acc: 40.445% (673/1664)\n",
            "13 391 Loss: 1.673 | Acc: 40.346% (723/1792)\n",
            "14 391 Loss: 1.666 | Acc: 40.208% (772/1920)\n",
            "15 391 Loss: 1.662 | Acc: 40.039% (820/2048)\n",
            "16 391 Loss: 1.662 | Acc: 39.936% (869/2176)\n",
            "17 391 Loss: 1.662 | Acc: 40.104% (924/2304)\n",
            "18 391 Loss: 1.663 | Acc: 39.803% (968/2432)\n",
            "19 391 Loss: 1.665 | Acc: 39.648% (1015/2560)\n",
            "20 391 Loss: 1.667 | Acc: 39.695% (1067/2688)\n",
            "21 391 Loss: 1.668 | Acc: 39.631% (1116/2816)\n",
            "22 391 Loss: 1.664 | Acc: 39.776% (1171/2944)\n",
            "23 391 Loss: 1.659 | Acc: 39.746% (1221/3072)\n",
            "24 391 Loss: 1.657 | Acc: 40.000% (1280/3200)\n",
            "25 391 Loss: 1.655 | Acc: 40.054% (1333/3328)\n",
            "26 391 Loss: 1.657 | Acc: 39.815% (1376/3456)\n",
            "27 391 Loss: 1.656 | Acc: 39.760% (1425/3584)\n",
            "28 391 Loss: 1.656 | Acc: 39.628% (1471/3712)\n",
            "29 391 Loss: 1.652 | Acc: 39.661% (1523/3840)\n",
            "30 391 Loss: 1.652 | Acc: 39.541% (1569/3968)\n",
            "31 391 Loss: 1.655 | Acc: 39.502% (1618/4096)\n",
            "32 391 Loss: 1.653 | Acc: 39.560% (1671/4224)\n",
            "33 391 Loss: 1.661 | Acc: 39.476% (1718/4352)\n",
            "34 391 Loss: 1.663 | Acc: 39.330% (1762/4480)\n",
            "35 391 Loss: 1.665 | Acc: 39.345% (1813/4608)\n",
            "36 391 Loss: 1.669 | Acc: 39.274% (1860/4736)\n",
            "37 391 Loss: 1.669 | Acc: 39.289% (1911/4864)\n",
            "38 391 Loss: 1.668 | Acc: 39.263% (1960/4992)\n",
            "39 391 Loss: 1.667 | Acc: 39.297% (2012/5120)\n",
            "40 391 Loss: 1.667 | Acc: 39.310% (2063/5248)\n",
            "41 391 Loss: 1.668 | Acc: 39.100% (2102/5376)\n",
            "42 391 Loss: 1.667 | Acc: 39.153% (2155/5504)\n",
            "43 391 Loss: 1.665 | Acc: 39.222% (2209/5632)\n",
            "44 391 Loss: 1.663 | Acc: 39.340% (2266/5760)\n",
            "45 391 Loss: 1.662 | Acc: 39.368% (2318/5888)\n",
            "46 391 Loss: 1.663 | Acc: 39.428% (2372/6016)\n",
            "47 391 Loss: 1.659 | Acc: 39.600% (2433/6144)\n",
            "48 391 Loss: 1.660 | Acc: 39.573% (2482/6272)\n",
            "49 391 Loss: 1.658 | Acc: 39.562% (2532/6400)\n",
            "50 391 Loss: 1.655 | Acc: 39.752% (2595/6528)\n",
            "51 391 Loss: 1.653 | Acc: 39.769% (2647/6656)\n",
            "52 391 Loss: 1.651 | Acc: 39.858% (2704/6784)\n",
            "53 391 Loss: 1.654 | Acc: 39.815% (2752/6912)\n",
            "54 391 Loss: 1.654 | Acc: 39.787% (2801/7040)\n",
            "55 391 Loss: 1.653 | Acc: 39.760% (2850/7168)\n",
            "56 391 Loss: 1.654 | Acc: 39.762% (2901/7296)\n",
            "57 391 Loss: 1.653 | Acc: 39.763% (2952/7424)\n",
            "58 391 Loss: 1.652 | Acc: 39.791% (3005/7552)\n",
            "59 391 Loss: 1.653 | Acc: 39.753% (3053/7680)\n",
            "60 391 Loss: 1.653 | Acc: 39.677% (3098/7808)\n",
            "61 391 Loss: 1.653 | Acc: 39.630% (3145/7936)\n",
            "62 391 Loss: 1.655 | Acc: 39.534% (3188/8064)\n",
            "63 391 Loss: 1.656 | Acc: 39.441% (3231/8192)\n",
            "64 391 Loss: 1.655 | Acc: 39.435% (3281/8320)\n",
            "65 391 Loss: 1.654 | Acc: 39.429% (3331/8448)\n",
            "66 391 Loss: 1.655 | Acc: 39.354% (3375/8576)\n",
            "67 391 Loss: 1.656 | Acc: 39.281% (3419/8704)\n",
            "68 391 Loss: 1.656 | Acc: 39.244% (3466/8832)\n",
            "69 391 Loss: 1.654 | Acc: 39.330% (3524/8960)\n",
            "70 391 Loss: 1.653 | Acc: 39.459% (3586/9088)\n",
            "71 391 Loss: 1.655 | Acc: 39.290% (3621/9216)\n",
            "72 391 Loss: 1.653 | Acc: 39.309% (3673/9344)\n",
            "73 391 Loss: 1.651 | Acc: 39.379% (3730/9472)\n",
            "74 391 Loss: 1.650 | Acc: 39.323% (3775/9600)\n",
            "75 391 Loss: 1.651 | Acc: 39.330% (3826/9728)\n",
            "76 391 Loss: 1.650 | Acc: 39.397% (3883/9856)\n",
            "77 391 Loss: 1.648 | Acc: 39.413% (3935/9984)\n",
            "78 391 Loss: 1.648 | Acc: 39.458% (3990/10112)\n",
            "79 391 Loss: 1.647 | Acc: 39.482% (4043/10240)\n",
            "80 391 Loss: 1.645 | Acc: 39.516% (4097/10368)\n",
            "81 391 Loss: 1.642 | Acc: 39.663% (4163/10496)\n",
            "82 391 Loss: 1.643 | Acc: 39.637% (4211/10624)\n",
            "83 391 Loss: 1.645 | Acc: 39.509% (4248/10752)\n",
            "84 391 Loss: 1.644 | Acc: 39.522% (4300/10880)\n",
            "85 391 Loss: 1.644 | Acc: 39.571% (4356/11008)\n",
            "86 391 Loss: 1.642 | Acc: 39.574% (4407/11136)\n",
            "87 391 Loss: 1.642 | Acc: 39.524% (4452/11264)\n",
            "88 391 Loss: 1.644 | Acc: 39.493% (4499/11392)\n",
            "89 391 Loss: 1.644 | Acc: 39.488% (4549/11520)\n",
            "90 391 Loss: 1.642 | Acc: 39.466% (4597/11648)\n",
            "91 391 Loss: 1.641 | Acc: 39.521% (4654/11776)\n",
            "92 391 Loss: 1.640 | Acc: 39.474% (4699/11904)\n",
            "93 391 Loss: 1.639 | Acc: 39.461% (4748/12032)\n",
            "94 391 Loss: 1.640 | Acc: 39.507% (4804/12160)\n",
            "95 391 Loss: 1.638 | Acc: 39.600% (4866/12288)\n",
            "96 391 Loss: 1.639 | Acc: 39.610% (4918/12416)\n",
            "97 391 Loss: 1.638 | Acc: 39.581% (4965/12544)\n",
            "98 391 Loss: 1.638 | Acc: 39.552% (5012/12672)\n",
            "99 391 Loss: 1.637 | Acc: 39.570% (5065/12800)\n",
            "100 391 Loss: 1.637 | Acc: 39.581% (5117/12928)\n",
            "101 391 Loss: 1.636 | Acc: 39.591% (5169/13056)\n",
            "102 391 Loss: 1.635 | Acc: 39.601% (5221/13184)\n",
            "103 391 Loss: 1.634 | Acc: 39.626% (5275/13312)\n",
            "104 391 Loss: 1.633 | Acc: 39.650% (5329/13440)\n",
            "105 391 Loss: 1.631 | Acc: 39.733% (5391/13568)\n",
            "106 391 Loss: 1.632 | Acc: 39.712% (5439/13696)\n",
            "107 391 Loss: 1.632 | Acc: 39.706% (5489/13824)\n",
            "108 391 Loss: 1.632 | Acc: 39.686% (5537/13952)\n",
            "109 391 Loss: 1.632 | Acc: 39.723% (5593/14080)\n",
            "110 391 Loss: 1.632 | Acc: 39.738% (5646/14208)\n",
            "111 391 Loss: 1.631 | Acc: 39.767% (5701/14336)\n",
            "112 391 Loss: 1.631 | Acc: 39.747% (5749/14464)\n",
            "113 391 Loss: 1.631 | Acc: 39.741% (5799/14592)\n",
            "114 391 Loss: 1.631 | Acc: 39.708% (5845/14720)\n",
            "115 391 Loss: 1.630 | Acc: 39.702% (5895/14848)\n",
            "116 391 Loss: 1.629 | Acc: 39.717% (5948/14976)\n",
            "117 391 Loss: 1.627 | Acc: 39.817% (6014/15104)\n",
            "118 391 Loss: 1.626 | Acc: 39.870% (6073/15232)\n",
            "119 391 Loss: 1.625 | Acc: 39.883% (6126/15360)\n",
            "120 391 Loss: 1.625 | Acc: 39.889% (6178/15488)\n",
            "121 391 Loss: 1.626 | Acc: 39.825% (6219/15616)\n",
            "122 391 Loss: 1.625 | Acc: 39.876% (6278/15744)\n",
            "123 391 Loss: 1.625 | Acc: 39.900% (6333/15872)\n",
            "124 391 Loss: 1.625 | Acc: 39.856% (6377/16000)\n",
            "125 391 Loss: 1.624 | Acc: 39.881% (6432/16128)\n",
            "126 391 Loss: 1.623 | Acc: 39.881% (6483/16256)\n",
            "127 391 Loss: 1.625 | Acc: 39.874% (6533/16384)\n",
            "128 391 Loss: 1.624 | Acc: 39.898% (6588/16512)\n",
            "129 391 Loss: 1.623 | Acc: 39.970% (6651/16640)\n",
            "130 391 Loss: 1.623 | Acc: 39.939% (6697/16768)\n",
            "131 391 Loss: 1.624 | Acc: 39.856% (6734/16896)\n",
            "132 391 Loss: 1.625 | Acc: 39.861% (6786/17024)\n",
            "133 391 Loss: 1.623 | Acc: 39.920% (6847/17152)\n",
            "134 391 Loss: 1.623 | Acc: 39.925% (6899/17280)\n",
            "135 391 Loss: 1.623 | Acc: 39.878% (6942/17408)\n",
            "136 391 Loss: 1.622 | Acc: 39.861% (6990/17536)\n",
            "137 391 Loss: 1.621 | Acc: 39.923% (7052/17664)\n",
            "138 391 Loss: 1.621 | Acc: 39.934% (7105/17792)\n",
            "139 391 Loss: 1.620 | Acc: 39.950% (7159/17920)\n",
            "140 391 Loss: 1.619 | Acc: 40.021% (7223/18048)\n",
            "141 391 Loss: 1.618 | Acc: 40.075% (7284/18176)\n",
            "142 391 Loss: 1.617 | Acc: 40.166% (7352/18304)\n",
            "143 391 Loss: 1.616 | Acc: 40.213% (7412/18432)\n",
            "144 391 Loss: 1.616 | Acc: 40.178% (7457/18560)\n",
            "145 391 Loss: 1.617 | Acc: 40.149% (7503/18688)\n",
            "146 391 Loss: 1.617 | Acc: 40.104% (7546/18816)\n",
            "147 391 Loss: 1.616 | Acc: 40.155% (7607/18944)\n",
            "148 391 Loss: 1.616 | Acc: 40.153% (7658/19072)\n",
            "149 391 Loss: 1.617 | Acc: 40.115% (7702/19200)\n",
            "150 391 Loss: 1.617 | Acc: 40.087% (7748/19328)\n",
            "151 391 Loss: 1.617 | Acc: 40.085% (7799/19456)\n",
            "152 391 Loss: 1.617 | Acc: 40.104% (7854/19584)\n",
            "153 391 Loss: 1.615 | Acc: 40.179% (7920/19712)\n",
            "154 391 Loss: 1.615 | Acc: 40.207% (7977/19840)\n",
            "155 391 Loss: 1.616 | Acc: 40.189% (8025/19968)\n",
            "156 391 Loss: 1.617 | Acc: 40.157% (8070/20096)\n",
            "157 391 Loss: 1.617 | Acc: 40.160% (8122/20224)\n",
            "158 391 Loss: 1.616 | Acc: 40.212% (8184/20352)\n",
            "159 391 Loss: 1.615 | Acc: 40.215% (8236/20480)\n",
            "160 391 Loss: 1.615 | Acc: 40.198% (8284/20608)\n",
            "161 391 Loss: 1.614 | Acc: 40.234% (8343/20736)\n",
            "162 391 Loss: 1.614 | Acc: 40.251% (8398/20864)\n",
            "163 391 Loss: 1.614 | Acc: 40.282% (8456/20992)\n",
            "164 391 Loss: 1.614 | Acc: 40.279% (8507/21120)\n",
            "165 391 Loss: 1.615 | Acc: 40.263% (8555/21248)\n",
            "166 391 Loss: 1.615 | Acc: 40.181% (8589/21376)\n",
            "167 391 Loss: 1.615 | Acc: 40.202% (8645/21504)\n",
            "168 391 Loss: 1.616 | Acc: 40.126% (8680/21632)\n",
            "169 391 Loss: 1.616 | Acc: 40.147% (8736/21760)\n",
            "170 391 Loss: 1.615 | Acc: 40.168% (8792/21888)\n",
            "171 391 Loss: 1.615 | Acc: 40.175% (8845/22016)\n",
            "172 391 Loss: 1.615 | Acc: 40.169% (8895/22144)\n",
            "173 391 Loss: 1.615 | Acc: 40.172% (8947/22272)\n",
            "174 391 Loss: 1.615 | Acc: 40.143% (8992/22400)\n",
            "175 391 Loss: 1.616 | Acc: 40.123% (9039/22528)\n",
            "176 391 Loss: 1.616 | Acc: 40.148% (9096/22656)\n",
            "177 391 Loss: 1.616 | Acc: 40.147% (9147/22784)\n",
            "178 391 Loss: 1.616 | Acc: 40.136% (9196/22912)\n",
            "179 391 Loss: 1.616 | Acc: 40.135% (9247/23040)\n",
            "180 391 Loss: 1.616 | Acc: 40.159% (9304/23168)\n",
            "181 391 Loss: 1.616 | Acc: 40.166% (9357/23296)\n",
            "182 391 Loss: 1.616 | Acc: 40.172% (9410/23424)\n",
            "183 391 Loss: 1.616 | Acc: 40.179% (9463/23552)\n",
            "184 391 Loss: 1.615 | Acc: 40.194% (9518/23680)\n",
            "185 391 Loss: 1.616 | Acc: 40.134% (9555/23808)\n",
            "186 391 Loss: 1.616 | Acc: 40.161% (9613/23936)\n",
            "187 391 Loss: 1.615 | Acc: 40.176% (9668/24064)\n",
            "188 391 Loss: 1.614 | Acc: 40.199% (9725/24192)\n",
            "189 391 Loss: 1.614 | Acc: 40.201% (9777/24320)\n",
            "190 391 Loss: 1.613 | Acc: 40.216% (9832/24448)\n",
            "191 391 Loss: 1.613 | Acc: 40.177% (9874/24576)\n",
            "192 391 Loss: 1.612 | Acc: 40.240% (9941/24704)\n",
            "193 391 Loss: 1.612 | Acc: 40.246% (9994/24832)\n",
            "194 391 Loss: 1.612 | Acc: 40.296% (10058/24960)\n",
            "195 391 Loss: 1.611 | Acc: 40.330% (10118/25088)\n",
            "196 391 Loss: 1.611 | Acc: 40.343% (10173/25216)\n",
            "197 391 Loss: 1.610 | Acc: 40.345% (10225/25344)\n",
            "198 391 Loss: 1.610 | Acc: 40.342% (10276/25472)\n",
            "199 391 Loss: 1.611 | Acc: 40.359% (10332/25600)\n",
            "200 391 Loss: 1.610 | Acc: 40.384% (10390/25728)\n",
            "201 391 Loss: 1.610 | Acc: 40.358% (10435/25856)\n",
            "202 391 Loss: 1.609 | Acc: 40.375% (10491/25984)\n",
            "203 391 Loss: 1.610 | Acc: 40.353% (10537/26112)\n",
            "204 391 Loss: 1.610 | Acc: 40.373% (10594/26240)\n",
            "205 391 Loss: 1.610 | Acc: 40.409% (10655/26368)\n",
            "206 391 Loss: 1.610 | Acc: 40.395% (10703/26496)\n",
            "207 391 Loss: 1.610 | Acc: 40.415% (10760/26624)\n",
            "208 391 Loss: 1.609 | Acc: 40.446% (10820/26752)\n",
            "209 391 Loss: 1.609 | Acc: 40.420% (10865/26880)\n",
            "210 391 Loss: 1.609 | Acc: 40.469% (10930/27008)\n",
            "211 391 Loss: 1.608 | Acc: 40.500% (10990/27136)\n",
            "212 391 Loss: 1.608 | Acc: 40.544% (11054/27264)\n",
            "213 391 Loss: 1.608 | Acc: 40.541% (11105/27392)\n",
            "214 391 Loss: 1.607 | Acc: 40.574% (11166/27520)\n",
            "215 391 Loss: 1.607 | Acc: 40.589% (11222/27648)\n",
            "216 391 Loss: 1.606 | Acc: 40.611% (11280/27776)\n",
            "217 391 Loss: 1.605 | Acc: 40.654% (11344/27904)\n",
            "218 391 Loss: 1.605 | Acc: 40.682% (11404/28032)\n",
            "219 391 Loss: 1.605 | Acc: 40.678% (11455/28160)\n",
            "220 391 Loss: 1.605 | Acc: 40.689% (11510/28288)\n",
            "221 391 Loss: 1.604 | Acc: 40.688% (11562/28416)\n",
            "222 391 Loss: 1.604 | Acc: 40.709% (11620/28544)\n",
            "223 391 Loss: 1.604 | Acc: 40.716% (11674/28672)\n",
            "224 391 Loss: 1.604 | Acc: 40.712% (11725/28800)\n",
            "225 391 Loss: 1.604 | Acc: 40.739% (11785/28928)\n",
            "226 391 Loss: 1.604 | Acc: 40.766% (11845/29056)\n",
            "227 391 Loss: 1.603 | Acc: 40.807% (11909/29184)\n",
            "228 391 Loss: 1.603 | Acc: 40.779% (11953/29312)\n",
            "229 391 Loss: 1.602 | Acc: 40.791% (12009/29440)\n",
            "230 391 Loss: 1.602 | Acc: 40.808% (12066/29568)\n",
            "231 391 Loss: 1.601 | Acc: 40.851% (12131/29696)\n",
            "232 391 Loss: 1.602 | Acc: 40.826% (12176/29824)\n",
            "233 391 Loss: 1.603 | Acc: 40.812% (12224/29952)\n",
            "234 391 Loss: 1.603 | Acc: 40.818% (12278/30080)\n",
            "235 391 Loss: 1.603 | Acc: 40.840% (12337/30208)\n",
            "236 391 Loss: 1.602 | Acc: 40.846% (12391/30336)\n",
            "237 391 Loss: 1.602 | Acc: 40.845% (12443/30464)\n",
            "238 391 Loss: 1.602 | Acc: 40.851% (12497/30592)\n",
            "239 391 Loss: 1.601 | Acc: 40.850% (12549/30720)\n",
            "240 391 Loss: 1.601 | Acc: 40.878% (12610/30848)\n",
            "241 391 Loss: 1.600 | Acc: 40.896% (12668/30976)\n",
            "242 391 Loss: 1.599 | Acc: 40.930% (12731/31104)\n",
            "243 391 Loss: 1.599 | Acc: 40.932% (12784/31232)\n",
            "244 391 Loss: 1.599 | Acc: 40.938% (12838/31360)\n",
            "245 391 Loss: 1.598 | Acc: 40.958% (12897/31488)\n",
            "246 391 Loss: 1.598 | Acc: 40.960% (12950/31616)\n",
            "247 391 Loss: 1.598 | Acc: 40.959% (13002/31744)\n",
            "248 391 Loss: 1.599 | Acc: 40.948% (13051/31872)\n",
            "249 391 Loss: 1.598 | Acc: 40.978% (13113/32000)\n",
            "250 391 Loss: 1.597 | Acc: 41.014% (13177/32128)\n",
            "251 391 Loss: 1.597 | Acc: 41.019% (13231/32256)\n",
            "252 391 Loss: 1.596 | Acc: 41.045% (13292/32384)\n",
            "253 391 Loss: 1.596 | Acc: 41.077% (13355/32512)\n",
            "254 391 Loss: 1.596 | Acc: 41.115% (13420/32640)\n",
            "255 391 Loss: 1.595 | Acc: 41.153% (13485/32768)\n",
            "256 391 Loss: 1.594 | Acc: 41.169% (13543/32896)\n",
            "257 391 Loss: 1.594 | Acc: 41.188% (13602/33024)\n",
            "258 391 Loss: 1.592 | Acc: 41.240% (13672/33152)\n",
            "259 391 Loss: 1.592 | Acc: 41.265% (13733/33280)\n",
            "260 391 Loss: 1.592 | Acc: 41.254% (13782/33408)\n",
            "261 391 Loss: 1.591 | Acc: 41.266% (13839/33536)\n",
            "262 391 Loss: 1.590 | Acc: 41.314% (13908/33664)\n",
            "263 391 Loss: 1.590 | Acc: 41.303% (13957/33792)\n",
            "264 391 Loss: 1.589 | Acc: 41.335% (14021/33920)\n",
            "265 391 Loss: 1.589 | Acc: 41.356% (14081/34048)\n",
            "266 391 Loss: 1.589 | Acc: 41.374% (14140/34176)\n",
            "267 391 Loss: 1.589 | Acc: 41.374% (14193/34304)\n",
            "268 391 Loss: 1.588 | Acc: 41.406% (14257/34432)\n",
            "269 391 Loss: 1.588 | Acc: 41.415% (14313/34560)\n",
            "270 391 Loss: 1.588 | Acc: 41.424% (14369/34688)\n",
            "271 391 Loss: 1.587 | Acc: 41.438% (14427/34816)\n",
            "272 391 Loss: 1.587 | Acc: 41.429% (14477/34944)\n",
            "273 391 Loss: 1.586 | Acc: 41.438% (14533/35072)\n",
            "274 391 Loss: 1.586 | Acc: 41.429% (14583/35200)\n",
            "275 391 Loss: 1.586 | Acc: 41.457% (14646/35328)\n",
            "276 391 Loss: 1.585 | Acc: 41.488% (14710/35456)\n",
            "277 391 Loss: 1.585 | Acc: 41.488% (14763/35584)\n",
            "278 391 Loss: 1.585 | Acc: 41.507% (14823/35712)\n",
            "279 391 Loss: 1.585 | Acc: 41.521% (14881/35840)\n",
            "280 391 Loss: 1.584 | Acc: 41.515% (14932/35968)\n",
            "281 391 Loss: 1.584 | Acc: 41.553% (14999/36096)\n",
            "282 391 Loss: 1.583 | Acc: 41.575% (15060/36224)\n",
            "283 391 Loss: 1.583 | Acc: 41.560% (15108/36352)\n",
            "284 391 Loss: 1.584 | Acc: 41.549% (15157/36480)\n",
            "285 391 Loss: 1.583 | Acc: 41.562% (15215/36608)\n",
            "286 391 Loss: 1.583 | Acc: 41.561% (15268/36736)\n",
            "287 391 Loss: 1.583 | Acc: 41.566% (15323/36864)\n",
            "288 391 Loss: 1.582 | Acc: 41.620% (15396/36992)\n",
            "289 391 Loss: 1.581 | Acc: 41.627% (15452/37120)\n",
            "290 391 Loss: 1.582 | Acc: 41.608% (15498/37248)\n",
            "291 391 Loss: 1.581 | Acc: 41.631% (15560/37376)\n",
            "292 391 Loss: 1.581 | Acc: 41.628% (15612/37504)\n",
            "293 391 Loss: 1.581 | Acc: 41.651% (15674/37632)\n",
            "294 391 Loss: 1.581 | Acc: 41.655% (15729/37760)\n",
            "295 391 Loss: 1.580 | Acc: 41.689% (15795/37888)\n",
            "296 391 Loss: 1.580 | Acc: 41.706% (15855/38016)\n",
            "297 391 Loss: 1.579 | Acc: 41.710% (15910/38144)\n",
            "298 391 Loss: 1.579 | Acc: 41.728% (15970/38272)\n",
            "299 391 Loss: 1.578 | Acc: 41.779% (16043/38400)\n",
            "300 391 Loss: 1.577 | Acc: 41.796% (16103/38528)\n",
            "301 391 Loss: 1.576 | Acc: 41.838% (16173/38656)\n",
            "302 391 Loss: 1.576 | Acc: 41.850% (16231/38784)\n",
            "303 391 Loss: 1.575 | Acc: 41.866% (16291/38912)\n",
            "304 391 Loss: 1.575 | Acc: 41.875% (16348/39040)\n",
            "305 391 Loss: 1.575 | Acc: 41.884% (16405/39168)\n",
            "306 391 Loss: 1.574 | Acc: 41.887% (16460/39296)\n",
            "307 391 Loss: 1.575 | Acc: 41.886% (16513/39424)\n",
            "308 391 Loss: 1.574 | Acc: 41.909% (16576/39552)\n",
            "309 391 Loss: 1.574 | Acc: 41.918% (16633/39680)\n",
            "310 391 Loss: 1.573 | Acc: 41.954% (16701/39808)\n",
            "311 391 Loss: 1.573 | Acc: 41.972% (16762/39936)\n",
            "312 391 Loss: 1.572 | Acc: 41.975% (16817/40064)\n",
            "313 391 Loss: 1.573 | Acc: 41.974% (16870/40192)\n",
            "314 391 Loss: 1.572 | Acc: 41.994% (16932/40320)\n",
            "315 391 Loss: 1.572 | Acc: 41.975% (16978/40448)\n",
            "316 391 Loss: 1.572 | Acc: 41.983% (17035/40576)\n",
            "317 391 Loss: 1.572 | Acc: 41.991% (17092/40704)\n",
            "318 391 Loss: 1.572 | Acc: 42.006% (17152/40832)\n",
            "319 391 Loss: 1.572 | Acc: 42.021% (17212/40960)\n",
            "320 391 Loss: 1.572 | Acc: 42.034% (17271/41088)\n",
            "321 391 Loss: 1.572 | Acc: 42.049% (17331/41216)\n",
            "322 391 Loss: 1.572 | Acc: 42.064% (17391/41344)\n",
            "323 391 Loss: 1.571 | Acc: 42.079% (17451/41472)\n",
            "324 391 Loss: 1.571 | Acc: 42.127% (17525/41600)\n",
            "325 391 Loss: 1.571 | Acc: 42.120% (17576/41728)\n",
            "326 391 Loss: 1.571 | Acc: 42.111% (17626/41856)\n",
            "327 391 Loss: 1.570 | Acc: 42.116% (17682/41984)\n",
            "328 391 Loss: 1.570 | Acc: 42.138% (17745/42112)\n",
            "329 391 Loss: 1.570 | Acc: 42.128% (17795/42240)\n",
            "330 391 Loss: 1.569 | Acc: 42.169% (17866/42368)\n",
            "331 391 Loss: 1.569 | Acc: 42.180% (17925/42496)\n",
            "332 391 Loss: 1.568 | Acc: 42.190% (17983/42624)\n",
            "333 391 Loss: 1.568 | Acc: 42.211% (18046/42752)\n",
            "334 391 Loss: 1.567 | Acc: 42.211% (18100/42880)\n",
            "335 391 Loss: 1.567 | Acc: 42.241% (18167/43008)\n",
            "336 391 Loss: 1.566 | Acc: 42.269% (18233/43136)\n",
            "337 391 Loss: 1.565 | Acc: 42.303% (18302/43264)\n",
            "338 391 Loss: 1.564 | Acc: 42.328% (18367/43392)\n",
            "339 391 Loss: 1.564 | Acc: 42.351% (18431/43520)\n",
            "340 391 Loss: 1.563 | Acc: 42.387% (18501/43648)\n",
            "341 391 Loss: 1.562 | Acc: 42.402% (18562/43776)\n",
            "342 391 Loss: 1.562 | Acc: 42.431% (18629/43904)\n",
            "343 391 Loss: 1.561 | Acc: 42.424% (18680/44032)\n",
            "344 391 Loss: 1.560 | Acc: 42.450% (18746/44160)\n",
            "345 391 Loss: 1.560 | Acc: 42.467% (18808/44288)\n",
            "346 391 Loss: 1.560 | Acc: 42.480% (18868/44416)\n",
            "347 391 Loss: 1.560 | Acc: 42.482% (18923/44544)\n",
            "348 391 Loss: 1.560 | Acc: 42.494% (18983/44672)\n",
            "349 391 Loss: 1.560 | Acc: 42.502% (19041/44800)\n",
            "350 391 Loss: 1.559 | Acc: 42.537% (19111/44928)\n",
            "351 391 Loss: 1.559 | Acc: 42.534% (19164/45056)\n",
            "352 391 Loss: 1.559 | Acc: 42.553% (19227/45184)\n",
            "353 391 Loss: 1.559 | Acc: 42.563% (19286/45312)\n",
            "354 391 Loss: 1.559 | Acc: 42.555% (19337/45440)\n",
            "355 391 Loss: 1.558 | Acc: 42.567% (19397/45568)\n",
            "356 391 Loss: 1.558 | Acc: 42.549% (19443/45696)\n",
            "357 391 Loss: 1.559 | Acc: 42.548% (19497/45824)\n",
            "358 391 Loss: 1.558 | Acc: 42.584% (19568/45952)\n",
            "359 391 Loss: 1.558 | Acc: 42.578% (19620/46080)\n",
            "360 391 Loss: 1.558 | Acc: 42.594% (19682/46208)\n",
            "361 391 Loss: 1.557 | Acc: 42.606% (19742/46336)\n",
            "362 391 Loss: 1.557 | Acc: 42.614% (19800/46464)\n",
            "363 391 Loss: 1.557 | Acc: 42.627% (19861/46592)\n",
            "364 391 Loss: 1.556 | Acc: 42.635% (19919/46720)\n",
            "365 391 Loss: 1.556 | Acc: 42.629% (19971/46848)\n",
            "366 391 Loss: 1.556 | Acc: 42.637% (20029/46976)\n",
            "367 391 Loss: 1.555 | Acc: 42.646% (20088/47104)\n",
            "368 391 Loss: 1.555 | Acc: 42.649% (20144/47232)\n",
            "369 391 Loss: 1.555 | Acc: 42.660% (20204/47360)\n",
            "370 391 Loss: 1.554 | Acc: 42.670% (20263/47488)\n",
            "371 391 Loss: 1.554 | Acc: 42.687% (20326/47616)\n",
            "372 391 Loss: 1.554 | Acc: 42.699% (20386/47744)\n",
            "373 391 Loss: 1.553 | Acc: 42.731% (20456/47872)\n",
            "374 391 Loss: 1.553 | Acc: 42.737% (20514/48000)\n",
            "375 391 Loss: 1.552 | Acc: 42.773% (20586/48128)\n",
            "376 391 Loss: 1.552 | Acc: 42.784% (20646/48256)\n",
            "377 391 Loss: 1.552 | Acc: 42.785% (20701/48384)\n",
            "378 391 Loss: 1.551 | Acc: 42.783% (20755/48512)\n",
            "379 391 Loss: 1.551 | Acc: 42.804% (20820/48640)\n",
            "380 391 Loss: 1.551 | Acc: 42.819% (20882/48768)\n",
            "381 391 Loss: 1.550 | Acc: 42.852% (20953/48896)\n",
            "382 391 Loss: 1.550 | Acc: 42.859% (21011/49024)\n",
            "383 391 Loss: 1.550 | Acc: 42.867% (21070/49152)\n",
            "384 391 Loss: 1.549 | Acc: 42.871% (21127/49280)\n",
            "385 391 Loss: 1.549 | Acc: 42.882% (21187/49408)\n",
            "386 391 Loss: 1.549 | Acc: 42.922% (21262/49536)\n",
            "387 391 Loss: 1.548 | Acc: 42.945% (21328/49664)\n",
            "388 391 Loss: 1.548 | Acc: 42.965% (21393/49792)\n",
            "389 391 Loss: 1.547 | Acc: 42.985% (21458/49920)\n",
            "390 391 Loss: 1.547 | Acc: 42.994% (21497/50000)\n",
            "0 100 Loss: 1.182 | Acc: 52.000% (52/100)\n",
            "1 100 Loss: 1.236 | Acc: 52.000% (104/200)\n",
            "2 100 Loss: 1.261 | Acc: 50.333% (151/300)\n",
            "3 100 Loss: 1.298 | Acc: 49.750% (199/400)\n",
            "4 100 Loss: 1.320 | Acc: 48.000% (240/500)\n",
            "5 100 Loss: 1.331 | Acc: 48.167% (289/600)\n",
            "6 100 Loss: 1.334 | Acc: 48.857% (342/700)\n",
            "7 100 Loss: 1.352 | Acc: 48.500% (388/800)\n",
            "8 100 Loss: 1.360 | Acc: 48.667% (438/900)\n",
            "9 100 Loss: 1.353 | Acc: 49.400% (494/1000)\n",
            "10 100 Loss: 1.349 | Acc: 49.727% (547/1100)\n",
            "11 100 Loss: 1.355 | Acc: 49.500% (594/1200)\n",
            "12 100 Loss: 1.367 | Acc: 49.615% (645/1300)\n",
            "13 100 Loss: 1.381 | Acc: 49.214% (689/1400)\n",
            "14 100 Loss: 1.370 | Acc: 49.667% (745/1500)\n",
            "15 100 Loss: 1.368 | Acc: 49.750% (796/1600)\n",
            "16 100 Loss: 1.369 | Acc: 49.647% (844/1700)\n",
            "17 100 Loss: 1.360 | Acc: 50.000% (900/1800)\n",
            "18 100 Loss: 1.367 | Acc: 49.842% (947/1900)\n",
            "19 100 Loss: 1.372 | Acc: 49.800% (996/2000)\n",
            "20 100 Loss: 1.369 | Acc: 50.000% (1050/2100)\n",
            "21 100 Loss: 1.373 | Acc: 49.909% (1098/2200)\n",
            "22 100 Loss: 1.373 | Acc: 49.739% (1144/2300)\n",
            "23 100 Loss: 1.375 | Acc: 49.583% (1190/2400)\n",
            "24 100 Loss: 1.371 | Acc: 49.760% (1244/2500)\n",
            "25 100 Loss: 1.382 | Acc: 49.308% (1282/2600)\n",
            "26 100 Loss: 1.379 | Acc: 49.296% (1331/2700)\n",
            "27 100 Loss: 1.384 | Acc: 49.179% (1377/2800)\n",
            "28 100 Loss: 1.384 | Acc: 49.207% (1427/2900)\n",
            "29 100 Loss: 1.380 | Acc: 49.333% (1480/3000)\n",
            "30 100 Loss: 1.374 | Acc: 49.613% (1538/3100)\n",
            "31 100 Loss: 1.373 | Acc: 49.562% (1586/3200)\n",
            "32 100 Loss: 1.373 | Acc: 49.758% (1642/3300)\n",
            "33 100 Loss: 1.371 | Acc: 49.882% (1696/3400)\n",
            "34 100 Loss: 1.372 | Acc: 49.857% (1745/3500)\n",
            "35 100 Loss: 1.370 | Acc: 50.028% (1801/3600)\n",
            "36 100 Loss: 1.371 | Acc: 49.892% (1846/3700)\n",
            "37 100 Loss: 1.374 | Acc: 49.842% (1894/3800)\n",
            "38 100 Loss: 1.373 | Acc: 49.846% (1944/3900)\n",
            "39 100 Loss: 1.375 | Acc: 49.850% (1994/4000)\n",
            "40 100 Loss: 1.377 | Acc: 49.780% (2041/4100)\n",
            "41 100 Loss: 1.376 | Acc: 49.833% (2093/4200)\n",
            "42 100 Loss: 1.373 | Acc: 49.884% (2145/4300)\n",
            "43 100 Loss: 1.372 | Acc: 49.909% (2196/4400)\n",
            "44 100 Loss: 1.371 | Acc: 49.889% (2245/4500)\n",
            "45 100 Loss: 1.371 | Acc: 49.913% (2296/4600)\n",
            "46 100 Loss: 1.367 | Acc: 50.043% (2352/4700)\n",
            "47 100 Loss: 1.366 | Acc: 50.146% (2407/4800)\n",
            "48 100 Loss: 1.366 | Acc: 50.041% (2452/4900)\n",
            "49 100 Loss: 1.365 | Acc: 50.100% (2505/5000)\n",
            "50 100 Loss: 1.366 | Acc: 50.059% (2553/5100)\n",
            "51 100 Loss: 1.367 | Acc: 50.058% (2603/5200)\n",
            "52 100 Loss: 1.366 | Acc: 49.962% (2648/5300)\n",
            "53 100 Loss: 1.367 | Acc: 49.963% (2698/5400)\n",
            "54 100 Loss: 1.370 | Acc: 49.873% (2743/5500)\n",
            "55 100 Loss: 1.374 | Acc: 49.821% (2790/5600)\n",
            "56 100 Loss: 1.372 | Acc: 49.895% (2844/5700)\n",
            "57 100 Loss: 1.368 | Acc: 50.034% (2902/5800)\n",
            "58 100 Loss: 1.369 | Acc: 50.034% (2952/5900)\n",
            "59 100 Loss: 1.371 | Acc: 49.967% (2998/6000)\n",
            "60 100 Loss: 1.372 | Acc: 49.984% (3049/6100)\n",
            "61 100 Loss: 1.372 | Acc: 49.952% (3097/6200)\n",
            "62 100 Loss: 1.372 | Acc: 49.968% (3148/6300)\n",
            "63 100 Loss: 1.373 | Acc: 50.031% (3202/6400)\n",
            "64 100 Loss: 1.375 | Acc: 49.846% (3240/6500)\n",
            "65 100 Loss: 1.375 | Acc: 49.788% (3286/6600)\n",
            "66 100 Loss: 1.374 | Acc: 49.776% (3335/6700)\n",
            "67 100 Loss: 1.372 | Acc: 49.794% (3386/6800)\n",
            "68 100 Loss: 1.373 | Acc: 49.739% (3432/6900)\n",
            "69 100 Loss: 1.376 | Acc: 49.643% (3475/7000)\n",
            "70 100 Loss: 1.375 | Acc: 49.634% (3524/7100)\n",
            "71 100 Loss: 1.375 | Acc: 49.778% (3584/7200)\n",
            "72 100 Loss: 1.373 | Acc: 49.822% (3637/7300)\n",
            "73 100 Loss: 1.371 | Acc: 49.959% (3697/7400)\n",
            "74 100 Loss: 1.372 | Acc: 49.907% (3743/7500)\n",
            "75 100 Loss: 1.372 | Acc: 49.934% (3795/7600)\n",
            "76 100 Loss: 1.370 | Acc: 50.013% (3851/7700)\n",
            "77 100 Loss: 1.369 | Acc: 50.038% (3903/7800)\n",
            "78 100 Loss: 1.369 | Acc: 50.076% (3956/7900)\n",
            "79 100 Loss: 1.371 | Acc: 49.913% (3993/8000)\n",
            "80 100 Loss: 1.371 | Acc: 49.938% (4045/8100)\n",
            "81 100 Loss: 1.372 | Acc: 49.854% (4088/8200)\n",
            "82 100 Loss: 1.374 | Acc: 49.723% (4127/8300)\n",
            "83 100 Loss: 1.375 | Acc: 49.690% (4174/8400)\n",
            "84 100 Loss: 1.375 | Acc: 49.694% (4224/8500)\n",
            "85 100 Loss: 1.374 | Acc: 49.802% (4283/8600)\n",
            "86 100 Loss: 1.376 | Acc: 49.759% (4329/8700)\n",
            "87 100 Loss: 1.376 | Acc: 49.818% (4384/8800)\n",
            "88 100 Loss: 1.376 | Acc: 49.899% (4441/8900)\n",
            "89 100 Loss: 1.377 | Acc: 49.833% (4485/9000)\n",
            "90 100 Loss: 1.376 | Acc: 49.835% (4535/9100)\n",
            "91 100 Loss: 1.376 | Acc: 49.913% (4592/9200)\n",
            "92 100 Loss: 1.377 | Acc: 49.914% (4642/9300)\n",
            "93 100 Loss: 1.376 | Acc: 49.851% (4686/9400)\n",
            "94 100 Loss: 1.376 | Acc: 49.853% (4736/9500)\n",
            "95 100 Loss: 1.375 | Acc: 49.906% (4791/9600)\n",
            "96 100 Loss: 1.375 | Acc: 49.897% (4840/9700)\n",
            "97 100 Loss: 1.375 | Acc: 49.837% (4884/9800)\n",
            "98 100 Loss: 1.376 | Acc: 49.758% (4926/9900)\n",
            "99 100 Loss: 1.376 | Acc: 49.750% (4975/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            "0 391 Loss: 1.286 | Acc: 50.781% (65/128)\n",
            "1 391 Loss: 1.377 | Acc: 50.781% (130/256)\n",
            "2 391 Loss: 1.348 | Acc: 52.604% (202/384)\n",
            "3 391 Loss: 1.356 | Acc: 52.148% (267/512)\n",
            "4 391 Loss: 1.375 | Acc: 51.875% (332/640)\n",
            "5 391 Loss: 1.362 | Acc: 52.865% (406/768)\n",
            "6 391 Loss: 1.370 | Acc: 51.562% (462/896)\n",
            "7 391 Loss: 1.374 | Acc: 50.977% (522/1024)\n",
            "8 391 Loss: 1.402 | Acc: 50.260% (579/1152)\n",
            "9 391 Loss: 1.413 | Acc: 49.375% (632/1280)\n",
            "10 391 Loss: 1.413 | Acc: 49.361% (695/1408)\n",
            "11 391 Loss: 1.414 | Acc: 49.414% (759/1536)\n",
            "12 391 Loss: 1.411 | Acc: 49.459% (823/1664)\n",
            "13 391 Loss: 1.399 | Acc: 49.554% (888/1792)\n",
            "14 391 Loss: 1.411 | Acc: 48.802% (937/1920)\n",
            "15 391 Loss: 1.417 | Acc: 48.633% (996/2048)\n",
            "16 391 Loss: 1.426 | Acc: 48.346% (1052/2176)\n",
            "17 391 Loss: 1.426 | Acc: 48.611% (1120/2304)\n",
            "18 391 Loss: 1.424 | Acc: 48.520% (1180/2432)\n",
            "19 391 Loss: 1.428 | Acc: 48.125% (1232/2560)\n",
            "20 391 Loss: 1.427 | Acc: 48.103% (1293/2688)\n",
            "21 391 Loss: 1.427 | Acc: 48.331% (1361/2816)\n",
            "22 391 Loss: 1.427 | Acc: 48.030% (1414/2944)\n",
            "23 391 Loss: 1.429 | Acc: 47.819% (1469/3072)\n",
            "24 391 Loss: 1.423 | Acc: 48.031% (1537/3200)\n",
            "25 391 Loss: 1.423 | Acc: 48.227% (1605/3328)\n",
            "26 391 Loss: 1.420 | Acc: 48.032% (1660/3456)\n",
            "27 391 Loss: 1.422 | Acc: 48.075% (1723/3584)\n",
            "28 391 Loss: 1.417 | Acc: 48.276% (1792/3712)\n",
            "29 391 Loss: 1.419 | Acc: 48.229% (1852/3840)\n",
            "30 391 Loss: 1.414 | Acc: 48.513% (1925/3968)\n",
            "31 391 Loss: 1.407 | Acc: 48.706% (1995/4096)\n",
            "32 391 Loss: 1.406 | Acc: 48.816% (2062/4224)\n",
            "33 391 Loss: 1.405 | Acc: 48.805% (2124/4352)\n",
            "34 391 Loss: 1.404 | Acc: 48.795% (2186/4480)\n",
            "35 391 Loss: 1.410 | Acc: 48.503% (2235/4608)\n",
            "36 391 Loss: 1.406 | Acc: 48.691% (2306/4736)\n",
            "37 391 Loss: 1.404 | Acc: 48.684% (2368/4864)\n",
            "38 391 Loss: 1.403 | Acc: 48.618% (2427/4992)\n",
            "39 391 Loss: 1.402 | Acc: 48.496% (2483/5120)\n",
            "40 391 Loss: 1.402 | Acc: 48.533% (2547/5248)\n",
            "41 391 Loss: 1.400 | Acc: 48.772% (2622/5376)\n",
            "42 391 Loss: 1.401 | Acc: 48.746% (2683/5504)\n",
            "43 391 Loss: 1.405 | Acc: 48.757% (2746/5632)\n",
            "44 391 Loss: 1.406 | Acc: 48.715% (2806/5760)\n",
            "45 391 Loss: 1.404 | Acc: 48.760% (2871/5888)\n",
            "46 391 Loss: 1.400 | Acc: 49.036% (2950/6016)\n",
            "47 391 Loss: 1.400 | Acc: 49.056% (3014/6144)\n",
            "48 391 Loss: 1.397 | Acc: 49.203% (3086/6272)\n",
            "49 391 Loss: 1.397 | Acc: 49.188% (3148/6400)\n",
            "50 391 Loss: 1.401 | Acc: 49.127% (3207/6528)\n",
            "51 391 Loss: 1.401 | Acc: 49.144% (3271/6656)\n",
            "52 391 Loss: 1.399 | Acc: 49.204% (3338/6784)\n",
            "53 391 Loss: 1.398 | Acc: 49.190% (3400/6912)\n",
            "54 391 Loss: 1.398 | Acc: 49.233% (3466/7040)\n",
            "55 391 Loss: 1.398 | Acc: 49.247% (3530/7168)\n",
            "56 391 Loss: 1.398 | Acc: 49.219% (3591/7296)\n",
            "57 391 Loss: 1.397 | Acc: 49.178% (3651/7424)\n",
            "58 391 Loss: 1.396 | Acc: 49.272% (3721/7552)\n",
            "59 391 Loss: 1.399 | Acc: 49.206% (3779/7680)\n",
            "60 391 Loss: 1.399 | Acc: 49.129% (3836/7808)\n",
            "61 391 Loss: 1.400 | Acc: 49.131% (3899/7936)\n",
            "62 391 Loss: 1.401 | Acc: 49.070% (3957/8064)\n",
            "63 391 Loss: 1.400 | Acc: 49.023% (4016/8192)\n",
            "64 391 Loss: 1.401 | Acc: 49.002% (4077/8320)\n",
            "65 391 Loss: 1.400 | Acc: 49.065% (4145/8448)\n",
            "66 391 Loss: 1.399 | Acc: 49.172% (4217/8576)\n",
            "67 391 Loss: 1.399 | Acc: 49.138% (4277/8704)\n",
            "68 391 Loss: 1.399 | Acc: 49.106% (4337/8832)\n",
            "69 391 Loss: 1.399 | Acc: 49.174% (4406/8960)\n",
            "70 391 Loss: 1.399 | Acc: 49.186% (4470/9088)\n",
            "71 391 Loss: 1.398 | Acc: 49.219% (4536/9216)\n",
            "72 391 Loss: 1.397 | Acc: 49.251% (4602/9344)\n",
            "73 391 Loss: 1.395 | Acc: 49.261% (4666/9472)\n",
            "74 391 Loss: 1.395 | Acc: 49.281% (4731/9600)\n",
            "75 391 Loss: 1.397 | Acc: 49.239% (4790/9728)\n",
            "76 391 Loss: 1.398 | Acc: 49.229% (4852/9856)\n",
            "77 391 Loss: 1.397 | Acc: 49.239% (4916/9984)\n",
            "78 391 Loss: 1.397 | Acc: 49.219% (4977/10112)\n",
            "79 391 Loss: 1.397 | Acc: 49.287% (5047/10240)\n",
            "80 391 Loss: 1.398 | Acc: 49.257% (5107/10368)\n",
            "81 391 Loss: 1.398 | Acc: 49.276% (5172/10496)\n",
            "82 391 Loss: 1.398 | Acc: 49.209% (5228/10624)\n",
            "83 391 Loss: 1.395 | Acc: 49.321% (5303/10752)\n",
            "84 391 Loss: 1.396 | Acc: 49.347% (5369/10880)\n",
            "85 391 Loss: 1.396 | Acc: 49.319% (5429/11008)\n",
            "86 391 Loss: 1.399 | Acc: 49.183% (5477/11136)\n",
            "87 391 Loss: 1.397 | Acc: 49.290% (5552/11264)\n",
            "88 391 Loss: 1.397 | Acc: 49.307% (5617/11392)\n",
            "89 391 Loss: 1.396 | Acc: 49.375% (5688/11520)\n",
            "90 391 Loss: 1.397 | Acc: 49.348% (5748/11648)\n",
            "91 391 Loss: 1.396 | Acc: 49.338% (5810/11776)\n",
            "92 391 Loss: 1.395 | Acc: 49.387% (5879/11904)\n",
            "93 391 Loss: 1.395 | Acc: 49.360% (5939/12032)\n",
            "94 391 Loss: 1.395 | Acc: 49.326% (5998/12160)\n",
            "95 391 Loss: 1.395 | Acc: 49.365% (6066/12288)\n",
            "96 391 Loss: 1.393 | Acc: 49.444% (6139/12416)\n",
            "97 391 Loss: 1.394 | Acc: 49.426% (6200/12544)\n",
            "98 391 Loss: 1.393 | Acc: 49.503% (6273/12672)\n",
            "99 391 Loss: 1.392 | Acc: 49.531% (6340/12800)\n",
            "100 391 Loss: 1.392 | Acc: 49.528% (6403/12928)\n",
            "101 391 Loss: 1.392 | Acc: 49.517% (6465/13056)\n",
            "102 391 Loss: 1.392 | Acc: 49.583% (6537/13184)\n",
            "103 391 Loss: 1.393 | Acc: 49.549% (6596/13312)\n",
            "104 391 Loss: 1.393 | Acc: 49.501% (6653/13440)\n",
            "105 391 Loss: 1.394 | Acc: 49.447% (6709/13568)\n",
            "106 391 Loss: 1.397 | Acc: 49.372% (6762/13696)\n",
            "107 391 Loss: 1.398 | Acc: 49.334% (6820/13824)\n",
            "108 391 Loss: 1.398 | Acc: 49.326% (6882/13952)\n",
            "109 391 Loss: 1.398 | Acc: 49.290% (6940/14080)\n",
            "110 391 Loss: 1.398 | Acc: 49.345% (7011/14208)\n",
            "111 391 Loss: 1.398 | Acc: 49.344% (7074/14336)\n",
            "112 391 Loss: 1.398 | Acc: 49.329% (7135/14464)\n",
            "113 391 Loss: 1.398 | Acc: 49.342% (7200/14592)\n",
            "114 391 Loss: 1.399 | Acc: 49.287% (7255/14720)\n",
            "115 391 Loss: 1.399 | Acc: 49.327% (7324/14848)\n",
            "116 391 Loss: 1.398 | Acc: 49.339% (7389/14976)\n",
            "117 391 Loss: 1.397 | Acc: 49.351% (7454/15104)\n",
            "118 391 Loss: 1.396 | Acc: 49.409% (7526/15232)\n",
            "119 391 Loss: 1.397 | Acc: 49.453% (7596/15360)\n",
            "120 391 Loss: 1.397 | Acc: 49.445% (7658/15488)\n",
            "121 391 Loss: 1.397 | Acc: 49.462% (7724/15616)\n",
            "122 391 Loss: 1.395 | Acc: 49.511% (7795/15744)\n",
            "123 391 Loss: 1.395 | Acc: 49.509% (7858/15872)\n",
            "124 391 Loss: 1.395 | Acc: 49.525% (7924/16000)\n",
            "125 391 Loss: 1.394 | Acc: 49.560% (7993/16128)\n",
            "126 391 Loss: 1.393 | Acc: 49.569% (8058/16256)\n",
            "127 391 Loss: 1.393 | Acc: 49.561% (8120/16384)\n",
            "128 391 Loss: 1.392 | Acc: 49.558% (8183/16512)\n",
            "129 391 Loss: 1.392 | Acc: 49.573% (8249/16640)\n",
            "130 391 Loss: 1.392 | Acc: 49.547% (8308/16768)\n",
            "131 391 Loss: 1.392 | Acc: 49.538% (8370/16896)\n",
            "132 391 Loss: 1.392 | Acc: 49.554% (8436/17024)\n",
            "133 391 Loss: 1.390 | Acc: 49.644% (8515/17152)\n",
            "134 391 Loss: 1.388 | Acc: 49.734% (8594/17280)\n",
            "135 391 Loss: 1.388 | Acc: 49.782% (8666/17408)\n",
            "136 391 Loss: 1.387 | Acc: 49.823% (8737/17536)\n",
            "137 391 Loss: 1.387 | Acc: 49.836% (8803/17664)\n",
            "138 391 Loss: 1.386 | Acc: 49.843% (8868/17792)\n",
            "139 391 Loss: 1.386 | Acc: 49.855% (8934/17920)\n",
            "140 391 Loss: 1.385 | Acc: 49.856% (8998/18048)\n",
            "141 391 Loss: 1.385 | Acc: 49.857% (9062/18176)\n",
            "142 391 Loss: 1.384 | Acc: 49.902% (9134/18304)\n",
            "143 391 Loss: 1.382 | Acc: 49.940% (9205/18432)\n",
            "144 391 Loss: 1.381 | Acc: 49.941% (9269/18560)\n",
            "145 391 Loss: 1.382 | Acc: 49.882% (9322/18688)\n",
            "146 391 Loss: 1.382 | Acc: 49.915% (9392/18816)\n",
            "147 391 Loss: 1.382 | Acc: 49.942% (9461/18944)\n",
            "148 391 Loss: 1.381 | Acc: 49.984% (9533/19072)\n",
            "149 391 Loss: 1.381 | Acc: 50.005% (9601/19200)\n",
            "150 391 Loss: 1.380 | Acc: 50.057% (9675/19328)\n",
            "151 391 Loss: 1.380 | Acc: 50.015% (9731/19456)\n",
            "152 391 Loss: 1.380 | Acc: 50.026% (9797/19584)\n",
            "153 391 Loss: 1.381 | Acc: 50.000% (9856/19712)\n",
            "154 391 Loss: 1.382 | Acc: 49.950% (9910/19840)\n",
            "155 391 Loss: 1.383 | Acc: 49.890% (9962/19968)\n",
            "156 391 Loss: 1.382 | Acc: 49.876% (10023/20096)\n",
            "157 391 Loss: 1.382 | Acc: 49.871% (10086/20224)\n",
            "158 391 Loss: 1.381 | Acc: 49.916% (10159/20352)\n",
            "159 391 Loss: 1.382 | Acc: 49.897% (10219/20480)\n",
            "160 391 Loss: 1.381 | Acc: 49.898% (10283/20608)\n",
            "161 391 Loss: 1.381 | Acc: 49.908% (10349/20736)\n",
            "162 391 Loss: 1.381 | Acc: 49.919% (10415/20864)\n",
            "163 391 Loss: 1.381 | Acc: 49.952% (10486/20992)\n",
            "164 391 Loss: 1.381 | Acc: 49.920% (10543/21120)\n",
            "165 391 Loss: 1.381 | Acc: 49.925% (10608/21248)\n",
            "166 391 Loss: 1.380 | Acc: 49.972% (10682/21376)\n",
            "167 391 Loss: 1.379 | Acc: 50.000% (10752/21504)\n",
            "168 391 Loss: 1.380 | Acc: 49.968% (10809/21632)\n",
            "169 391 Loss: 1.379 | Acc: 50.018% (10884/21760)\n",
            "170 391 Loss: 1.379 | Acc: 50.037% (10952/21888)\n",
            "171 391 Loss: 1.379 | Acc: 50.064% (11022/22016)\n",
            "172 391 Loss: 1.379 | Acc: 50.104% (11095/22144)\n",
            "173 391 Loss: 1.378 | Acc: 50.121% (11163/22272)\n",
            "174 391 Loss: 1.377 | Acc: 50.129% (11229/22400)\n",
            "175 391 Loss: 1.377 | Acc: 50.138% (11295/22528)\n",
            "176 391 Loss: 1.377 | Acc: 50.154% (11363/22656)\n",
            "177 391 Loss: 1.377 | Acc: 50.158% (11428/22784)\n",
            "178 391 Loss: 1.376 | Acc: 50.161% (11493/22912)\n",
            "179 391 Loss: 1.376 | Acc: 50.161% (11557/23040)\n",
            "180 391 Loss: 1.375 | Acc: 50.168% (11623/23168)\n",
            "181 391 Loss: 1.374 | Acc: 50.202% (11695/23296)\n",
            "182 391 Loss: 1.375 | Acc: 50.196% (11758/23424)\n",
            "183 391 Loss: 1.374 | Acc: 50.242% (11833/23552)\n",
            "184 391 Loss: 1.374 | Acc: 50.203% (11888/23680)\n",
            "185 391 Loss: 1.375 | Acc: 50.185% (11948/23808)\n",
            "186 391 Loss: 1.375 | Acc: 50.159% (12006/23936)\n",
            "187 391 Loss: 1.375 | Acc: 50.150% (12068/24064)\n",
            "188 391 Loss: 1.376 | Acc: 50.149% (12132/24192)\n",
            "189 391 Loss: 1.376 | Acc: 50.160% (12199/24320)\n",
            "190 391 Loss: 1.376 | Acc: 50.147% (12260/24448)\n",
            "191 391 Loss: 1.376 | Acc: 50.163% (12328/24576)\n",
            "192 391 Loss: 1.375 | Acc: 50.154% (12390/24704)\n",
            "193 391 Loss: 1.375 | Acc: 50.193% (12464/24832)\n",
            "194 391 Loss: 1.374 | Acc: 50.216% (12534/24960)\n",
            "195 391 Loss: 1.374 | Acc: 50.203% (12595/25088)\n",
            "196 391 Loss: 1.374 | Acc: 50.206% (12660/25216)\n",
            "197 391 Loss: 1.374 | Acc: 50.249% (12735/25344)\n",
            "198 391 Loss: 1.373 | Acc: 50.267% (12804/25472)\n",
            "199 391 Loss: 1.372 | Acc: 50.277% (12871/25600)\n",
            "200 391 Loss: 1.371 | Acc: 50.369% (12959/25728)\n",
            "201 391 Loss: 1.369 | Acc: 50.402% (13032/25856)\n",
            "202 391 Loss: 1.369 | Acc: 50.396% (13095/25984)\n",
            "203 391 Loss: 1.369 | Acc: 50.414% (13164/26112)\n",
            "204 391 Loss: 1.368 | Acc: 50.431% (13233/26240)\n",
            "205 391 Loss: 1.368 | Acc: 50.444% (13301/26368)\n",
            "206 391 Loss: 1.368 | Acc: 50.442% (13365/26496)\n",
            "207 391 Loss: 1.368 | Acc: 50.454% (13433/26624)\n",
            "208 391 Loss: 1.367 | Acc: 50.490% (13507/26752)\n",
            "209 391 Loss: 1.366 | Acc: 50.528% (13582/26880)\n",
            "210 391 Loss: 1.365 | Acc: 50.544% (13651/27008)\n",
            "211 391 Loss: 1.366 | Acc: 50.516% (13708/27136)\n",
            "212 391 Loss: 1.365 | Acc: 50.572% (13788/27264)\n",
            "213 391 Loss: 1.364 | Acc: 50.588% (13857/27392)\n",
            "214 391 Loss: 1.365 | Acc: 50.560% (13914/27520)\n",
            "215 391 Loss: 1.364 | Acc: 50.557% (13978/27648)\n",
            "216 391 Loss: 1.364 | Acc: 50.565% (14045/27776)\n",
            "217 391 Loss: 1.365 | Acc: 50.548% (14105/27904)\n",
            "218 391 Loss: 1.366 | Acc: 50.521% (14162/28032)\n",
            "219 391 Loss: 1.364 | Acc: 50.568% (14240/28160)\n",
            "220 391 Loss: 1.364 | Acc: 50.573% (14306/28288)\n",
            "221 391 Loss: 1.364 | Acc: 50.581% (14373/28416)\n",
            "222 391 Loss: 1.363 | Acc: 50.606% (14445/28544)\n",
            "223 391 Loss: 1.362 | Acc: 50.614% (14512/28672)\n",
            "224 391 Loss: 1.362 | Acc: 50.639% (14584/28800)\n",
            "225 391 Loss: 1.362 | Acc: 50.636% (14648/28928)\n",
            "226 391 Loss: 1.362 | Acc: 50.630% (14711/29056)\n",
            "227 391 Loss: 1.361 | Acc: 50.648% (14781/29184)\n",
            "228 391 Loss: 1.361 | Acc: 50.675% (14854/29312)\n",
            "229 391 Loss: 1.361 | Acc: 50.690% (14923/29440)\n",
            "230 391 Loss: 1.361 | Acc: 50.720% (14997/29568)\n",
            "231 391 Loss: 1.361 | Acc: 50.731% (15065/29696)\n",
            "232 391 Loss: 1.360 | Acc: 50.741% (15133/29824)\n",
            "233 391 Loss: 1.360 | Acc: 50.728% (15194/29952)\n",
            "234 391 Loss: 1.360 | Acc: 50.721% (15257/30080)\n",
            "235 391 Loss: 1.359 | Acc: 50.748% (15330/30208)\n",
            "236 391 Loss: 1.359 | Acc: 50.765% (15400/30336)\n",
            "237 391 Loss: 1.359 | Acc: 50.778% (15469/30464)\n",
            "238 391 Loss: 1.358 | Acc: 50.801% (15541/30592)\n",
            "239 391 Loss: 1.358 | Acc: 50.833% (15616/30720)\n",
            "240 391 Loss: 1.357 | Acc: 50.853% (15687/30848)\n",
            "241 391 Loss: 1.357 | Acc: 50.875% (15759/30976)\n",
            "242 391 Loss: 1.357 | Acc: 50.858% (15819/31104)\n",
            "243 391 Loss: 1.356 | Acc: 50.881% (15891/31232)\n",
            "244 391 Loss: 1.356 | Acc: 50.867% (15952/31360)\n",
            "245 391 Loss: 1.356 | Acc: 50.886% (16023/31488)\n",
            "246 391 Loss: 1.355 | Acc: 50.905% (16094/31616)\n",
            "247 391 Loss: 1.355 | Acc: 50.942% (16171/31744)\n",
            "248 391 Loss: 1.354 | Acc: 50.985% (16250/31872)\n",
            "249 391 Loss: 1.353 | Acc: 51.000% (16320/32000)\n",
            "250 391 Loss: 1.354 | Acc: 51.012% (16389/32128)\n",
            "251 391 Loss: 1.353 | Acc: 51.048% (16466/32256)\n",
            "252 391 Loss: 1.353 | Acc: 51.050% (16532/32384)\n",
            "253 391 Loss: 1.353 | Acc: 51.052% (16598/32512)\n",
            "254 391 Loss: 1.352 | Acc: 51.060% (16666/32640)\n",
            "255 391 Loss: 1.351 | Acc: 51.089% (16741/32768)\n",
            "256 391 Loss: 1.350 | Acc: 51.110% (16813/32896)\n",
            "257 391 Loss: 1.350 | Acc: 51.117% (16881/33024)\n",
            "258 391 Loss: 1.350 | Acc: 51.110% (16944/33152)\n",
            "259 391 Loss: 1.350 | Acc: 51.124% (17014/33280)\n",
            "260 391 Loss: 1.350 | Acc: 51.137% (17084/33408)\n",
            "261 391 Loss: 1.349 | Acc: 51.184% (17165/33536)\n",
            "262 391 Loss: 1.349 | Acc: 51.197% (17235/33664)\n",
            "263 391 Loss: 1.348 | Acc: 51.210% (17305/33792)\n",
            "264 391 Loss: 1.348 | Acc: 51.232% (17378/33920)\n",
            "265 391 Loss: 1.348 | Acc: 51.231% (17443/34048)\n",
            "266 391 Loss: 1.348 | Acc: 51.232% (17509/34176)\n",
            "267 391 Loss: 1.347 | Acc: 51.236% (17576/34304)\n",
            "268 391 Loss: 1.348 | Acc: 51.234% (17641/34432)\n",
            "269 391 Loss: 1.347 | Acc: 51.238% (17708/34560)\n",
            "270 391 Loss: 1.346 | Acc: 51.260% (17781/34688)\n",
            "271 391 Loss: 1.346 | Acc: 51.264% (17848/34816)\n",
            "272 391 Loss: 1.346 | Acc: 51.285% (17921/34944)\n",
            "273 391 Loss: 1.345 | Acc: 51.317% (17998/35072)\n",
            "274 391 Loss: 1.346 | Acc: 51.335% (18070/35200)\n",
            "275 391 Loss: 1.345 | Acc: 51.342% (18138/35328)\n",
            "276 391 Loss: 1.345 | Acc: 51.348% (18206/35456)\n",
            "277 391 Loss: 1.345 | Acc: 51.346% (18271/35584)\n",
            "278 391 Loss: 1.344 | Acc: 51.358% (18341/35712)\n",
            "279 391 Loss: 1.344 | Acc: 51.364% (18409/35840)\n",
            "280 391 Loss: 1.344 | Acc: 51.368% (18476/35968)\n",
            "281 391 Loss: 1.344 | Acc: 51.377% (18545/36096)\n",
            "282 391 Loss: 1.344 | Acc: 51.386% (18614/36224)\n",
            "283 391 Loss: 1.345 | Acc: 51.356% (18669/36352)\n",
            "284 391 Loss: 1.344 | Acc: 51.368% (18739/36480)\n",
            "285 391 Loss: 1.345 | Acc: 51.360% (18802/36608)\n",
            "286 391 Loss: 1.345 | Acc: 51.353% (18865/36736)\n",
            "287 391 Loss: 1.344 | Acc: 51.383% (18942/36864)\n",
            "288 391 Loss: 1.344 | Acc: 51.408% (19017/36992)\n",
            "289 391 Loss: 1.344 | Acc: 51.412% (19084/37120)\n",
            "290 391 Loss: 1.344 | Acc: 51.412% (19150/37248)\n",
            "291 391 Loss: 1.344 | Acc: 51.418% (19218/37376)\n",
            "292 391 Loss: 1.344 | Acc: 51.421% (19285/37504)\n",
            "293 391 Loss: 1.343 | Acc: 51.398% (19342/37632)\n",
            "294 391 Loss: 1.343 | Acc: 51.427% (19419/37760)\n",
            "295 391 Loss: 1.342 | Acc: 51.454% (19495/37888)\n",
            "296 391 Loss: 1.342 | Acc: 51.447% (19558/38016)\n",
            "297 391 Loss: 1.342 | Acc: 51.437% (19620/38144)\n",
            "298 391 Loss: 1.342 | Acc: 51.461% (19695/38272)\n",
            "299 391 Loss: 1.342 | Acc: 51.469% (19764/38400)\n",
            "300 391 Loss: 1.341 | Acc: 51.474% (19832/38528)\n",
            "301 391 Loss: 1.341 | Acc: 51.482% (19901/38656)\n",
            "302 391 Loss: 1.341 | Acc: 51.485% (19968/38784)\n",
            "303 391 Loss: 1.340 | Acc: 51.488% (20035/38912)\n",
            "304 391 Loss: 1.341 | Acc: 51.496% (20104/39040)\n",
            "305 391 Loss: 1.341 | Acc: 51.506% (20174/39168)\n",
            "306 391 Loss: 1.341 | Acc: 51.489% (20233/39296)\n",
            "307 391 Loss: 1.341 | Acc: 51.479% (20295/39424)\n",
            "308 391 Loss: 1.340 | Acc: 51.512% (20374/39552)\n",
            "309 391 Loss: 1.340 | Acc: 51.532% (20448/39680)\n",
            "310 391 Loss: 1.340 | Acc: 51.535% (20515/39808)\n",
            "311 391 Loss: 1.340 | Acc: 51.540% (20583/39936)\n",
            "312 391 Loss: 1.340 | Acc: 51.535% (20647/40064)\n",
            "313 391 Loss: 1.340 | Acc: 51.528% (20710/40192)\n",
            "314 391 Loss: 1.340 | Acc: 51.538% (20780/40320)\n",
            "315 391 Loss: 1.340 | Acc: 51.535% (20845/40448)\n",
            "316 391 Loss: 1.340 | Acc: 51.533% (20910/40576)\n",
            "317 391 Loss: 1.340 | Acc: 51.523% (20972/40704)\n",
            "318 391 Loss: 1.340 | Acc: 51.523% (21038/40832)\n",
            "319 391 Loss: 1.340 | Acc: 51.526% (21105/40960)\n",
            "320 391 Loss: 1.340 | Acc: 51.545% (21179/41088)\n",
            "321 391 Loss: 1.339 | Acc: 51.565% (21253/41216)\n",
            "322 391 Loss: 1.340 | Acc: 51.558% (21316/41344)\n",
            "323 391 Loss: 1.340 | Acc: 51.577% (21390/41472)\n",
            "324 391 Loss: 1.339 | Acc: 51.579% (21457/41600)\n",
            "325 391 Loss: 1.339 | Acc: 51.577% (21522/41728)\n",
            "326 391 Loss: 1.339 | Acc: 51.584% (21591/41856)\n",
            "327 391 Loss: 1.339 | Acc: 51.601% (21664/41984)\n",
            "328 391 Loss: 1.339 | Acc: 51.596% (21728/42112)\n",
            "329 391 Loss: 1.338 | Acc: 51.593% (21793/42240)\n",
            "330 391 Loss: 1.338 | Acc: 51.610% (21866/42368)\n",
            "331 391 Loss: 1.338 | Acc: 51.628% (21940/42496)\n",
            "332 391 Loss: 1.337 | Acc: 51.640% (22011/42624)\n",
            "333 391 Loss: 1.337 | Acc: 51.663% (22087/42752)\n",
            "334 391 Loss: 1.336 | Acc: 51.674% (22158/42880)\n",
            "335 391 Loss: 1.336 | Acc: 51.669% (22222/43008)\n",
            "336 391 Loss: 1.336 | Acc: 51.690% (22297/43136)\n",
            "337 391 Loss: 1.336 | Acc: 51.683% (22360/43264)\n",
            "338 391 Loss: 1.336 | Acc: 51.671% (22421/43392)\n",
            "339 391 Loss: 1.335 | Acc: 51.691% (22496/43520)\n",
            "340 391 Loss: 1.335 | Acc: 51.725% (22577/43648)\n",
            "341 391 Loss: 1.334 | Acc: 51.727% (22644/43776)\n",
            "342 391 Loss: 1.334 | Acc: 51.745% (22718/43904)\n",
            "343 391 Loss: 1.333 | Acc: 51.769% (22795/44032)\n",
            "344 391 Loss: 1.333 | Acc: 51.775% (22864/44160)\n",
            "345 391 Loss: 1.333 | Acc: 51.793% (22938/44288)\n",
            "346 391 Loss: 1.333 | Acc: 51.810% (23012/44416)\n",
            "347 391 Loss: 1.333 | Acc: 51.807% (23077/44544)\n",
            "348 391 Loss: 1.333 | Acc: 51.815% (23147/44672)\n",
            "349 391 Loss: 1.333 | Acc: 51.817% (23214/44800)\n",
            "350 391 Loss: 1.333 | Acc: 51.816% (23280/44928)\n",
            "351 391 Loss: 1.333 | Acc: 51.827% (23351/45056)\n",
            "352 391 Loss: 1.332 | Acc: 51.846% (23426/45184)\n",
            "353 391 Loss: 1.332 | Acc: 51.867% (23502/45312)\n",
            "354 391 Loss: 1.332 | Acc: 51.873% (23571/45440)\n",
            "355 391 Loss: 1.332 | Acc: 51.874% (23638/45568)\n",
            "356 391 Loss: 1.332 | Acc: 51.875% (23705/45696)\n",
            "357 391 Loss: 1.332 | Acc: 51.890% (23778/45824)\n",
            "358 391 Loss: 1.331 | Acc: 51.889% (23844/45952)\n",
            "359 391 Loss: 1.331 | Acc: 51.888% (23910/46080)\n",
            "360 391 Loss: 1.331 | Acc: 51.902% (23983/46208)\n",
            "361 391 Loss: 1.331 | Acc: 51.912% (24054/46336)\n",
            "362 391 Loss: 1.330 | Acc: 51.928% (24128/46464)\n",
            "363 391 Loss: 1.330 | Acc: 51.951% (24205/46592)\n",
            "364 391 Loss: 1.329 | Acc: 51.948% (24270/46720)\n",
            "365 391 Loss: 1.329 | Acc: 51.953% (24339/46848)\n",
            "366 391 Loss: 1.329 | Acc: 51.952% (24405/46976)\n",
            "367 391 Loss: 1.328 | Acc: 51.955% (24473/47104)\n",
            "368 391 Loss: 1.328 | Acc: 51.977% (24550/47232)\n",
            "369 391 Loss: 1.327 | Acc: 52.000% (24627/47360)\n",
            "370 391 Loss: 1.327 | Acc: 52.005% (24696/47488)\n",
            "371 391 Loss: 1.326 | Acc: 52.046% (24782/47616)\n",
            "372 391 Loss: 1.326 | Acc: 52.061% (24856/47744)\n",
            "373 391 Loss: 1.325 | Acc: 52.064% (24924/47872)\n",
            "374 391 Loss: 1.325 | Acc: 52.077% (24997/48000)\n",
            "375 391 Loss: 1.325 | Acc: 52.080% (25065/48128)\n",
            "376 391 Loss: 1.324 | Acc: 52.112% (25147/48256)\n",
            "377 391 Loss: 1.324 | Acc: 52.127% (25221/48384)\n",
            "378 391 Loss: 1.323 | Acc: 52.154% (25301/48512)\n",
            "379 391 Loss: 1.323 | Acc: 52.159% (25370/48640)\n",
            "380 391 Loss: 1.323 | Acc: 52.167% (25441/48768)\n",
            "381 391 Loss: 1.323 | Acc: 52.168% (25508/48896)\n",
            "382 391 Loss: 1.322 | Acc: 52.170% (25576/49024)\n",
            "383 391 Loss: 1.322 | Acc: 52.183% (25649/49152)\n",
            "384 391 Loss: 1.322 | Acc: 52.196% (25722/49280)\n",
            "385 391 Loss: 1.322 | Acc: 52.196% (25789/49408)\n",
            "386 391 Loss: 1.322 | Acc: 52.182% (25849/49536)\n",
            "387 391 Loss: 1.322 | Acc: 52.203% (25926/49664)\n",
            "388 391 Loss: 1.322 | Acc: 52.201% (25992/49792)\n",
            "389 391 Loss: 1.321 | Acc: 52.218% (26067/49920)\n",
            "390 391 Loss: 1.321 | Acc: 52.232% (26116/50000)\n",
            "0 100 Loss: 1.043 | Acc: 65.000% (65/100)\n",
            "1 100 Loss: 1.155 | Acc: 58.500% (117/200)\n",
            "2 100 Loss: 1.187 | Acc: 58.000% (174/300)\n",
            "3 100 Loss: 1.199 | Acc: 56.000% (224/400)\n",
            "4 100 Loss: 1.197 | Acc: 57.400% (287/500)\n",
            "5 100 Loss: 1.204 | Acc: 57.000% (342/600)\n",
            "6 100 Loss: 1.221 | Acc: 56.571% (396/700)\n",
            "7 100 Loss: 1.227 | Acc: 56.000% (448/800)\n",
            "8 100 Loss: 1.227 | Acc: 55.556% (500/900)\n",
            "9 100 Loss: 1.212 | Acc: 56.900% (569/1000)\n",
            "10 100 Loss: 1.195 | Acc: 57.727% (635/1100)\n",
            "11 100 Loss: 1.197 | Acc: 57.500% (690/1200)\n",
            "12 100 Loss: 1.202 | Acc: 57.385% (746/1300)\n",
            "13 100 Loss: 1.212 | Acc: 56.643% (793/1400)\n",
            "14 100 Loss: 1.206 | Acc: 56.800% (852/1500)\n",
            "15 100 Loss: 1.207 | Acc: 56.875% (910/1600)\n",
            "16 100 Loss: 1.218 | Acc: 56.647% (963/1700)\n",
            "17 100 Loss: 1.217 | Acc: 56.444% (1016/1800)\n",
            "18 100 Loss: 1.220 | Acc: 56.684% (1077/1900)\n",
            "19 100 Loss: 1.227 | Acc: 56.450% (1129/2000)\n",
            "20 100 Loss: 1.233 | Acc: 56.429% (1185/2100)\n",
            "21 100 Loss: 1.235 | Acc: 56.364% (1240/2200)\n",
            "22 100 Loss: 1.241 | Acc: 56.217% (1293/2300)\n",
            "23 100 Loss: 1.239 | Acc: 56.333% (1352/2400)\n",
            "24 100 Loss: 1.234 | Acc: 56.520% (1413/2500)\n",
            "25 100 Loss: 1.241 | Acc: 56.231% (1462/2600)\n",
            "26 100 Loss: 1.241 | Acc: 56.407% (1523/2700)\n",
            "27 100 Loss: 1.239 | Acc: 56.429% (1580/2800)\n",
            "28 100 Loss: 1.235 | Acc: 56.621% (1642/2900)\n",
            "29 100 Loss: 1.232 | Acc: 56.800% (1704/3000)\n",
            "30 100 Loss: 1.229 | Acc: 56.903% (1764/3100)\n",
            "31 100 Loss: 1.231 | Acc: 56.844% (1819/3200)\n",
            "32 100 Loss: 1.230 | Acc: 56.970% (1880/3300)\n",
            "33 100 Loss: 1.231 | Acc: 56.882% (1934/3400)\n",
            "34 100 Loss: 1.231 | Acc: 56.914% (1992/3500)\n",
            "35 100 Loss: 1.228 | Acc: 57.028% (2053/3600)\n",
            "36 100 Loss: 1.235 | Acc: 56.919% (2106/3700)\n",
            "37 100 Loss: 1.235 | Acc: 56.842% (2160/3800)\n",
            "38 100 Loss: 1.231 | Acc: 56.974% (2222/3900)\n",
            "39 100 Loss: 1.232 | Acc: 56.850% (2274/4000)\n",
            "40 100 Loss: 1.238 | Acc: 56.780% (2328/4100)\n",
            "41 100 Loss: 1.236 | Acc: 56.905% (2390/4200)\n",
            "42 100 Loss: 1.232 | Acc: 56.977% (2450/4300)\n",
            "43 100 Loss: 1.230 | Acc: 57.068% (2511/4400)\n",
            "44 100 Loss: 1.232 | Acc: 57.156% (2572/4500)\n",
            "45 100 Loss: 1.233 | Acc: 57.043% (2624/4600)\n",
            "46 100 Loss: 1.229 | Acc: 57.149% (2686/4700)\n",
            "47 100 Loss: 1.228 | Acc: 57.354% (2753/4800)\n",
            "48 100 Loss: 1.227 | Acc: 57.306% (2808/4900)\n",
            "49 100 Loss: 1.227 | Acc: 57.260% (2863/5000)\n",
            "50 100 Loss: 1.227 | Acc: 57.294% (2922/5100)\n",
            "51 100 Loss: 1.227 | Acc: 57.365% (2983/5200)\n",
            "52 100 Loss: 1.227 | Acc: 57.302% (3037/5300)\n",
            "53 100 Loss: 1.228 | Acc: 57.333% (3096/5400)\n",
            "54 100 Loss: 1.228 | Acc: 57.327% (3153/5500)\n",
            "55 100 Loss: 1.230 | Acc: 57.196% (3203/5600)\n",
            "56 100 Loss: 1.230 | Acc: 57.333% (3268/5700)\n",
            "57 100 Loss: 1.226 | Acc: 57.397% (3329/5800)\n",
            "58 100 Loss: 1.229 | Acc: 57.237% (3377/5900)\n",
            "59 100 Loss: 1.229 | Acc: 57.317% (3439/6000)\n",
            "60 100 Loss: 1.228 | Acc: 57.262% (3493/6100)\n",
            "61 100 Loss: 1.231 | Acc: 57.145% (3543/6200)\n",
            "62 100 Loss: 1.229 | Acc: 57.206% (3604/6300)\n",
            "63 100 Loss: 1.228 | Acc: 57.203% (3661/6400)\n",
            "64 100 Loss: 1.227 | Acc: 57.215% (3719/6500)\n",
            "65 100 Loss: 1.226 | Acc: 57.152% (3772/6600)\n",
            "66 100 Loss: 1.225 | Acc: 57.224% (3834/6700)\n",
            "67 100 Loss: 1.224 | Acc: 57.221% (3891/6800)\n",
            "68 100 Loss: 1.227 | Acc: 57.072% (3938/6900)\n",
            "69 100 Loss: 1.228 | Acc: 57.043% (3993/7000)\n",
            "70 100 Loss: 1.229 | Acc: 57.070% (4052/7100)\n",
            "71 100 Loss: 1.228 | Acc: 57.111% (4112/7200)\n",
            "72 100 Loss: 1.227 | Acc: 57.041% (4164/7300)\n",
            "73 100 Loss: 1.224 | Acc: 57.122% (4227/7400)\n",
            "74 100 Loss: 1.225 | Acc: 57.133% (4285/7500)\n",
            "75 100 Loss: 1.225 | Acc: 57.145% (4343/7600)\n",
            "76 100 Loss: 1.225 | Acc: 57.143% (4400/7700)\n",
            "77 100 Loss: 1.225 | Acc: 57.192% (4461/7800)\n",
            "78 100 Loss: 1.225 | Acc: 57.190% (4518/7900)\n",
            "79 100 Loss: 1.226 | Acc: 57.138% (4571/8000)\n",
            "80 100 Loss: 1.226 | Acc: 57.148% (4629/8100)\n",
            "81 100 Loss: 1.227 | Acc: 57.085% (4681/8200)\n",
            "82 100 Loss: 1.229 | Acc: 57.024% (4733/8300)\n",
            "83 100 Loss: 1.229 | Acc: 57.036% (4791/8400)\n",
            "84 100 Loss: 1.230 | Acc: 57.071% (4851/8500)\n",
            "85 100 Loss: 1.229 | Acc: 57.070% (4908/8600)\n",
            "86 100 Loss: 1.232 | Acc: 56.943% (4954/8700)\n",
            "87 100 Loss: 1.231 | Acc: 56.932% (5010/8800)\n",
            "88 100 Loss: 1.230 | Acc: 56.978% (5071/8900)\n",
            "89 100 Loss: 1.230 | Acc: 56.978% (5128/9000)\n",
            "90 100 Loss: 1.230 | Acc: 56.989% (5186/9100)\n",
            "91 100 Loss: 1.228 | Acc: 57.065% (5250/9200)\n",
            "92 100 Loss: 1.227 | Acc: 57.075% (5308/9300)\n",
            "93 100 Loss: 1.228 | Acc: 57.011% (5359/9400)\n",
            "94 100 Loss: 1.229 | Acc: 56.958% (5411/9500)\n",
            "95 100 Loss: 1.228 | Acc: 57.000% (5472/9600)\n",
            "96 100 Loss: 1.226 | Acc: 57.062% (5535/9700)\n",
            "97 100 Loss: 1.228 | Acc: 57.020% (5588/9800)\n",
            "98 100 Loss: 1.227 | Acc: 57.030% (5646/9900)\n",
            "99 100 Loss: 1.227 | Acc: 56.990% (5699/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            "0 391 Loss: 1.176 | Acc: 59.375% (76/128)\n",
            "1 391 Loss: 1.131 | Acc: 62.109% (159/256)\n",
            "2 391 Loss: 1.154 | Acc: 59.896% (230/384)\n",
            "3 391 Loss: 1.159 | Acc: 59.766% (306/512)\n",
            "4 391 Loss: 1.171 | Acc: 58.594% (375/640)\n",
            "5 391 Loss: 1.159 | Acc: 58.594% (450/768)\n",
            "6 391 Loss: 1.161 | Acc: 58.817% (527/896)\n",
            "7 391 Loss: 1.151 | Acc: 58.789% (602/1024)\n",
            "8 391 Loss: 1.136 | Acc: 59.288% (683/1152)\n",
            "9 391 Loss: 1.107 | Acc: 60.703% (777/1280)\n",
            "10 391 Loss: 1.121 | Acc: 60.511% (852/1408)\n",
            "11 391 Loss: 1.129 | Acc: 60.221% (925/1536)\n",
            "12 391 Loss: 1.142 | Acc: 59.976% (998/1664)\n",
            "13 391 Loss: 1.152 | Acc: 59.208% (1061/1792)\n",
            "14 391 Loss: 1.162 | Acc: 58.802% (1129/1920)\n",
            "15 391 Loss: 1.173 | Acc: 58.838% (1205/2048)\n",
            "16 391 Loss: 1.172 | Acc: 59.099% (1286/2176)\n",
            "17 391 Loss: 1.170 | Acc: 58.898% (1357/2304)\n",
            "18 391 Loss: 1.166 | Acc: 59.252% (1441/2432)\n",
            "19 391 Loss: 1.165 | Acc: 59.531% (1524/2560)\n",
            "20 391 Loss: 1.169 | Acc: 59.301% (1594/2688)\n",
            "21 391 Loss: 1.172 | Acc: 59.126% (1665/2816)\n",
            "22 391 Loss: 1.169 | Acc: 59.001% (1737/2944)\n",
            "23 391 Loss: 1.165 | Acc: 58.984% (1812/3072)\n",
            "24 391 Loss: 1.168 | Acc: 58.781% (1881/3200)\n",
            "25 391 Loss: 1.168 | Acc: 58.684% (1953/3328)\n",
            "26 391 Loss: 1.167 | Acc: 58.709% (2029/3456)\n",
            "27 391 Loss: 1.171 | Acc: 58.482% (2096/3584)\n",
            "28 391 Loss: 1.169 | Acc: 58.540% (2173/3712)\n",
            "29 391 Loss: 1.172 | Acc: 58.594% (2250/3840)\n",
            "30 391 Loss: 1.169 | Acc: 58.669% (2328/3968)\n",
            "31 391 Loss: 1.168 | Acc: 58.667% (2403/4096)\n",
            "32 391 Loss: 1.171 | Acc: 58.475% (2470/4224)\n",
            "33 391 Loss: 1.175 | Acc: 58.479% (2545/4352)\n",
            "34 391 Loss: 1.178 | Acc: 58.393% (2616/4480)\n",
            "35 391 Loss: 1.178 | Acc: 58.225% (2683/4608)\n",
            "36 391 Loss: 1.182 | Acc: 58.193% (2756/4736)\n",
            "37 391 Loss: 1.179 | Acc: 58.285% (2835/4864)\n",
            "38 391 Loss: 1.178 | Acc: 58.474% (2919/4992)\n",
            "39 391 Loss: 1.178 | Acc: 58.457% (2993/5120)\n",
            "40 391 Loss: 1.180 | Acc: 58.422% (3066/5248)\n",
            "41 391 Loss: 1.182 | Acc: 58.333% (3136/5376)\n",
            "42 391 Loss: 1.185 | Acc: 58.285% (3208/5504)\n",
            "43 391 Loss: 1.185 | Acc: 58.310% (3284/5632)\n",
            "44 391 Loss: 1.183 | Acc: 58.264% (3356/5760)\n",
            "45 391 Loss: 1.181 | Acc: 58.288% (3432/5888)\n",
            "46 391 Loss: 1.182 | Acc: 58.162% (3499/6016)\n",
            "47 391 Loss: 1.180 | Acc: 58.122% (3571/6144)\n",
            "48 391 Loss: 1.180 | Acc: 58.179% (3649/6272)\n",
            "49 391 Loss: 1.181 | Acc: 58.094% (3718/6400)\n",
            "50 391 Loss: 1.179 | Acc: 58.119% (3794/6528)\n",
            "51 391 Loss: 1.180 | Acc: 57.963% (3858/6656)\n",
            "52 391 Loss: 1.180 | Acc: 57.975% (3933/6784)\n",
            "53 391 Loss: 1.182 | Acc: 57.943% (4005/6912)\n",
            "54 391 Loss: 1.180 | Acc: 58.082% (4089/7040)\n",
            "55 391 Loss: 1.179 | Acc: 58.105% (4165/7168)\n",
            "56 391 Loss: 1.180 | Acc: 57.991% (4231/7296)\n",
            "57 391 Loss: 1.181 | Acc: 57.920% (4300/7424)\n",
            "58 391 Loss: 1.184 | Acc: 57.812% (4366/7552)\n",
            "59 391 Loss: 1.183 | Acc: 57.799% (4439/7680)\n",
            "60 391 Loss: 1.182 | Acc: 57.851% (4517/7808)\n",
            "61 391 Loss: 1.182 | Acc: 57.888% (4594/7936)\n",
            "62 391 Loss: 1.182 | Acc: 57.875% (4667/8064)\n",
            "63 391 Loss: 1.182 | Acc: 57.849% (4739/8192)\n",
            "64 391 Loss: 1.183 | Acc: 57.873% (4815/8320)\n",
            "65 391 Loss: 1.182 | Acc: 57.860% (4888/8448)\n",
            "66 391 Loss: 1.183 | Acc: 57.824% (4959/8576)\n",
            "67 391 Loss: 1.185 | Acc: 57.732% (5025/8704)\n",
            "68 391 Loss: 1.184 | Acc: 57.722% (5098/8832)\n",
            "69 391 Loss: 1.183 | Acc: 57.746% (5174/8960)\n",
            "70 391 Loss: 1.182 | Acc: 57.691% (5243/9088)\n",
            "71 391 Loss: 1.181 | Acc: 57.693% (5317/9216)\n",
            "72 391 Loss: 1.181 | Acc: 57.673% (5389/9344)\n",
            "73 391 Loss: 1.182 | Acc: 57.538% (5450/9472)\n",
            "74 391 Loss: 1.182 | Acc: 57.552% (5525/9600)\n",
            "75 391 Loss: 1.185 | Acc: 57.463% (5590/9728)\n",
            "76 391 Loss: 1.185 | Acc: 57.437% (5661/9856)\n",
            "77 391 Loss: 1.184 | Acc: 57.412% (5732/9984)\n",
            "78 391 Loss: 1.183 | Acc: 57.437% (5808/10112)\n",
            "79 391 Loss: 1.183 | Acc: 57.441% (5882/10240)\n",
            "80 391 Loss: 1.183 | Acc: 57.388% (5950/10368)\n",
            "81 391 Loss: 1.182 | Acc: 57.403% (6025/10496)\n",
            "82 391 Loss: 1.181 | Acc: 57.502% (6109/10624)\n",
            "83 391 Loss: 1.178 | Acc: 57.645% (6198/10752)\n",
            "84 391 Loss: 1.177 | Acc: 57.730% (6281/10880)\n",
            "85 391 Loss: 1.178 | Acc: 57.722% (6354/11008)\n",
            "86 391 Loss: 1.178 | Acc: 57.714% (6427/11136)\n",
            "87 391 Loss: 1.178 | Acc: 57.653% (6494/11264)\n",
            "88 391 Loss: 1.179 | Acc: 57.549% (6556/11392)\n",
            "89 391 Loss: 1.179 | Acc: 57.552% (6630/11520)\n",
            "90 391 Loss: 1.181 | Acc: 57.503% (6698/11648)\n",
            "91 391 Loss: 1.181 | Acc: 57.481% (6769/11776)\n",
            "92 391 Loss: 1.181 | Acc: 57.476% (6842/11904)\n",
            "93 391 Loss: 1.180 | Acc: 57.480% (6916/12032)\n",
            "94 391 Loss: 1.180 | Acc: 57.434% (6984/12160)\n",
            "95 391 Loss: 1.179 | Acc: 57.487% (7064/12288)\n",
            "96 391 Loss: 1.179 | Acc: 57.490% (7138/12416)\n",
            "97 391 Loss: 1.178 | Acc: 57.526% (7216/12544)\n",
            "98 391 Loss: 1.178 | Acc: 57.497% (7286/12672)\n",
            "99 391 Loss: 1.180 | Acc: 57.422% (7350/12800)\n",
            "100 391 Loss: 1.179 | Acc: 57.472% (7430/12928)\n",
            "101 391 Loss: 1.178 | Acc: 57.506% (7508/13056)\n",
            "102 391 Loss: 1.177 | Acc: 57.517% (7583/13184)\n",
            "103 391 Loss: 1.176 | Acc: 57.520% (7657/13312)\n",
            "104 391 Loss: 1.176 | Acc: 57.589% (7740/13440)\n",
            "105 391 Loss: 1.175 | Acc: 57.614% (7817/13568)\n",
            "106 391 Loss: 1.175 | Acc: 57.623% (7892/13696)\n",
            "107 391 Loss: 1.176 | Acc: 57.516% (7951/13824)\n",
            "108 391 Loss: 1.175 | Acc: 57.519% (8025/13952)\n",
            "109 391 Loss: 1.176 | Acc: 57.528% (8100/14080)\n",
            "110 391 Loss: 1.175 | Acc: 57.587% (8182/14208)\n",
            "111 391 Loss: 1.175 | Acc: 57.596% (8257/14336)\n",
            "112 391 Loss: 1.175 | Acc: 57.612% (8333/14464)\n",
            "113 391 Loss: 1.174 | Acc: 57.634% (8410/14592)\n",
            "114 391 Loss: 1.174 | Acc: 57.595% (8478/14720)\n",
            "115 391 Loss: 1.174 | Acc: 57.604% (8553/14848)\n",
            "116 391 Loss: 1.173 | Acc: 57.679% (8638/14976)\n",
            "117 391 Loss: 1.174 | Acc: 57.687% (8713/15104)\n",
            "118 391 Loss: 1.175 | Acc: 57.609% (8775/15232)\n",
            "119 391 Loss: 1.177 | Acc: 57.565% (8842/15360)\n",
            "120 391 Loss: 1.176 | Acc: 57.554% (8914/15488)\n",
            "121 391 Loss: 1.178 | Acc: 57.473% (8975/15616)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    train(net, criterion, trainloader, optimizer, device, epoch)\n",
        "    test(net, testloader, device, criterion, epoch)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqoOO6IDCzGE"
      },
      "source": [
        "# Test model and compute accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1f3nMug4CnZU"
      },
      "outputs": [],
      "source": [
        "# Convert testset to appropriate format\n",
        "test_labels = []\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        test_labels.extend(labels.cpu().numpy().tolist())\n",
        "        predictions.extend(predicted.cpu().numpy().tolist())\n",
        "\n",
        "# Compute the classification report using sklearn library\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cr = classification_report(test_labels, predictions, target_names=classes)\n",
        "\n",
        "# Display the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVv2fLmXPZSq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrGzd9H5ZGyhUEOqIUs/Hy",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "005373afc6b041609bdf9544ad2f672b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5771af0af0f442cac636b10d8d368f9",
              "IPY_MODEL_4bfaf8dd49aa477b8825c0b37c26a629",
              "IPY_MODEL_403fb86925f144cbaca52883ebe5a84e"
            ],
            "layout": "IPY_MODEL_87c19b59d8ee4a39bf4623c3aad1c912"
          }
        },
        "a5771af0af0f442cac636b10d8d368f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a99eb29023114fe7905c060f854f7aea",
            "placeholder": "",
            "style": "IPY_MODEL_e1d40381a9e3486e8f299537f3740d14",
            "value": "100%"
          }
        },
        "4bfaf8dd49aa477b8825c0b37c26a629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65cfd10220c0446e88a6666762492ac4",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c0fa492672c4a3baccaf08a4869032b",
            "value": 170498071
          }
        },
        "403fb86925f144cbaca52883ebe5a84e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ffacaf7135a4942848c78791f56932f",
            "placeholder": "",
            "style": "IPY_MODEL_1b5b230f256847f3b83b987bf9da37f2",
            "value": " 170498071/170498071 [00:02&lt;00:00, 51356403.50it/s]"
          }
        },
        "87c19b59d8ee4a39bf4623c3aad1c912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99eb29023114fe7905c060f854f7aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d40381a9e3486e8f299537f3740d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65cfd10220c0446e88a6666762492ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c0fa492672c4a3baccaf08a4869032b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ffacaf7135a4942848c78791f56932f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b5b230f256847f3b83b987bf9da37f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}